{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lSCsGrI6DRh5",
    "outputId": "575271c7-fa8c-49f5-d2f0-d8989e189838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\adity\\anaconda3\\lib\\site-packages (4.12.5)\n",
      "Requirement already satisfied: requests in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (0.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in c:\\users\\adity\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\adity\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\adity\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: pytorch_pretrained_bert in c:\\users\\adity\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from pytorch_pretrained_bert) (1.18.16)\n",
      "Requirement already satisfied: tqdm in c:\\users\\adity\\anaconda3\\lib\\site-packages (from pytorch_pretrained_bert) (4.59.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\adity\\anaconda3\\lib\\site-packages (from pytorch_pretrained_bert) (1.20.1)\n",
      "Requirement already satisfied: requests in c:\\users\\adity\\anaconda3\\lib\\site-packages (from pytorch_pretrained_bert) (2.25.1)\n",
      "Requirement already satisfied: torch>=0.4.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from pytorch_pretrained_bert) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from boto3->pytorch_pretrained_bert) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
      "Collecting botocore<1.22.0,>=1.21.16\n",
      "  Using cached botocore-1.21.65-py3-none-any.whl (8.0 MB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from botocore<1.22.0,>=1.21.16->boto3->pytorch_pretrained_bert) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from botocore<1.22.0,>=1.21.16->boto3->pytorch_pretrained_bert) (1.26.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.16->boto3->pytorch_pretrained_bert) (1.16.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->pytorch_pretrained_bert) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->pytorch_pretrained_bert) (2020.12.5)\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.23.19\n",
      "    Uninstalling botocore-1.23.19:\n",
      "      Successfully uninstalled botocore-1.23.19\n",
      "Successfully installed botocore-1.21.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.22.19 requires botocore==1.23.19, but you have botocore 1.21.65 which is incompatible.\n",
      "awscli 1.22.19 requires colorama<0.4.4,>=0.2.5, but you have colorama 0.4.4 which is incompatible.\n",
      "awscli 1.22.19 requires docutils<0.16,>=0.10, but you have docutils 0.17 which is incompatible.\n",
      "ERROR: Could not find a version that satisfies the requirement BertModel\n",
      "ERROR: No matching distribution found for BertModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\adity\\anaconda3\\lib\\site-packages (0.1.96)\n",
      "Collecting awscli"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\adity\\\\anaconda3\\\\Lib\\\\site-packages\\\\yaml\\\\_yaml.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached awscli-1.22.19-py3-none-any.whl (3.8 MB)\n",
      "Collecting six\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting rsa<4.8,>=3.1.2\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Using cached s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
      "Collecting botocore==1.23.19\n",
      "  Using cached botocore-1.23.19-py3-none-any.whl (8.4 MB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Collecting colorama<0.4.4,>=0.2.5\n",
      "  Using cached colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
      "Collecting PyYAML<5.5,>=3.10\n",
      "  Using cached PyYAML-5.4.1-cp38-cp38-win_amd64.whl (213 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: six, urllib3, python-dateutil, jmespath, pyasn1, botocore, s3transfer, rsa, PyYAML, docutils, colorama, awscli\n",
      "Requirement already satisfied: tools in c:\\users\\adity\\anaconda3\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: lxml in c:\\users\\adity\\anaconda3\\lib\\site-packages (from tools) (4.6.3)\n",
      "Requirement already satisfied: pytils in c:\\users\\adity\\anaconda3\\lib\\site-packages (from tools) (0.3)\n",
      "Requirement already satisfied: six in c:\\users\\adity\\anaconda3\\lib\\site-packages (from tools) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorch_pretrained_bert\n",
    "!pip install BertModel\n",
    "!pip install sentencepiece\n",
    "!pip install awscli --ignore-installed six\n",
    "!pip install tools\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from urllib.request import urlopen\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras import regularizers\n",
    "from keras.constraints import unit_norm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tools import *\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "nltk.download('stopwords')\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, Embedding\n",
    "from transformers import BertTokenizer, RobertaTokenizer, BertForMaskedLM, RobertaForMaskedLM, GPT2Tokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "uzX2fkVCIqyM",
    "outputId": "5548f873-d5b8-4d56-d99b-b30405533d80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.config.list_physical_devices('GPU')\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorboard\n",
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 23.5G    0 5536k    0     0  5713k      0  1:11:59 --:--:--  1:11:59 5713k\n",
      "  0 23.5G    0 17.3M    0     0  9028k      0  0:45:33  0:00:01  0:45:32 9028k\n",
      "  0 23.5G    0 29.6M    0     0   9.9M      0  0:40:14  0:00:02  0:40:12  9.9M\n",
      "  0 23.5G    0 41.6M    0     0  10.5M      0  0:38:14  0:00:03  0:38:11 10.5M\n",
      "  0 23.5G    0 54.5M    0     0  10.9M      0  0:36:38  0:00:04  0:36:34 10.9M\n",
      "  0 23.5G    0 66.0M    0     0  11.0M      0  0:36:22  0:00:05  0:36:17 12.1M\n",
      "  0 23.5G    0 78.8M    0     0  11.3M      0  0:35:29  0:00:06  0:35:23 12.3M\n",
      "  0 23.5G    0 91.4M    0     0  11.4M      0  0:35:02  0:00:07  0:34:55 12.3M\n",
      "  0 23.5G    0  104M    0     0  11.6M      0  0:34:36  0:00:08  0:34:28 12.4M\n",
      "  0 23.5G    0  116M    0     0  11.6M      0  0:34:28  0:00:09  0:34:19 12.3M\n",
      "  0 23.5G    0  128M    0     0  11.6M      0  0:34:20  0:00:10  0:34:10 12.4M\n",
      "  0 23.5G    0  141M    0     0  11.7M      0  0:34:04  0:00:11  0:33:53 12.4M\n",
      "  0 23.5G    0  153M    0     0  11.8M      0  0:34:00  0:00:12  0:33:48 12.3M\n",
      "  0 23.5G    0  165M    0     0  11.8M      0  0:33:52  0:00:13  0:33:39 12.3M\n",
      "  0 23.5G    0  178M    0     0  11.9M      0  0:33:39  0:00:14  0:33:25 12.5M\n",
      "  0 23.5G    0  191M    0     0  11.9M      0  0:33:30  0:00:15  0:33:15 12.6M\n",
      "  0 23.5G    0  204M    0     0  12.0M      0  0:33:20  0:00:16  0:33:04 12.6M\n",
      "  0 23.5G    0  216M    0     0  12.0M      0  0:33:19  0:00:17  0:33:02 12.6M\n",
      "  0 23.5G    0  229M    0     0  12.0M      0  0:33:14  0:00:18  0:32:56 12.7M\n",
      "  1 23.5G    1  242M    0     0  12.1M      0  0:33:06  0:00:19  0:32:47 12.7M\n",
      "  1 23.5G    1  254M    0     0  12.1M      0  0:33:02  0:00:20  0:32:42 12.6M\n",
      "  1 23.5G    1  267M    0     0  12.1M      0  0:32:56  0:00:21  0:32:35 12.6M\n",
      "  1 23.5G    1  280M    0     0  12.2M      0  0:32:50  0:00:22  0:32:28 12.8M\n",
      "  1 23.5G    1  293M    0     0  12.2M      0  0:32:46  0:00:23  0:32:23 12.9M\n",
      "  1 23.5G    1  306M    0     0  12.2M      0  0:32:44  0:00:24  0:32:20 12.8M\n",
      "  1 23.5G    1  319M    0     0  12.3M      0  0:32:39  0:00:25  0:32:14 12.9M\n",
      "  1 23.5G    1  331M    0     0  12.3M      0  0:32:38  0:00:26  0:32:12 12.7M\n",
      "  1 23.5G    1  344M    0     0  12.3M      0  0:32:37  0:00:27  0:32:10 12.7M\n",
      "  1 23.5G    1  357M    0     0  12.3M      0  0:32:32  0:00:28  0:32:04 12.7M\n",
      "  1 23.5G    1  370M    0     0  12.3M      0  0:32:32  0:00:29  0:32:03 12.7M\n",
      "  1 23.5G    1  383M    0     0  12.3M      0  0:32:27  0:00:30  0:31:57 12.7M\n",
      "  1 23.5G    1  395M    0     0  12.3M      0  0:32:37  0:00:32  0:32:05 12.3M\n",
      "  1 23.5G    1  408M    0     0  12.3M      0  0:32:25  0:00:32  0:31:53 12.8M\n",
      "  1 23.5G    1  420M    0     0  12.3M      0  0:32:27  0:00:33  0:31:54 12.5M\n",
      "  1 23.5G    1  433M    0     0  12.3M      0  0:32:25  0:00:34  0:31:51 12.6M\n",
      "  1 23.5G    1  446M    0     0  12.4M      0  0:32:22  0:00:35  0:31:47 12.5M\n",
      "  1 23.5G    1  458M    0     0  12.4M      0  0:32:22  0:00:36  0:31:46 13.0M\n",
      "  1 23.5G    1  471M    0     0  12.4M      0  0:32:21  0:00:37  0:31:44 12.5M\n",
      "  2 23.5G    2  483M    0     0  12.4M      0  0:32:23  0:00:38  0:31:45 12.5M\n",
      "  2 23.5G    2  496M    0     0  12.4M      0  0:32:20  0:00:39  0:31:41 12.6M\n",
      "  2 23.5G    2  509M    0     0  12.4M      0  0:32:17  0:00:40  0:31:37 12.6M\n",
      "  2 23.5G    2  522M    0     0  12.4M      0  0:32:14  0:00:41  0:31:33 12.8M\n",
      "  2 23.5G    2  535M    0     0  12.4M      0  0:32:12  0:00:42  0:31:30 12.8M\n",
      "  2 23.5G    2  548M    0     0  12.4M      0  0:32:11  0:00:43  0:31:28 13.0M\n",
      "  2 23.5G    2  561M    0     0  12.4M      0  0:32:12  0:00:45  0:31:27 12.9M\n",
      "  2 23.5G    2  574M    0     0  12.5M      0  0:32:07  0:00:45  0:31:22 13.0M\n",
      "  2 23.5G    2  588M    0     0  12.5M      0  0:32:04  0:00:46  0:31:18 13.0M\n",
      "  2 23.5G    2  601M    0     0  12.5M      0  0:32:02  0:00:47  0:31:15 13.1M\n",
      "  2 23.5G    2  615M    0     0  12.5M      0  0:31:58  0:00:48  0:31:10 13.2M\n",
      "  2 23.5G    2  628M    0     0  12.5M      0  0:31:56  0:00:49  0:31:07 13.4M\n",
      "  2 23.5G    2  641M    0     0  12.5M      0  0:31:54  0:00:50  0:31:04 13.3M\n",
      "  2 23.5G    2  654M    0     0  12.5M      0  0:31:53  0:00:51  0:31:02 13.2M\n",
      "  2 23.5G    2  667M    0     0  12.6M      0  0:31:51  0:00:52  0:30:59 13.2M\n",
      "  2 23.5G    2  681M    0     0  12.6M      0  0:31:48  0:00:53  0:30:55 13.2M\n",
      "  2 23.5G    2  694M    0     0  12.6M      0  0:31:47  0:00:54  0:30:53 13.2M\n",
      "  2 23.5G    2  708M    0     0  12.6M      0  0:31:45  0:00:55  0:30:50 13.3M\n",
      "  2 23.5G    2  721M    0     0  12.6M      0  0:31:44  0:00:56  0:30:48 13.2M\n",
      "  3 23.5G    3  734M    0     0  12.6M      0  0:31:42  0:00:57  0:30:45 13.2M\n",
      "  3 23.5G    3  747M    0     0  12.6M      0  0:31:40  0:00:58  0:30:42 13.2M\n",
      "  3 23.5G    3  761M    0     0  12.6M      0  0:31:38  0:00:59  0:30:39 13.2M\n",
      "  3 23.5G    3  774M    0     0  12.6M      0  0:31:37  0:01:00  0:30:37 13.2M\n",
      "  3 23.5G    3  787M    0     0  12.7M      0  0:31:36  0:01:01  0:30:35 13.2M\n",
      "  3 23.5G    3  800M    0     0  12.7M      0  0:31:36  0:01:02  0:30:34 13.2M\n",
      "  3 23.5G    3  813M    0     0  12.7M      0  0:31:35  0:01:03  0:30:32 13.1M\n",
      "  3 23.5G    3  826M    0     0  12.7M      0  0:31:34  0:01:04  0:30:30 13.0M\n",
      "  3 23.5G    3  839M    0     0  12.7M      0  0:31:32  0:01:05  0:30:27 13.1M\n",
      "  3 23.5G    3  853M    0     0  12.7M      0  0:31:31  0:01:06  0:30:25 13.2M\n",
      "  3 23.5G    3  866M    0     0  12.7M      0  0:31:30  0:01:07  0:30:23 13.2M\n",
      "  3 23.5G    3  880M    0     0  12.7M      0  0:31:28  0:01:08  0:30:20 13.3M\n",
      "  3 23.5G    3  893M    0     0  12.7M      0  0:31:27  0:01:09  0:30:18 13.3M\n",
      "  3 23.5G    3  906M    0     0  12.7M      0  0:31:26  0:01:10  0:30:16 13.2M\n",
      "  3 23.5G    3  919M    0     0  12.7M      0  0:31:26  0:01:11  0:30:15 13.1M\n",
      "  3 23.5G    3  932M    0     0  12.7M      0  0:31:25  0:01:12  0:30:13 13.1M\n",
      "  3 23.5G    3  945M    0     0  12.7M      0  0:31:25  0:01:13  0:30:12 13.0M\n",
      "  3 23.5G    3  959M    0     0  12.7M      0  0:31:23  0:01:14  0:30:09 13.1M\n",
      "  4 23.5G    4  972M    0     0  12.7M      0  0:31:23  0:01:15  0:30:08 13.1M\n",
      "  4 23.5G    4  985M    0     0  12.8M      0  0:31:22  0:01:16  0:30:06 13.1M\n",
      "  4 23.5G    4  999M    0     0  12.8M      0  0:31:20  0:01:17  0:30:03 13.3M\n",
      "  4 23.5G    4 1012M    0     0  12.8M      0  0:31:20  0:01:18  0:30:02 13.3M\n",
      "  4 23.5G    4 1025M    0     0  12.8M      0  0:31:19  0:01:19  0:30:00 13.3M\n",
      "  4 23.5G    4 1038M    0     0  12.8M      0  0:31:18  0:01:20  0:29:58 13.3M\n",
      "  4 23.5G    4 1052M    0     0  12.8M      0  0:31:16  0:01:21  0:29:55 13.4M\n",
      "  4 23.5G    4 1066M    0     0  12.8M      0  0:31:15  0:01:22  0:29:53 13.4M\n",
      "  4 23.5G    4 1079M    0     0  12.8M      0  0:31:14  0:01:23  0:29:51 13.4M\n",
      "  4 23.5G    4 1092M    0     0  12.8M      0  0:31:13  0:01:24  0:29:49 13.4M\n",
      "  4 23.5G    4 1105M    0     0  12.8M      0  0:31:13  0:01:25  0:29:48 13.4M\n",
      "  4 23.5G    4 1119M    0     0  12.8M      0  0:31:12  0:01:26  0:29:46 13.3M\n",
      "  4 23.5G    4 1132M    0     0  12.8M      0  0:31:11  0:01:27  0:29:44 13.3M\n",
      "  4 23.5G    4 1146M    0     0  12.8M      0  0:31:11  0:01:28  0:29:43 13.2M\n",
      "  4 23.5G    4 1159M    0     0  12.8M      0  0:31:10  0:01:29  0:29:41 13.2M\n",
      "  4 23.5G    4 1172M    0     0  12.8M      0  0:31:09  0:01:30  0:29:39 13.3M\n",
      "  4 23.5G    4 1184M    0     0  12.8M      0  0:31:10  0:01:31  0:29:39 13.0M\n",
      "  4 23.5G    4 1197M    0     0  12.8M      0  0:31:11  0:01:32  0:29:39 12.9M\n",
      "  5 23.5G    5 1209M    0     0  12.8M      0  0:31:11  0:01:33  0:29:38 12.8M\n",
      "  5 23.5G    5 1223M    0     0  12.8M      0  0:31:11  0:01:34  0:29:37 12.7M\n",
      "  5 23.5G    5 1236M    0     0  12.8M      0  0:31:10  0:01:35  0:29:35 12.7M\n",
      "  5 23.5G    5 1249M    0     0  12.8M      0  0:31:10  0:01:36  0:29:34 12.9M\n",
      "  5 23.5G    5 1262M    0     0  12.8M      0  0:31:09  0:01:37  0:29:32 13.0M\n",
      "  5 23.5G    5 1276M    0     0  12.8M      0  0:31:09  0:01:38  0:29:31 13.2M\n",
      "  5 23.5G    5 1289M    0     0  12.8M      0  0:31:08  0:01:39  0:29:29 13.2M\n",
      "  5 23.5G    5 1302M    0     0  12.8M      0  0:31:08  0:01:40  0:29:28 13.2M\n",
      "  5 23.5G    5 1315M    0     0  12.9M      0  0:31:07  0:01:41  0:29:26 13.2M\n",
      "  5 23.5G    5 1327M    0     0  12.8M      0  0:31:08  0:01:42  0:29:26 13.0M\n",
      "  5 23.5G    5 1341M    0     0  12.8M      0  0:31:08  0:01:43  0:29:25 12.9M\n",
      "  5 23.5G    5 1354M    0     0  12.9M      0  0:31:08  0:01:44  0:29:24 12.9M\n",
      "  5 23.5G    5 1366M    0     0  12.8M      0  0:31:08  0:01:45  0:29:23 12.8M\n",
      "  5 23.5G    5 1380M    0     0  12.9M      0  0:31:07  0:01:46  0:29:21 12.9M\n",
      "  5 23.5G    5 1393M    0     0  12.9M      0  0:31:07  0:01:47  0:29:20 13.0M\n",
      "  5 23.5G    5 1406M    0     0  12.9M      0  0:31:07  0:01:48  0:29:19 13.0M\n",
      "  5 23.5G    5 1418M    0     0  12.9M      0  0:31:07  0:01:49  0:29:18 12.9M\n",
      "  5 23.5G    5 1431M    0     0  12.8M      0  0:31:08  0:01:50  0:29:18 12.9M\n",
      "  5 23.5G    5 1444M    0     0  12.9M      0  0:31:08  0:01:51  0:29:17 12.8M\n",
      "  6 23.5G    6 1457M    0     0  12.9M      0  0:31:07  0:01:52  0:29:15 12.8M\n",
      "  6 23.5G    6 1470M    0     0  12.9M      0  0:31:07  0:01:53  0:29:14 12.9M\n",
      "  6 23.5G    6 1484M    0     0  12.9M      0  0:31:06  0:01:54  0:29:12 13.1M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6 23.5G    6 1497M    0     0  12.9M      0  0:31:06  0:01:55  0:29:11 13.2M\n",
      "  6 23.5G    6 1510M    0     0  12.9M      0  0:31:05  0:01:56  0:29:09 13.2M\n",
      "  6 23.5G    6 1524M    0     0  12.9M      0  0:31:05  0:01:57  0:29:08 13.3M\n",
      "  6 23.5G    6 1537M    0     0  12.9M      0  0:31:05  0:01:58  0:29:07 13.2M\n",
      "  6 23.5G    6 1550M    0     0  12.9M      0  0:31:04  0:01:59  0:29:05 13.2M\n",
      "  6 23.5G    6 1563M    0     0  12.9M      0  0:31:05  0:02:00  0:29:05 13.0M\n",
      "  6 23.5G    6 1576M    0     0  12.9M      0  0:31:04  0:02:01  0:29:03 13.0M\n",
      "  6 23.5G    6 1589M    0     0  12.9M      0  0:31:04  0:02:02  0:29:02 13.0M\n",
      "  6 23.5G    6 1602M    0     0  12.9M      0  0:31:04  0:02:03  0:29:01 13.0M\n",
      "  6 23.5G    6 1615M    0     0  12.9M      0  0:31:04  0:02:04  0:29:00 13.0M\n",
      "  6 23.5G    6 1628M    0     0  12.9M      0  0:31:04  0:02:05  0:28:59 12.9M\n",
      "  6 23.5G    6 1641M    0     0  12.9M      0  0:31:04  0:02:06  0:28:58 13.0M\n",
      "  6 23.5G    6 1654M    0     0  12.9M      0  0:31:04  0:02:07  0:28:57 12.9M\n",
      "  6 23.5G    6 1667M    0     0  12.9M      0  0:31:03  0:02:08  0:28:55 13.1M\n",
      "  6 23.5G    6 1679M    0     0  12.7M      0  0:31:34  0:02:12  0:29:22 9267k\n",
      "  7 23.5G    7 1707M    0     0  12.8M      0  0:31:16  0:02:12  0:29:04 11.3M\n",
      "  7 23.5G    7 1719M    0     0  12.8M      0  0:31:19  0:02:14  0:29:05 10.9M\n",
      "  7 23.5G    7 1725M    0     0  12.7M      0  0:31:26  0:02:15  0:29:11 10.0M\n",
      "  7 23.5G    7 1736M    0     0  12.7M      0  0:31:30  0:02:16  0:29:14 9745k\n",
      "  7 23.5G    7 1743M    0     0  12.7M      0  0:31:33  0:02:16  0:29:17 13.0M\n",
      "  7 23.5G    7 1751M    0     0  12.6M      0  0:31:38  0:02:17  0:29:21 9079k\n",
      "  7 23.5G    7 1762M    0     0  12.6M      0  0:31:40  0:02:18  0:29:22 8913k\n",
      "  7 23.5G    7 1773M    0     0  12.6M      0  0:31:41  0:02:19  0:29:22  9.7M\n",
      "  7 23.5G    7 1786M    0     0  12.6M      0  0:31:41  0:02:20  0:29:21 10.4M\n",
      "  7 23.5G    7 1798M    0     0  12.6M      0  0:31:42  0:02:21  0:29:21 10.9M\n",
      "  7 23.5G    7 1811M    0     0  12.6M      0  0:31:42  0:02:22  0:29:20 11.9M\n",
      "  7 23.5G    7 1823M    0     0  12.6M      0  0:31:42  0:02:23  0:29:19 12.3M\n",
      "  7 23.5G    7 1836M    0     0  12.6M      0  0:31:42  0:02:24  0:29:18 12.6M\n",
      "  7 23.5G    7 1848M    0     0  12.6M      0  0:31:45  0:02:26  0:29:19 11.9M\n",
      "  7 23.5G    7 1863M    0     0  12.6M      0  0:31:40  0:02:26  0:29:14 12.9M\n",
      "  7 23.5G    7 1876M    0     0  12.6M      0  0:31:40  0:02:27  0:29:13 13.1M\n",
      "  7 23.5G    7 1889M    0     0  12.6M      0  0:31:40  0:02:28  0:29:12 13.0M\n",
      "  7 23.5G    7 1902M    0     0  12.6M      0  0:31:39  0:02:29  0:29:10 13.2M\n",
      "  7 23.5G    7 1915M    0     0  12.6M      0  0:31:39  0:02:30  0:29:09 13.9M\n",
      "  7 23.5G    7 1927M    0     0  12.6M      0  0:31:39  0:02:31  0:29:08 12.8M\n",
      "  8 23.5G    8 1940M    0     0  12.6M      0  0:31:39  0:02:32  0:29:07 12.8M\n",
      "  8 23.5G    8 1953M    0     0  12.6M      0  0:31:39  0:02:33  0:29:06 12.8M\n",
      "  8 23.5G    8 1966M    0     0  12.6M      0  0:31:38  0:02:34  0:29:04 12.8M\n",
      "  8 23.5G    8 1979M    0     0  12.6M      0  0:31:38  0:02:35  0:29:03 12.8M\n",
      "  8 23.5G    8 1992M    0     0  12.6M      0  0:31:38  0:02:36  0:29:02 13.0M\n",
      "  8 23.5G    8 2006M    0     0  12.6M      0  0:31:37  0:02:37  0:29:00 13.0M\n",
      "  8 23.5G    8 2019M    0     0  12.7M      0  0:31:36  0:02:38  0:28:58 13.2M\n",
      "  8 23.5G    8 2032M    0     0  12.7M      0  0:31:36  0:02:39  0:28:57 13.0M\n",
      "  8 23.5G    8 2045M    0     0  12.7M      0  0:31:36  0:02:40  0:28:56 13.1M\n",
      "  8 23.5G    8 2058M    0     0  12.7M      0  0:31:36  0:02:41  0:28:55 13.0M\n",
      "  8 23.5G    8 2071M    0     0  12.7M      0  0:31:36  0:02:42  0:28:54 13.0M\n",
      "  8 23.5G    8 2084M    0     0  12.7M      0  0:31:36  0:02:43  0:28:53 12.8M\n",
      "  8 23.5G    8 2097M    0     0  12.7M      0  0:31:35  0:02:44  0:28:51 12.9M\n",
      "  8 23.5G    8 2110M    0     0  12.7M      0  0:31:35  0:02:45  0:28:50 12.9M\n",
      "  8 23.5G    8 2123M    0     0  12.7M      0  0:31:35  0:02:46  0:28:49 12.9M\n",
      "  8 23.5G    8 2136M    0     0  12.7M      0  0:31:34  0:02:47  0:28:47 12.9M\n",
      "  8 23.5G    8 2149M    0     0  12.7M      0  0:31:34  0:02:48  0:28:46 13.0M\n",
      "  8 23.5G    8 2162M    0     0  12.7M      0  0:31:34  0:02:49  0:28:45 13.0M\n",
      "  9 23.5G    9 2175M    0     0  12.7M      0  0:31:33  0:02:50  0:28:43 13.1M\n",
      "  9 23.5G    9 2189M    0     0  12.7M      0  0:31:32  0:02:51  0:28:41 13.2M\n",
      "  9 23.5G    9 2202M    0     0  12.7M      0  0:31:32  0:02:52  0:28:40 13.2M\n",
      "  9 23.5G    9 2215M    0     0  12.7M      0  0:31:32  0:02:53  0:28:39 13.2M\n",
      "  9 23.5G    9 2228M    0     0  12.7M      0  0:31:32  0:02:54  0:28:38 13.2M\n",
      "  9 23.5G    9 2241M    0     0  12.7M      0  0:31:31  0:02:55  0:28:36 13.1M\n",
      "  9 23.5G    9 2255M    0     0  12.7M      0  0:31:31  0:02:56  0:28:35 13.1M\n",
      "  9 23.5G    9 2268M    0     0  12.7M      0  0:31:30  0:02:57  0:28:33 13.1M\n",
      "  9 23.5G    9 2281M    0     0  12.7M      0  0:31:30  0:02:58  0:28:32 13.1M\n",
      "  9 23.5G    9 2294M    0     0  12.7M      0  0:31:30  0:02:59  0:28:31 13.1M\n",
      "  9 23.5G    9 2307M    0     0  12.7M      0  0:31:30  0:03:00  0:28:30 13.0M\n",
      "  9 23.5G    9 2319M    0     0  12.7M      0  0:31:30  0:03:01  0:28:29 12.8M\n",
      "  9 23.5G    9 2332M    0     0  12.7M      0  0:31:30  0:03:02  0:28:28 12.8M\n",
      "  9 23.5G    9 2345M    0     0  12.7M      0  0:31:30  0:03:03  0:28:27 12.8M\n",
      "  9 23.5G    9 2357M    0     0  12.7M      0  0:31:30  0:03:04  0:28:26 12.7M\n",
      "  9 23.5G    9 2371M    0     0  12.7M      0  0:31:30  0:03:05  0:28:25 12.8M\n",
      "  9 23.5G    9 2384M    0     0  12.7M      0  0:31:29  0:03:06  0:28:23 13.0M\n",
      "  9 23.5G    9 2397M    0     0  12.7M      0  0:31:29  0:03:07  0:28:22 13.0M\n",
      " 10 23.5G   10 2410M    0     0  12.7M      0  0:31:29  0:03:08  0:28:21 13.0M\n",
      " 10 23.5G   10 2424M    0     0  12.7M      0  0:31:28  0:03:09  0:28:19 13.2M\n",
      " 10 23.5G   10 2437M    0     0  12.7M      0  0:31:28  0:03:10  0:28:18 13.2M\n",
      " 10 23.5G   10 2450M    0     0  12.7M      0  0:31:28  0:03:11  0:28:17 13.1M\n",
      " 10 23.5G   10 2463M    0     0  12.7M      0  0:31:28  0:03:12  0:28:16 13.0M\n",
      " 10 23.5G   10 2476M    0     0  12.7M      0  0:31:27  0:03:13  0:28:14 13.1M\n",
      " 10 23.5G   10 2490M    0     0  12.7M      0  0:31:27  0:03:14  0:28:13 13.1M\n",
      " 10 23.5G   10 2503M    0     0  12.7M      0  0:31:26  0:03:15  0:28:11 13.0M\n",
      " 10 23.5G   10 2516M    0     0  12.7M      0  0:31:26  0:03:16  0:28:10 13.1M\n",
      " 10 23.5G   10 2529M    0     0  12.7M      0  0:31:26  0:03:17  0:28:09 13.2M\n",
      " 10 23.5G   10 2541M    0     0  12.7M      0  0:31:26  0:03:18  0:28:08 13.0M\n",
      " 10 23.5G   10 2553M    0     0  12.7M      0  0:31:27  0:03:19  0:28:08 12.7M\n",
      " 10 23.5G   10 2567M    0     0  12.7M      0  0:31:26  0:03:20  0:28:06 12.8M\n",
      " 10 23.5G   10 2580M    0     0  12.7M      0  0:31:26  0:03:21  0:28:05 12.8M\n",
      " 10 23.5G   10 2593M    0     0  12.7M      0  0:31:26  0:03:22  0:28:04 12.7M\n",
      " 10 23.5G   10 2605M    0     0  12.7M      0  0:31:26  0:03:23  0:28:03 12.7M\n",
      " 10 23.5G   10 2619M    0     0  12.7M      0  0:31:25  0:03:24  0:28:01 13.1M\n",
      " 10 23.5G   10 2633M    0     0  12.7M      0  0:31:25  0:03:25  0:28:00 13.2M\n",
      " 10 23.5G   10 2646M    0     0  12.7M      0  0:31:24  0:03:26  0:27:58 13.2M\n",
      " 11 23.5G   11 2659M    0     0  12.7M      0  0:31:24  0:03:27  0:27:57 13.3M\n",
      " 11 23.5G   11 2672M    0     0  12.7M      0  0:31:24  0:03:28  0:27:56 13.4M\n",
      " 11 23.5G   11 2686M    0     0  12.7M      0  0:31:23  0:03:29  0:27:54 13.3M\n",
      " 11 23.5G   11 2698M    0     0  12.7M      0  0:31:23  0:03:30  0:27:53 13.1M\n",
      " 11 23.5G   11 2712M    0     0  12.7M      0  0:31:23  0:03:31  0:27:52 13.0M\n",
      " 11 23.5G   11 2725M    0     0  12.7M      0  0:31:23  0:03:32  0:27:51 13.0M\n",
      " 11 23.5G   11 2737M    0     0  12.7M      0  0:31:23  0:03:33  0:27:50 12.9M\n",
      " 11 23.5G   11 2751M    0     0  12.7M      0  0:31:23  0:03:34  0:27:49 12.9M\n",
      " 11 23.5G   11 2764M    0     0  12.7M      0  0:31:23  0:03:35  0:27:48 13.0M\n",
      " 11 23.5G   11 2776M    0     0  12.7M      0  0:31:23  0:03:36  0:27:47 12.9M\n",
      " 11 23.5G   11 2790M    0     0  12.8M      0  0:31:22  0:03:37  0:27:45 13.0M\n",
      " 11 23.5G   11 2803M    0     0  12.8M      0  0:31:22  0:03:38  0:27:44 13.1M\n",
      " 11 23.5G   11 2816M    0     0  12.8M      0  0:31:22  0:03:39  0:27:43 13.1M\n",
      " 11 23.5G   11 2829M    0     0  12.8M      0  0:31:22  0:03:40  0:27:42 13.0M\n",
      " 11 23.5G   11 2842M    0     0  12.8M      0  0:31:21  0:03:41  0:27:40 13.2M\n",
      " 11 23.5G   11 2856M    0     0  12.8M      0  0:31:21  0:03:42  0:27:39 13.2M\n",
      " 11 23.5G   11 2869M    0     0  12.8M      0  0:31:21  0:03:43  0:27:38 13.1M\n",
      " 11 23.5G   11 2882M    0     0  12.8M      0  0:31:20  0:03:44  0:27:36 13.2M\n",
      " 12 23.5G   12 2895M    0     0  12.8M      0  0:31:20  0:03:45  0:27:35 13.2M\n",
      " 12 23.5G   12 2909M    0     0  12.8M      0  0:31:20  0:03:46  0:27:34 13.2M\n",
      " 12 23.5G   12 2922M    0     0  12.8M      0  0:31:19  0:03:47  0:27:32 13.2M\n",
      " 12 23.5G   12 2935M    0     0  12.8M      0  0:31:19  0:03:48  0:27:31 13.3M\n",
      " 12 23.5G   12 2949M    0     0  12.8M      0  0:31:19  0:03:49  0:27:30 13.3M\n",
      " 12 23.5G   12 2962M    0     0  12.8M      0  0:31:18  0:03:50  0:27:28 13.4M\n",
      " 12 23.5G   12 2976M    0     0  12.8M      0  0:31:18  0:03:51  0:27:27 13.4M\n",
      " 12 23.5G   12 2989M    0     0  12.8M      0  0:31:17  0:03:52  0:27:25 13.4M\n",
      " 12 23.5G   12 3003M    0     0  12.8M      0  0:31:17  0:03:53  0:27:24 13.4M\n",
      " 12 23.5G   12 3016M    0     0  12.8M      0  0:31:17  0:03:54  0:27:23 13.4M\n",
      " 12 23.5G   12 3029M    0     0  12.8M      0  0:31:17  0:03:55  0:27:22 13.3M\n",
      " 12 23.5G   12 3043M    0     0  12.8M      0  0:31:16  0:03:56  0:27:20 13.3M\n",
      " 12 23.5G   12 3056M    0     0  12.8M      0  0:31:16  0:03:57  0:27:19 13.3M\n",
      " 12 23.5G   12 3069M    0     0  12.8M      0  0:31:16  0:03:58  0:27:18 13.2M\n",
      " 12 23.5G   12 3082M    0     0  12.8M      0  0:31:15  0:03:59  0:27:16 13.3M\n",
      " 12 23.5G   12 3096M    0     0  12.8M      0  0:31:15  0:04:00  0:27:15 13.4M\n",
      " 12 23.5G   12 3109M    0     0  12.8M      0  0:31:15  0:04:01  0:27:14 13.2M\n",
      " 12 23.5G   12 3122M    0     0  12.8M      0  0:31:15  0:04:02  0:27:13 13.2M\n",
      " 13 23.5G   13 3135M    0     0  12.8M      0  0:31:14  0:04:03  0:27:11 13.2M\n",
      " 13 23.5G   13 3149M    0     0  12.8M      0  0:31:14  0:04:04  0:27:10 13.2M\n",
      " 13 23.5G   13 3161M    0     0  12.8M      0  0:31:14  0:04:05  0:27:09 13.0M\n",
      " 13 23.5G   13 3175M    0     0  12.8M      0  0:31:14  0:04:06  0:27:08 13.1M\n",
      " 13 23.5G   13 3188M    0     0  12.8M      0  0:31:14  0:04:07  0:27:07 13.1M\n",
      " 13 23.5G   13 3201M    0     0  12.8M      0  0:31:13  0:04:08  0:27:05 13.1M\n",
      " 13 23.5G   13 3215M    0     0  12.8M      0  0:31:13  0:04:09  0:27:04 13.2M\n",
      " 13 23.5G   13 3227M    0     0  12.8M      0  0:31:13  0:04:10  0:27:03 13.2M\n",
      " 13 23.5G   13 3231M    0     0  12.8M      0  0:31:20  0:04:12  0:27:08 10.7M\n",
      " 13 23.5G   13 3254M    0     0  12.8M      0  0:31:13  0:04:12  0:27:01 13.1M\n",
      " 13 23.5G   13 3266M    0     0  12.8M      0  0:31:14  0:04:14  0:27:00 12.5M\n",
      " 13 23.5G   13 3280M    0     0  12.8M      0  0:31:13  0:04:14  0:26:59 12.9M\n",
      " 13 23.5G   13 3293M    0     0  12.8M      0  0:31:13  0:04:15  0:26:58 13.1M\n",
      " 13 23.5G   13 3306M    0     0  12.8M      0  0:31:12  0:04:16  0:26:56 15.7M\n",
      " 13 23.5G   13 3319M    0     0  12.8M      0  0:31:12  0:04:17  0:26:55 13.1M\n",
      " 13 23.5G   13 3333M    0     0  12.8M      0  0:31:12  0:04:18  0:26:54 13.7M\n",
      " 13 23.5G   13 3346M    0     0  12.8M      0  0:31:12  0:04:19  0:26:53 13.2M\n",
      " 13 23.5G   13 3359M    0     0  12.8M      0  0:31:12  0:04:20  0:26:52 13.2M\n",
      " 13 23.5G   13 3372M    0     0  12.8M      0  0:31:12  0:04:21  0:26:51 13.1M\n",
      " 14 23.5G   14 3385M    0     0  12.8M      0  0:31:11  0:04:22  0:26:49 13.1M\n",
      " 14 23.5G   14 3398M    0     0  12.8M      0  0:31:11  0:04:23  0:26:48 13.1M\n",
      " 14 23.5G   14 3412M    0     0  12.8M      0  0:31:11  0:04:24  0:26:47 13.1M\n",
      " 14 23.5G   14 3425M    0     0  12.8M      0  0:31:11  0:04:25  0:26:46 13.1M\n",
      " 14 23.5G   14 3438M    0     0  12.8M      0  0:31:11  0:04:26  0:26:45 13.1M\n",
      " 14 23.5G   14 3451M    0     0  12.8M      0  0:31:11  0:04:27  0:26:44 13.0M\n",
      " 14 23.5G   14 3464M    0     0  12.8M      0  0:31:11  0:04:28  0:26:43 13.0M\n",
      " 14 23.5G   14 3475M    0     0  12.8M      0  0:31:12  0:04:30  0:26:42 12.5M\n",
      " 14 23.5G   14 3489M    0     0  12.8M      0  0:31:11  0:04:30  0:26:41 12.9M\n",
      " 14 23.5G   14 3502M    0     0  12.8M      0  0:31:11  0:04:31  0:26:40 12.8M\n",
      " 14 23.5G   14 3513M    0     0  12.8M      0  0:31:12  0:04:32  0:26:40 12.5M\n",
      " 14 23.5G   14 3526M    0     0  12.8M      0  0:31:12  0:04:33  0:26:39 12.4M\n",
      " 14 23.5G   14 3539M    0     0  12.8M      0  0:31:12  0:04:34  0:26:38 12.8M\n",
      " 14 23.5G   14 3551M    0     0  12.8M      0  0:31:12  0:04:35  0:26:37 12.3M\n",
      " 14 23.5G   14 3564M    0     0  12.8M      0  0:31:12  0:04:36  0:26:36 12.3M\n",
      " 14 23.5G   14 3576M    0     0  12.8M      0  0:31:13  0:04:37  0:26:36 12.5M\n",
      " 14 23.5G   14 3589M    0     0  12.8M      0  0:31:12  0:04:38  0:26:34 12.6M\n",
      " 14 23.5G   14 3601M    0     0  12.8M      0  0:31:13  0:04:39  0:26:34 12.5M\n",
      " 14 23.5G   14 3614M    0     0  12.8M      0  0:31:13  0:04:40  0:26:33 12.6M\n",
      " 15 23.5G   15 3627M    0     0  12.8M      0  0:31:13  0:04:41  0:26:32 12.7M\n",
      " 15 23.5G   15 3640M    0     0  12.8M      0  0:31:13  0:04:42  0:26:31 12.8M\n",
      " 15 23.5G   15 3653M    0     0  12.8M      0  0:31:13  0:04:43  0:26:30 12.7M\n",
      " 15 23.5G   15 3666M    0     0  12.8M      0  0:31:13  0:04:44  0:26:29 12.9M\n",
      " 15 23.5G   15 3679M    0     0  12.8M      0  0:31:12  0:04:45  0:26:27 12.9M\n",
      " 15 23.5G   15 3692M    0     0  12.8M      0  0:31:12  0:04:46  0:26:26 13.0M\n",
      " 15 23.5G   15 3706M    0     0  12.8M      0  0:31:12  0:04:47  0:26:25 13.1M\n",
      " 15 23.5G   15 3719M    0     0  12.8M      0  0:31:12  0:04:48  0:26:24 13.2M\n",
      " 15 23.5G   15 3730M    0     0  12.8M      0  0:31:13  0:04:50  0:26:23 12.5M\n",
      " 15 23.5G   15 3746M    0     0  12.8M      0  0:31:11  0:04:50  0:26:21 13.3M\n",
      " 15 23.5G   15 3759M    0     0  12.8M      0  0:31:11  0:04:51  0:26:20 13.2M\n",
      " 15 23.5G   15 3772M    0     0  12.8M      0  0:31:11  0:04:52  0:26:19 13.2M\n",
      " 15 23.5G   15 3786M    0     0  12.8M      0  0:31:11  0:04:53  0:26:18 13.3M\n",
      " 15 23.5G   15 3799M    0     0  12.8M      0  0:31:10  0:04:54  0:26:16 14.1M\n",
      " 15 23.5G   15 3812M    0     0  12.8M      0  0:31:10  0:04:55  0:26:15 13.2M\n",
      " 15 23.5G   15 3826M    0     0  12.8M      0  0:31:10  0:04:56  0:26:14 13.3M\n",
      " 15 23.5G   15 3839M    0     0  12.8M      0  0:31:10  0:04:57  0:26:13 13.2M\n",
      " 15 23.5G   15 3852M    0     0  12.8M      0  0:31:10  0:04:58  0:26:12 13.2M\n",
      " 16 23.5G   16 3865M    0     0  12.8M      0  0:31:10  0:04:59  0:26:11 13.0M\n",
      " 16 23.5G   16 3878M    0     0  12.8M      0  0:31:10  0:05:00  0:26:10 13.1M\n",
      " 16 23.5G   16 3891M    0     0  12.8M      0  0:31:10  0:05:01  0:26:09 13.0M\n",
      " 16 23.5G   16 3904M    0     0  12.8M      0  0:31:10  0:05:02  0:26:08 13.0M\n",
      " 16 23.5G   16 3917M    0     0  12.8M      0  0:31:10  0:05:03  0:26:07 12.9M\n",
      " 16 23.5G   16 3929M    0     0  12.8M      0  0:31:10  0:05:04  0:26:06 12.9M\n",
      " 16 23.5G   16 3942M    0     0  12.8M      0  0:31:10  0:05:05  0:26:05 12.9M\n",
      " 16 23.5G   16 3955M    0     0  12.8M      0  0:31:10  0:05:06  0:26:04 12.8M\n",
      " 16 23.5G   16 3968M    0     0  12.8M      0  0:31:10  0:05:07  0:26:03 12.8M\n",
      " 16 23.5G   16 3982M    0     0  12.8M      0  0:31:09  0:05:08  0:26:01 13.0M\n",
      " 16 23.5G   16 3995M    0     0  12.8M      0  0:31:09  0:05:09  0:26:00 13.0M\n",
      " 16 23.5G   16 4008M    0     0  12.8M      0  0:31:09  0:05:10  0:25:59 13.0M\n",
      " 16 23.5G   16 4021M    0     0  12.8M      0  0:31:09  0:05:11  0:25:58 13.2M\n",
      " 16 23.5G   16 4034M    0     0  12.8M      0  0:31:09  0:05:12  0:25:57 13.1M\n",
      " 16 23.5G   16 4048M    0     0  12.8M      0  0:31:09  0:05:13  0:25:56 13.2M\n",
      " 16 23.5G   16 4061M    0     0  12.8M      0  0:31:08  0:05:14  0:25:54 13.2M\n",
      " 16 23.5G   16 4074M    0     0  12.8M      0  0:31:08  0:05:15  0:25:53 13.2M\n",
      " 16 23.5G   16 4087M    0     0  12.8M      0  0:31:08  0:05:16  0:25:52 13.1M\n",
      " 17 23.5G   17 4100M    0     0  12.8M      0  0:31:08  0:05:17  0:25:51 13.2M\n",
      " 17 23.5G   17 4113M    0     0  12.8M      0  0:31:08  0:05:18  0:25:50 13.0M\n",
      " 17 23.5G   17 4126M    0     0  12.8M      0  0:31:08  0:05:19  0:25:49 13.0M\n",
      " 17 23.5G   17 4140M    0     0  12.9M      0  0:31:08  0:05:20  0:25:48 13.2M\n",
      " 17 23.5G   17 4153M    0     0  12.9M      0  0:31:08  0:05:21  0:25:47 13.1M\n",
      " 17 23.5G   17 4166M    0     0  12.9M      0  0:31:07  0:05:22  0:25:45 13.1M\n",
      " 17 23.5G   17 4180M    0     0  12.9M      0  0:31:07  0:05:23  0:25:44 13.3M\n",
      " 17 23.5G   17 4193M    0     0  12.9M      0  0:31:07  0:05:24  0:25:43 13.4M\n",
      " 17 23.5G   17 4207M    0     0  12.9M      0  0:31:07  0:05:25  0:25:42 13.3M\n",
      " 17 23.5G   17 4221M    0     0  12.9M      0  0:31:06  0:05:26  0:25:40 13.4M\n",
      " 17 23.5G   17 4234M    0     0  12.9M      0  0:31:06  0:05:27  0:25:39 13.5M\n",
      " 17 23.5G   17 4247M    0     0  12.9M      0  0:31:06  0:05:28  0:25:38 13.4M\n",
      " 17 23.5G   17 4261M    0     0  12.9M      0  0:31:06  0:05:29  0:25:37 13.4M\n",
      " 17 23.5G   17 4274M    0     0  12.9M      0  0:31:05  0:05:30  0:25:35 13.4M\n",
      " 17 23.5G   17 4287M    0     0  12.9M      0  0:31:05  0:05:31  0:25:34 13.3M\n",
      " 17 23.5G   17 4301M    0     0  12.9M      0  0:31:05  0:05:32  0:25:33 13.3M\n",
      " 17 23.5G   17 4314M    0     0  12.9M      0  0:31:05  0:05:33  0:25:32 13.3M\n",
      " 17 23.5G   17 4328M    0     0  12.9M      0  0:31:05  0:05:34  0:25:31 13.4M\n",
      " 18 23.5G   18 4341M    0     0  12.9M      0  0:31:04  0:05:35  0:25:29 13.3M\n",
      " 18 23.5G   18 4355M    0     0  12.9M      0  0:31:04  0:05:36  0:25:28 13.4M\n",
      " 18 23.5G   18 4368M    0     0  12.9M      0  0:31:04  0:05:37  0:25:27 13.5M\n",
      " 18 23.5G   18 4382M    0     0  12.9M      0  0:31:04  0:05:38  0:25:26 13.4M\n",
      " 18 23.5G   18 4394M    0     0  12.9M      0  0:31:04  0:05:39  0:25:25 13.2M\n",
      " 18 23.5G   18 4407M    0     0  12.9M      0  0:31:04  0:05:40  0:25:24 13.1M\n",
      " 18 23.5G   18 4419M    0     0  12.9M      0  0:31:04  0:05:41  0:25:23 12.9M\n",
      " 18 23.5G   18 4433M    0     0  12.9M      0  0:31:04  0:05:42  0:25:22 12.8M\n",
      " 18 23.5G   18 4446M    0     0  12.9M      0  0:31:04  0:05:43  0:25:21 12.8M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18 23.5G   18 4459M    0     0  12.9M      0  0:31:04  0:05:44  0:25:20 12.9M\n",
      " 18 23.5G   18 4472M    0     0  12.9M      0  0:31:04  0:05:45  0:25:19 13.0M\n",
      " 18 23.5G   18 4485M    0     0  12.9M      0  0:31:04  0:05:46  0:25:18 13.2M\n",
      " 18 23.5G   18 4499M    0     0  12.9M      0  0:31:03  0:05:47  0:25:16 13.1M\n",
      " 18 23.5G   18 4512M    0     0  12.9M      0  0:31:03  0:05:48  0:25:15 13.1M\n",
      " 18 23.5G   18 4525M    0     0  12.9M      0  0:31:03  0:05:49  0:25:14 13.2M\n",
      " 18 23.5G   18 4539M    0     0  12.9M      0  0:31:03  0:05:50  0:25:13 13.3M\n",
      " 18 23.5G   18 4552M    0     0  12.9M      0  0:31:03  0:05:51  0:25:12 13.2M\n",
      " 18 23.5G   18 4565M    0     0  12.9M      0  0:31:03  0:05:52  0:25:11 13.3M\n",
      " 18 23.5G   18 4578M    0     0  12.9M      0  0:31:03  0:05:53  0:25:10 13.3M\n",
      " 19 23.5G   19 4592M    0     0  12.9M      0  0:31:02  0:05:54  0:25:08 13.2M\n",
      " 19 23.5G   19 4605M    0     0  12.9M      0  0:31:02  0:05:55  0:25:07 13.3M\n",
      " 19 23.5G   19 4618M    0     0  12.9M      0  0:31:02  0:05:56  0:25:06 13.3M\n",
      " 19 23.5G   19 4632M    0     0  12.9M      0  0:31:02  0:05:57  0:25:05 13.3M\n",
      " 19 23.5G   19 4645M    0     0  12.9M      0  0:31:02  0:05:58  0:25:04 13.3M\n",
      " 19 23.5G   19 4658M    0     0  12.9M      0  0:31:02  0:05:59  0:25:03 13.3M\n",
      " 19 23.5G   19 4671M    0     0  12.9M      0  0:31:02  0:06:00  0:25:02 13.2M\n",
      " 19 23.5G   19 4684M    0     0  12.9M      0  0:31:02  0:06:01  0:25:01 13.2M\n",
      " 19 23.5G   19 4698M    0     0  12.9M      0  0:31:01  0:06:02  0:24:59 13.1M\n",
      " 19 23.5G   19 4711M    0     0  12.9M      0  0:31:01  0:06:03  0:24:58 13.1M\n",
      " 19 23.5G   19 4723M    0     0  12.9M      0  0:31:01  0:06:04  0:24:57 12.9M\n",
      " 19 23.5G   19 4737M    0     0  12.9M      0  0:31:01  0:06:05  0:24:56 13.0M\n",
      " 19 23.5G   19 4750M    0     0  12.9M      0  0:31:01  0:06:06  0:24:55 13.1M\n",
      " 19 23.5G   19 4762M    0     0  12.9M      0  0:31:02  0:06:08  0:24:54 12.7M\n",
      " 19 23.5G   19 4777M    0     0  12.9M      0  0:31:01  0:06:08  0:24:53 13.1M\n",
      " 19 23.5G   19 4790M    0     0  12.9M      0  0:31:01  0:06:09  0:24:52 13.2M\n",
      " 19 23.5G   19 4803M    0     0  12.9M      0  0:31:01  0:06:10  0:24:51 13.2M\n",
      " 19 23.5G   19 4816M    0     0  12.9M      0  0:31:01  0:06:11  0:24:50 13.2M\n",
      " 20 23.5G   20 4830M    0     0  12.9M      0  0:31:00  0:06:12  0:24:48 13.7M\n",
      " 20 23.5G   20 4842M    0     0  12.9M      0  0:31:00  0:06:13  0:24:47 13.1M\n",
      " 20 23.5G   20 4856M    0     0  12.9M      0  0:31:00  0:06:14  0:24:46 13.2M\n",
      " 20 23.5G   20 4870M    0     0  12.9M      0  0:31:00  0:06:15  0:24:45 13.3M\n",
      " 20 23.5G   20 4883M    0     0  12.9M      0  0:31:00  0:06:16  0:24:44 13.3M\n",
      " 20 23.5G   20 4897M    0     0  12.9M      0  0:31:00  0:06:17  0:24:43 13.3M\n",
      " 20 23.5G   20 4910M    0     0  12.9M      0  0:30:59  0:06:18  0:24:41 13.5M\n",
      " 20 23.5G   20 4923M    0     0  12.9M      0  0:30:59  0:06:19  0:24:40 13.4M\n",
      " 20 23.5G   20 4937M    0     0  12.9M      0  0:30:59  0:06:20  0:24:39 13.4M\n",
      " 20 23.5G   20 4950M    0     0  12.9M      0  0:30:59  0:06:21  0:24:38 13.3M\n",
      " 20 23.5G   20 4964M    0     0  12.9M      0  0:30:59  0:06:22  0:24:37 13.3M\n",
      " 20 23.5G   20 4977M    0     0  12.9M      0  0:30:59  0:06:23  0:24:36 13.3M\n",
      " 20 23.5G   20 4990M    0     0  12.9M      0  0:30:59  0:06:24  0:24:35 13.2M\n",
      " 20 23.5G   20 5003M    0     0  12.9M      0  0:30:59  0:06:25  0:24:34 13.1M\n",
      " 20 23.5G   20 5016M    0     0  12.9M      0  0:30:59  0:06:26  0:24:33 13.1M\n",
      " 20 23.5G   20 5029M    0     0  12.9M      0  0:30:59  0:06:27  0:24:32 13.0M\n",
      " 20 23.5G   20 5042M    0     0  12.9M      0  0:30:59  0:06:28  0:24:31 13.0M\n",
      " 20 23.5G   20 5055M    0     0  12.9M      0  0:30:59  0:06:29  0:24:30 13.0M\n",
      " 21 23.5G   21 5068M    0     0  12.9M      0  0:30:59  0:06:30  0:24:29 13.0M\n",
      " 21 23.5G   21 5081M    0     0  12.9M      0  0:30:59  0:06:31  0:24:28 12.9M\n",
      " 21 23.5G   21 5094M    0     0  12.9M      0  0:30:59  0:06:32  0:24:27 13.0M\n",
      " 21 23.5G   21 5107M    0     0  12.9M      0  0:30:59  0:06:33  0:24:26 12.9M\n",
      " 21 23.5G   21 5120M    0     0  12.9M      0  0:30:58  0:06:34  0:24:24 13.0M\n",
      " 21 23.5G   21 5134M    0     0  12.9M      0  0:30:58  0:06:35  0:24:23 13.1M\n",
      " 21 23.5G   21 5147M    0     0  12.9M      0  0:30:58  0:06:36  0:24:22 13.2M\n",
      " 21 23.5G   21 5160M    0     0  12.9M      0  0:30:58  0:06:37  0:24:21 13.2M\n",
      " 21 23.5G   21 5174M    0     0  12.9M      0  0:30:58  0:06:38  0:24:20 13.4M\n",
      " 21 23.5G   21 5188M    0     0  12.9M      0  0:30:57  0:06:39  0:24:18 13.4M\n",
      " 21 23.5G   21 5201M    0     0  12.9M      0  0:30:57  0:06:40  0:24:17 13.4M\n",
      " 21 23.5G   21 5215M    0     0  12.9M      0  0:30:57  0:06:41  0:24:16 13.4M\n",
      " 21 23.5G   21 5228M    0     0  12.9M      0  0:30:57  0:06:42  0:24:15 13.5M\n",
      " 21 23.5G   21 5241M    0     0  12.9M      0  0:30:57  0:06:43  0:24:14 13.4M\n",
      " 21 23.5G   21 5254M    0     0  12.9M      0  0:30:57  0:06:44  0:24:13 13.3M\n",
      " 21 23.5G   21 5268M    0     0  12.9M      0  0:30:57  0:06:45  0:24:12 13.3M\n",
      " 21 23.5G   21 5281M    0     0  12.9M      0  0:30:57  0:06:46  0:24:11 13.3M\n",
      " 21 23.5G   21 5294M    0     0  12.9M      0  0:30:56  0:06:47  0:24:09 13.3M\n",
      " 22 23.5G   22 5308M    0     0  12.9M      0  0:30:56  0:06:48  0:24:08 13.3M\n",
      " 22 23.5G   22 5321M    0     0  12.9M      0  0:30:56  0:06:49  0:24:07 13.3M\n",
      " 22 23.5G   22 5335M    0     0  12.9M      0  0:30:56  0:06:50  0:24:06 13.3M\n",
      " 22 23.5G   22 5348M    0     0  12.9M      0  0:30:56  0:06:51  0:24:05 13.4M\n",
      " 22 23.5G   22 5362M    0     0  12.9M      0  0:30:56  0:06:52  0:24:04 13.4M\n",
      " 22 23.5G   22 5375M    0     0  12.9M      0  0:30:55  0:06:53  0:24:02 13.4M\n",
      " 22 23.5G   22 5389M    0     0  12.9M      0  0:30:56  0:06:55  0:24:01 13.0M\n",
      " 22 23.5G   22 5402M    0     0  12.9M      0  0:30:55  0:06:55  0:24:00 13.5M\n",
      " 22 23.5G   22 5415M    0     0  12.9M      0  0:30:55  0:06:56  0:23:59 13.4M\n",
      " 22 23.5G   22 5428M    0     0  12.9M      0  0:30:55  0:06:57  0:23:58 13.3M\n",
      " 22 23.5G   22 5442M    0     0  12.9M      0  0:30:55  0:06:58  0:23:57 13.3M\n",
      " 22 23.5G   22 5456M    0     0  12.9M      0  0:30:55  0:06:59  0:23:56 13.8M\n",
      " 22 23.5G   22 5469M    0     0  12.9M      0  0:30:54  0:07:00  0:23:54 13.3M\n",
      " 22 23.5G   22 5482M    0     0  12.9M      0  0:30:54  0:07:01  0:23:53 13.2M\n",
      " 22 23.5G   22 5495M    0     0  12.9M      0  0:30:54  0:07:02  0:23:52 13.3M\n",
      " 22 23.5G   22 5509M    0     0  12.9M      0  0:30:54  0:07:03  0:23:51 13.3M\n",
      " 22 23.5G   22 5522M    0     0  12.9M      0  0:30:54  0:07:04  0:23:50 13.3M\n",
      " 22 23.5G   22 5536M    0     0  12.9M      0  0:30:54  0:07:05  0:23:49 13.3M\n",
      " 23 23.5G   23 5549M    0     0  12.9M      0  0:30:54  0:07:06  0:23:48 13.4M\n",
      " 23 23.5G   23 5562M    0     0  12.9M      0  0:30:54  0:07:07  0:23:47 13.3M\n",
      " 23 23.5G   23 5576M    0     0  12.9M      0  0:30:53  0:07:08  0:23:45 13.4M\n",
      " 23 23.5G   23 5589M    0     0  13.0M      0  0:30:53  0:07:09  0:23:44 13.4M\n",
      " 23 23.5G   23 5603M    0     0  13.0M      0  0:30:53  0:07:10  0:23:43 13.4M\n",
      " 23 23.5G   23 5617M    0     0  13.0M      0  0:30:53  0:07:11  0:23:42 13.4M\n",
      " 23 23.5G   23 5630M    0     0  13.0M      0  0:30:53  0:07:12  0:23:41 13.6M\n",
      " 23 23.5G   23 5643M    0     0  13.0M      0  0:30:53  0:07:13  0:23:40 13.5M\n",
      " 23 23.5G   23 5657M    0     0  13.0M      0  0:30:52  0:07:14  0:23:38 13.5M\n",
      " 23 23.5G   23 5670M    0     0  13.0M      0  0:30:52  0:07:15  0:23:37 13.4M\n",
      " 23 23.5G   23 5684M    0     0  13.0M      0  0:30:52  0:07:16  0:23:36 13.4M\n",
      " 23 23.5G   23 5698M    0     0  13.0M      0  0:30:52  0:07:17  0:23:35 13.5M\n",
      " 23 23.5G   23 5711M    0     0  13.0M      0  0:30:52  0:07:18  0:23:34 13.5M\n",
      " 23 23.5G   23 5725M    0     0  13.0M      0  0:30:51  0:07:19  0:23:32 13.5M\n",
      " 23 23.5G   23 5739M    0     0  13.0M      0  0:30:51  0:07:20  0:23:31 13.6M\n",
      " 23 23.5G   23 5752M    0     0  13.0M      0  0:30:51  0:07:21  0:23:30 13.5M\n",
      " 23 23.5G   23 5765M    0     0  13.0M      0  0:30:51  0:07:22  0:23:29 13.5M\n",
      " 23 23.5G   23 5779M    0     0  13.0M      0  0:30:51  0:07:23  0:23:28 13.4M\n",
      " 24 23.5G   24 5792M    0     0  13.0M      0  0:30:51  0:07:24  0:23:27 13.4M\n",
      " 24 23.5G   24 5806M    0     0  13.0M      0  0:30:51  0:07:25  0:23:26 13.4M\n",
      " 24 23.5G   24 5819M    0     0  13.0M      0  0:30:50  0:07:26  0:23:24 13.4M\n",
      " 24 23.5G   24 5832M    0     0  13.0M      0  0:30:50  0:07:27  0:23:23 13.3M\n",
      " 24 23.5G   24 5846M    0     0  13.0M      0  0:30:50  0:07:28  0:23:22 13.4M\n",
      " 24 23.5G   24 5859M    0     0  13.0M      0  0:30:50  0:07:29  0:23:21 13.4M\n",
      " 24 23.5G   24 5873M    0     0  13.0M      0  0:30:50  0:07:30  0:23:20 13.4M\n",
      " 24 23.5G   24 5886M    0     0  13.0M      0  0:30:50  0:07:31  0:23:19 13.3M\n",
      " 24 23.5G   24 5900M    0     0  13.0M      0  0:30:50  0:07:32  0:23:18 13.4M\n",
      " 24 23.5G   24 5913M    0     0  13.0M      0  0:30:50  0:07:33  0:23:17 13.4M\n",
      " 24 23.5G   24 5927M    0     0  13.0M      0  0:30:49  0:07:34  0:23:15 13.5M\n",
      " 24 23.5G   24 5940M    0     0  13.0M      0  0:30:49  0:07:35  0:23:14 13.4M\n",
      " 24 23.5G   24 5954M    0     0  13.0M      0  0:30:49  0:07:36  0:23:13 13.5M\n",
      " 24 23.5G   24 5967M    0     0  13.0M      0  0:30:49  0:07:37  0:23:12 13.4M\n",
      " 24 23.5G   24 5981M    0     0  13.0M      0  0:30:49  0:07:38  0:23:11 13.5M\n",
      " 24 23.5G   24 5994M    0     0  13.0M      0  0:30:49  0:07:39  0:23:10 13.4M\n",
      " 24 23.5G   24 6008M    0     0  13.0M      0  0:30:48  0:07:40  0:23:08 13.5M\n",
      " 24 23.5G   24 6022M    0     0  13.0M      0  0:30:48  0:07:41  0:23:07 13.6M\n",
      " 25 23.5G   25 6035M    0     0  13.0M      0  0:30:48  0:07:42  0:23:06 13.6M\n",
      " 25 23.5G   25 6049M    0     0  13.0M      0  0:30:48  0:07:43  0:23:05 13.5M\n",
      " 25 23.5G   25 6062M    0     0  13.0M      0  0:30:48  0:07:44  0:23:04 13.5M\n",
      " 25 23.5G   25 6076M    0     0  13.0M      0  0:30:48  0:07:45  0:23:03 13.5M\n",
      " 25 23.5G   25 6089M    0     0  13.0M      0  0:30:48  0:07:46  0:23:02 13.5M\n",
      " 25 23.5G   25 6103M    0     0  13.0M      0  0:30:47  0:07:47  0:23:00 13.4M\n",
      " 25 23.5G   25 6116M    0     0  13.0M      0  0:30:47  0:07:48  0:22:59 13.5M\n",
      " 25 23.5G   25 6129M    0     0  13.0M      0  0:30:47  0:07:49  0:22:58 13.4M\n",
      " 25 23.5G   25 6143M    0     0  13.0M      0  0:30:47  0:07:50  0:22:57 13.4M\n",
      " 25 23.5G   25 6156M    0     0  13.0M      0  0:30:47  0:07:51  0:22:56 13.4M\n",
      " 25 23.5G   25 6170M    0     0  13.0M      0  0:30:47  0:07:52  0:22:55 13.4M\n",
      " 25 23.5G   25 6183M    0     0  13.0M      0  0:30:47  0:07:53  0:22:54 13.4M\n",
      " 25 23.5G   25 6197M    0     0  13.0M      0  0:30:46  0:07:54  0:22:52 13.5M\n",
      " 25 23.5G   25 6210M    0     0  13.0M      0  0:30:46  0:07:55  0:22:51 13.5M\n",
      " 25 23.5G   25 6224M    0     0  13.0M      0  0:30:46  0:07:56  0:22:50 13.5M\n",
      " 25 23.5G   25 6238M    0     0  13.0M      0  0:30:46  0:07:57  0:22:49 13.5M\n",
      " 25 23.5G   25 6249M    0     0  13.0M      0  0:30:47  0:07:59  0:22:48 12.9M\n",
      " 25 23.5G   25 6263M    0     0  13.0M      0  0:30:46  0:07:59  0:22:47 13.2M\n",
      " 26 23.5G   26 6277M    0     0  13.0M      0  0:30:46  0:08:00  0:22:46 13.2M\n",
      " 26 23.5G   26 6290M    0     0  13.0M      0  0:30:46  0:08:01  0:22:45 13.1M\n",
      " 26 23.5G   26 6304M    0     0  13.0M      0  0:30:46  0:08:02  0:22:44 13.2M\n",
      " 26 23.5G   26 6317M    0     0  13.0M      0  0:30:46  0:08:03  0:22:43 13.7M\n",
      " 26 23.5G   26 6330M    0     0  13.0M      0  0:30:46  0:08:04  0:22:42 13.3M\n",
      " 26 23.5G   26 6344M    0     0  13.0M      0  0:30:46  0:08:05  0:22:41 13.3M\n",
      " 26 23.5G   26 6357M    0     0  13.0M      0  0:30:45  0:08:06  0:22:39 13.4M\n",
      " 26 23.5G   26 6370M    0     0  13.0M      0  0:30:45  0:08:07  0:22:38 13.3M\n",
      " 26 23.5G   26 6384M    0     0  13.0M      0  0:30:45  0:08:08  0:22:37 13.3M\n",
      " 26 23.5G   26 6397M    0     0  13.0M      0  0:30:45  0:08:09  0:22:36 13.3M\n",
      " 26 23.5G   26 6411M    0     0  13.0M      0  0:30:45  0:08:10  0:22:35 13.3M\n",
      " 26 23.5G   26 6424M    0     0  13.0M      0  0:30:45  0:08:11  0:22:34 13.4M\n",
      " 26 23.5G   26 6437M    0     0  13.0M      0  0:30:45  0:08:12  0:22:33 13.4M\n",
      " 26 23.5G   26 6451M    0     0  13.0M      0  0:30:45  0:08:13  0:22:32 13.4M\n",
      " 26 23.5G   26 6465M    0     0  13.0M      0  0:30:45  0:08:14  0:22:31 13.5M\n",
      " 26 23.5G   26 6478M    0     0  13.0M      0  0:30:45  0:08:15  0:22:30 13.4M\n",
      " 26 23.5G   26 6491M    0     0  13.0M      0  0:30:45  0:08:16  0:22:29 13.3M\n",
      " 26 23.5G   26 6505M    0     0  13.0M      0  0:30:44  0:08:17  0:22:27 13.4M\n",
      " 27 23.5G   27 6518M    0     0  13.0M      0  0:30:44  0:08:18  0:22:26 13.4M\n",
      " 27 23.5G   27 6531M    0     0  13.0M      0  0:30:44  0:08:19  0:22:25 13.3M\n",
      " 27 23.5G   27 6545M    0     0  13.0M      0  0:30:44  0:08:20  0:22:24 13.3M\n",
      " 27 23.5G   27 6558M    0     0  13.0M      0  0:30:44  0:08:21  0:22:23 13.4M\n",
      " 27 23.5G   27 6572M    0     0  13.0M      0  0:30:44  0:08:22  0:22:22 13.4M\n",
      " 27 23.5G   27 6586M    0     0  13.0M      0  0:30:44  0:08:23  0:22:21 13.4M\n",
      " 27 23.5G   27 6599M    0     0  13.0M      0  0:30:44  0:08:25  0:22:19 13.1M\n",
      " 27 23.5G   27 6612M    0     0  13.0M      0  0:30:43  0:08:25  0:22:18 13.5M\n",
      " 27 23.5G   27 6626M    0     0  13.0M      0  0:30:43  0:08:26  0:22:17 13.4M\n",
      " 27 23.5G   27 6639M    0     0  13.0M      0  0:30:43  0:08:27  0:22:16 13.3M\n",
      " 27 23.5G   27 6652M    0     0  13.0M      0  0:30:43  0:08:28  0:22:15 13.2M\n",
      " 27 23.5G   27 6664M    0     0  13.0M      0  0:30:44  0:08:29  0:22:15 13.4M\n",
      " 27 23.5G   27 6677M    0     0  13.0M      0  0:30:44  0:08:30  0:22:14 12.9M\n",
      " 27 23.5G   27 6690M    0     0  13.0M      0  0:30:44  0:08:31  0:22:13 12.8M\n",
      " 27 23.5G   27 6703M    0     0  13.0M      0  0:30:44  0:08:32  0:22:12 12.8M\n",
      " 27 23.5G   27 6716M    0     0  13.0M      0  0:30:44  0:08:33  0:22:11 12.9M\n",
      " 27 23.5G   27 6728M    0     0  13.0M      0  0:30:44  0:08:34  0:22:10 12.8M\n",
      " 27 23.5G   27 6742M    0     0  13.0M      0  0:30:44  0:08:35  0:22:09 12.8M\n",
      " 28 23.5G   28 6755M    0     0  13.0M      0  0:30:44  0:08:36  0:22:08 12.9M\n",
      " 28 23.5G   28 6768M    0     0  13.0M      0  0:30:44  0:08:37  0:22:07 13.0M\n",
      " 28 23.5G   28 6782M    0     0  13.0M      0  0:30:44  0:08:38  0:22:06 13.1M\n",
      " 28 23.5G   28 6791M    0     0  13.0M      0  0:30:45  0:08:39  0:22:06 12.5M\n",
      " 28 23.5G   28 6803M    0     0  13.0M      0  0:30:45  0:08:40  0:22:05 12.2M\n",
      " 28 23.5G   28 6816M    0     0  13.0M      0  0:30:45  0:08:41  0:22:04 12.2M\n",
      " 28 23.5G   28 6830M    0     0  13.0M      0  0:30:45  0:08:42  0:22:03 12.2M\n",
      " 28 23.5G   28 6843M    0     0  13.0M      0  0:30:45  0:08:43  0:22:02 12.1M\n",
      " 28 23.5G   28 6856M    0     0  13.0M      0  0:30:45  0:08:44  0:22:01 12.9M\n",
      " 28 23.5G   28 6869M    0     0  13.0M      0  0:30:45  0:08:45  0:22:00 13.2M\n",
      " 28 23.5G   28 6882M    0     0  13.0M      0  0:30:45  0:08:46  0:21:59 13.2M\n",
      " 28 23.5G   28 6896M    0     0  13.0M      0  0:30:45  0:08:47  0:21:58 13.1M\n",
      " 28 23.5G   28 6909M    0     0  13.0M      0  0:30:44  0:08:48  0:21:56 13.2M\n",
      " 28 23.5G   28 6923M    0     0  13.0M      0  0:30:44  0:08:49  0:21:55 13.3M\n",
      " 28 23.5G   28 6932M    0     0  13.0M      0  0:30:45  0:08:50  0:21:55 12.5M\n",
      " 28 23.5G   28 6941M    0     0  13.0M      0  0:30:47  0:08:51  0:21:56 11.6M\n",
      " 28 23.5G   28 6949M    0     0  13.0M      0  0:30:48  0:08:52  0:21:56 10.6M\n",
      " 28 23.5G   28 6962M    0     0  13.0M      0  0:30:48  0:08:53  0:21:55 10.5M\n",
      " 28 23.5G   28 6975M    0     0  13.0M      0  0:30:48  0:08:54  0:21:54 10.5M\n",
      " 29 23.5G   29 6989M    0     0  13.0M      0  0:30:48  0:08:55  0:21:53 11.2M\n",
      " 29 23.5G   29 7002M    0     0  13.0M      0  0:30:48  0:08:56  0:21:52 12.2M\n",
      " 29 23.5G   29 7015M    0     0  13.0M      0  0:30:47  0:08:57  0:21:50 13.2M\n",
      " 29 23.5G   29 7028M    0     0  13.0M      0  0:30:47  0:08:58  0:21:49 13.2M\n",
      " 29 23.5G   29 7039M    0     0  13.0M      0  0:30:48  0:08:59  0:21:49 12.7M\n",
      " 29 23.5G   29 7053M    0     0  13.0M      0  0:30:48  0:09:00  0:21:48 12.8M\n",
      " 29 23.5G   29 7066M    0     0  13.0M      0  0:30:48  0:09:01  0:21:47 12.7M\n",
      " 29 23.5G   29 7079M    0     0  13.0M      0  0:30:48  0:09:02  0:21:46 12.7M\n",
      " 29 23.5G   29 7092M    0     0  13.0M      0  0:30:48  0:09:03  0:21:45 12.7M\n",
      " 29 23.5G   29 7105M    0     0  13.0M      0  0:30:48  0:09:04  0:21:44 13.2M\n",
      " 29 23.5G   29 7118M    0     0  13.0M      0  0:30:48  0:09:05  0:21:43 13.0M\n",
      " 29 23.5G   29 7131M    0     0  13.0M      0  0:30:48  0:09:06  0:21:42 12.9M\n",
      " 29 23.5G   29 7144M    0     0  13.0M      0  0:30:48  0:09:07  0:21:41 12.8M\n",
      " 29 23.5G   29 7156M    0     0  13.0M      0  0:30:48  0:09:08  0:21:40 12.8M\n",
      " 29 23.5G   29 7170M    0     0  13.0M      0  0:30:48  0:09:09  0:21:39 12.8M\n",
      " 29 23.5G   29 7180M    0     0  13.0M      0  0:30:49  0:09:10  0:21:39 12.4M\n",
      " 29 23.5G   29 7192M    0     0  13.0M      0  0:30:49  0:09:11  0:21:38 12.3M\n",
      " 29 23.5G   29 7206M    0     0  13.0M      0  0:30:49  0:09:12  0:21:37 12.4M\n",
      " 29 23.5G   29 7218M    0     0  13.0M      0  0:30:49  0:09:13  0:21:36 12.4M\n",
      " 30 23.5G   30 7232M    0     0  13.0M      0  0:30:49  0:09:14  0:21:35 12.4M\n",
      " 30 23.5G   30 7244M    0     0  13.0M      0  0:30:49  0:09:15  0:21:34 12.8M\n",
      " 30 23.5G   30 7258M    0     0  13.0M      0  0:30:49  0:09:16  0:21:33 13.1M\n",
      " 30 23.5G   30 7270M    0     0  13.0M      0  0:30:49  0:09:17  0:21:32 12.9M\n",
      " 30 23.5G   30 7283M    0     0  13.0M      0  0:30:49  0:09:18  0:21:31 12.9M\n",
      " 30 23.5G   30 7295M    0     0  13.0M      0  0:30:49  0:09:19  0:21:30 12.6M\n",
      " 30 23.5G   30 7307M    0     0  13.0M      0  0:30:50  0:09:20  0:21:30 12.5M\n",
      " 30 23.5G   30 7318M    0     0  13.0M      0  0:30:50  0:09:21  0:21:29 11.9M\n",
      " 30 23.5G   30 7330M    0     0  13.0M      0  0:30:50  0:09:22  0:21:28 11.8M\n",
      " 30 23.5G   30 7342M    0     0  13.0M      0  0:30:51  0:09:23  0:21:28 11.7M\n",
      " 30 23.5G   30 7355M    0     0  13.0M      0  0:30:51  0:09:25  0:21:26 11.4M\n",
      " 30 23.5G   30 7366M    0     0  13.0M      0  0:30:51  0:09:25  0:21:26 11.7M\n",
      " 30 23.5G   30 7377M    0     0  13.0M      0  0:30:52  0:09:26  0:21:26 11.8M\n",
      " 30 23.5G   30 7388M    0     0  13.0M      0  0:30:52  0:09:27  0:21:25 11.7M\n",
      " 30 23.5G   30 7401M    0     0  13.0M      0  0:30:52  0:09:28  0:21:24 11.7M\n",
      " 30 23.5G   30 7414M    0     0  13.0M      0  0:30:52  0:09:29  0:21:23 12.2M\n",
      " 30 23.5G   30 7426M    0     0  13.0M      0  0:30:52  0:09:30  0:21:22 12.0M\n",
      " 30 23.5G   30 7435M    0     0  13.0M      0  0:30:53  0:09:31  0:21:22 11.5M\n",
      " 30 23.5G   30 7447M    0     0  12.9M      0  0:30:54  0:09:32  0:21:22 11.7M\n",
      " 30 23.5G   30 7459M    0     0  12.9M      0  0:30:54  0:09:33  0:21:21 11.7M\n",
      " 31 23.5G   31 7471M    0     0  12.9M      0  0:30:54  0:09:34  0:21:20 11.5M\n",
      " 31 23.5G   31 7482M    0     0  12.9M      0  0:30:55  0:09:35  0:21:20 11.3M\n",
      " 31 23.5G   31 7495M    0     0  12.9M      0  0:30:55  0:09:36  0:21:19 11.8M\n",
      " 31 23.5G   31 7507M    0     0  12.9M      0  0:30:55  0:09:37  0:21:18 12.0M\n",
      " 31 23.5G   31 7521M    0     0  12.9M      0  0:30:55  0:09:38  0:21:17 12.2M\n",
      " 31 23.5G   31 7534M    0     0  12.9M      0  0:30:55  0:09:39  0:21:16 12.4M\n",
      " 31 23.5G   31 7547M    0     0  12.9M      0  0:30:55  0:09:40  0:21:15 12.9M\n",
      " 31 23.5G   31 7560M    0     0  12.9M      0  0:30:55  0:09:41  0:21:14 13.0M\n",
      " 31 23.5G   31 7573M    0     0  12.9M      0  0:30:55  0:09:42  0:21:13 13.1M\n",
      " 31 23.5G   31 7586M    0     0  12.9M      0  0:30:55  0:09:43  0:21:12 12.9M\n",
      " 31 23.5G   31 7599M    0     0  12.9M      0  0:30:55  0:09:44  0:21:11 13.0M\n",
      " 31 23.5G   31 7609M    0     0  12.9M      0  0:30:55  0:09:45  0:21:10 12.3M\n",
      " 31 23.5G   31 7618M    0     0  12.9M      0  0:30:56  0:09:46  0:21:10 11.6M\n",
      " 31 23.5G   31 7629M    0     0  12.9M      0  0:30:57  0:09:47  0:21:10 11.2M\n",
      " 31 23.5G   31 7641M    0     0  12.9M      0  0:30:57  0:09:48  0:21:09 11.0M\n",
      " 31 23.5G   31 7654M    0     0  12.9M      0  0:30:57  0:09:49  0:21:08 10.9M\n",
      " 31 23.5G   31 7667M    0     0  12.9M      0  0:30:57  0:09:50  0:21:07 11.6M\n",
      " 31 23.5G   31 7679M    0     0  12.9M      0  0:30:57  0:09:51  0:21:06 12.2M\n",
      " 31 23.5G   31 7693M    0     0  12.9M      0  0:30:57  0:09:52  0:21:05 12.8M\n",
      " 31 23.5G   31 7705M    0     0  12.9M      0  0:30:57  0:09:53  0:21:04 12.8M\n",
      " 32 23.5G   32 7719M    0     0  12.9M      0  0:30:57  0:09:54  0:21:03 13.0M\n",
      " 32 23.5G   32 7731M    0     0  12.9M      0  0:30:57  0:09:55  0:21:02 12.8M\n",
      " 32 23.5G   32 7745M    0     0  12.9M      0  0:30:57  0:09:56  0:21:01 13.0M\n",
      " 32 23.5G   32 7757M    0     0  12.9M      0  0:30:57  0:09:57  0:21:00 12.8M\n",
      " 32 23.5G   32 7770M    0     0  12.9M      0  0:30:57  0:09:58  0:20:59 12.9M\n",
      " 32 23.5G   32 7782M    0     0  12.9M      0  0:30:57  0:09:59  0:20:58 12.6M\n",
      " 32 23.5G   32 7795M    0     0  12.9M      0  0:30:57  0:10:00  0:20:57 12.7M\n",
      " 32 23.5G   32 7807M    0     0  12.9M      0  0:30:58  0:10:01  0:20:57 12.5M\n",
      " 32 23.5G   32 7821M    0     0  12.9M      0  0:30:57  0:10:02  0:20:55 12.7M\n",
      " 32 23.5G   32 7833M    0     0  12.9M      0  0:30:58  0:10:03  0:20:55 12.6M\n",
      " 32 23.5G   32 7846M    0     0  12.9M      0  0:30:58  0:10:04  0:20:54 12.7M\n",
      " 32 23.5G   32 7858M    0     0  12.9M      0  0:30:58  0:10:05  0:20:53 12.6M\n",
      " 32 23.5G   32 7872M    0     0  12.9M      0  0:30:58  0:10:06  0:20:52 12.8M\n",
      " 32 23.5G   32 7885M    0     0  12.9M      0  0:30:58  0:10:07  0:20:51 12.7M\n",
      " 32 23.5G   32 7898M    0     0  12.9M      0  0:30:58  0:10:08  0:20:50 12.9M\n",
      " 32 23.5G   32 7910M    0     0  12.9M      0  0:30:58  0:10:09  0:20:49 12.6M\n",
      " 32 23.5G   32 7922M    0     0  12.9M      0  0:30:58  0:10:10  0:20:48 12.8M\n",
      " 32 23.5G   32 7935M    0     0  12.9M      0  0:30:58  0:10:11  0:20:47 12.5M\n",
      " 32 23.5G   32 7948M    0     0  12.9M      0  0:30:58  0:10:12  0:20:46 12.6M\n",
      " 33 23.5G   33 7960M    0     0  12.9M      0  0:30:58  0:10:13  0:20:45 12.4M\n",
      " 33 23.5G   33 7973M    0     0  12.9M      0  0:30:58  0:10:14  0:20:44 12.6M\n",
      " 33 23.5G   33 7985M    0     0  12.9M      0  0:30:59  0:10:15  0:20:44 12.4M\n",
      " 33 23.5G   33 7998M    0     0  12.9M      0  0:30:58  0:10:16  0:20:42 12.6M\n",
      " 33 23.5G   33 8011M    0     0  12.9M      0  0:30:59  0:10:17  0:20:42 12.5M\n",
      " 33 23.5G   33 8024M    0     0  12.9M      0  0:30:59  0:10:18  0:20:41 12.7M\n",
      " 33 23.5G   33 8037M    0     0  12.9M      0  0:30:58  0:10:19  0:20:39 12.7M\n",
      " 33 23.5G   33 8050M    0     0  12.9M      0  0:30:58  0:10:20  0:20:38 13.0M\n",
      " 33 23.5G   33 8063M    0     0  12.9M      0  0:30:58  0:10:21  0:20:37 12.9M\n",
      " 33 23.5G   33 8076M    0     0  12.9M      0  0:30:58  0:10:22  0:20:36 13.0M\n",
      " 33 23.5G   33 8087M    0     0  12.9M      0  0:30:59  0:10:23  0:20:36 12.5M\n",
      " 33 23.5G   33 8102M    0     0  12.9M      0  0:30:58  0:10:24  0:20:34 12.9M\n",
      " 33 23.5G   33 8114M    0     0  12.9M      0  0:30:59  0:10:25  0:20:34 12.8M\n",
      " 33 23.5G   33 8127M    0     0  12.9M      0  0:30:59  0:10:26  0:20:33 12.7M\n",
      " 33 23.5G   33 8140M    0     0  12.9M      0  0:30:59  0:10:27  0:20:32 12.7M\n",
      " 33 23.5G   33 8153M    0     0  12.9M      0  0:30:59  0:10:28  0:20:31 13.2M\n",
      " 33 23.5G   33 8166M    0     0  12.9M      0  0:30:59  0:10:29  0:20:30 12.7M\n",
      " 33 23.5G   33 8178M    0     0  12.9M      0  0:30:59  0:10:30  0:20:29 12.8M\n",
      " 33 23.5G   33 8191M    0     0  12.9M      0  0:30:59  0:10:31  0:20:28 12.8M\n",
      " 34 23.5G   34 8204M    0     0  12.9M      0  0:30:59  0:10:32  0:20:27 12.8M\n",
      " 34 23.5G   34 8216M    0     0  12.9M      0  0:30:59  0:10:33  0:20:26 12.6M\n",
      " 34 23.5G   34 8229M    0     0  12.9M      0  0:30:59  0:10:34  0:20:25 12.6M\n",
      " 34 23.5G   34 8242M    0     0  12.9M      0  0:30:59  0:10:35  0:20:24 12.6M\n",
      " 34 23.5G   34 8255M    0     0  12.9M      0  0:30:59  0:10:36  0:20:23 12.6M\n",
      " 34 23.5G   34 8268M    0     0  12.9M      0  0:30:59  0:10:37  0:20:22 12.7M\n",
      " 34 23.5G   34 8281M    0     0  12.9M      0  0:30:59  0:10:38  0:20:21 13.0M\n",
      " 34 23.5G   34 8294M    0     0  12.9M      0  0:30:59  0:10:39  0:20:20 12.9M\n",
      " 34 23.5G   34 8307M    0     0  12.9M      0  0:30:59  0:10:40  0:20:19 13.0M\n",
      " 34 23.5G   34 8319M    0     0  12.9M      0  0:30:59  0:10:41  0:20:18 12.9M\n",
      " 34 23.5G   34 8332M    0     0  12.9M      0  0:30:59  0:10:42  0:20:17 12.9M\n",
      " 34 23.5G   34 8345M    0     0  12.9M      0  0:30:59  0:10:43  0:20:16 12.8M\n",
      " 34 23.5G   34 8358M    0     0  12.9M      0  0:30:59  0:10:44  0:20:15 12.9M\n",
      " 34 23.5G   34 8371M    0     0  12.9M      0  0:30:59  0:10:45  0:20:14 12.8M\n",
      " 34 23.5G   34 8384M    0     0  12.9M      0  0:30:59  0:10:46  0:20:13 12.8M\n",
      " 34 23.5G   34 8396M    0     0  12.9M      0  0:30:59  0:10:47  0:20:12 12.7M\n",
      " 34 23.5G   34 8409M    0     0  12.9M      0  0:30:59  0:10:48  0:20:11 12.7M\n",
      " 34 23.5G   34 8422M    0     0  12.9M      0  0:30:59  0:10:49  0:20:10 12.6M\n",
      " 35 23.5G   35 8435M    0     0  12.9M      0  0:30:59  0:10:50  0:20:09 12.7M\n",
      " 35 23.5G   35 8447M    0     0  12.9M      0  0:30:59  0:10:51  0:20:08 12.7M\n",
      " 35 23.5G   35 8460M    0     0  12.9M      0  0:30:59  0:10:52  0:20:07 12.8M\n",
      " 35 23.5G   35 8473M    0     0  12.9M      0  0:30:59  0:10:53  0:20:06 12.8M\n",
      " 35 23.5G   35 8486M    0     0  12.9M      0  0:30:59  0:10:54  0:20:05 12.8M\n",
      " 35 23.5G   35 8499M    0     0  12.9M      0  0:30:59  0:10:55  0:20:04 12.8M\n",
      " 35 23.5G   35 8512M    0     0  12.9M      0  0:30:59  0:10:56  0:20:03 12.9M\n",
      " 35 23.5G   35 8525M    0     0  12.9M      0  0:30:59  0:10:57  0:20:02 12.9M\n",
      " 35 23.5G   35 8538M    0     0  12.9M      0  0:30:59  0:10:58  0:20:01 12.9M\n",
      " 35 23.5G   35 8550M    0     0  12.9M      0  0:31:00  0:10:59  0:20:01 12.8M\n",
      " 35 23.5G   35 8563M    0     0  12.9M      0  0:31:00  0:11:00  0:20:00 12.8M\n",
      " 35 23.5G   35 8576M    0     0  12.9M      0  0:31:00  0:11:01  0:19:59 12.7M\n",
      " 35 23.5G   35 8589M    0     0  12.9M      0  0:31:00  0:11:02  0:19:58 12.7M\n",
      " 35 23.5G   35 8602M    0     0  12.9M      0  0:31:00  0:11:03  0:19:57 12.7M\n",
      " 35 23.5G   35 8615M    0     0  12.9M      0  0:31:00  0:11:04  0:19:56 12.8M\n",
      " 35 23.5G   35 8627M    0     0  12.9M      0  0:31:00  0:11:05  0:19:55 12.7M\n",
      " 35 23.5G   35 8640M    0     0  12.9M      0  0:31:00  0:11:06  0:19:54 12.8M\n",
      " 35 23.5G   35 8653M    0     0  12.9M      0  0:31:00  0:11:07  0:19:53 12.8M\n",
      " 35 23.5G   35 8666M    0     0  12.9M      0  0:31:00  0:11:08  0:19:52 12.7M\n",
      " 36 23.5G   36 8679M    0     0  12.9M      0  0:31:00  0:11:09  0:19:51 12.7M\n",
      " 36 23.5G   36 8692M    0     0  12.9M      0  0:31:00  0:11:10  0:19:50 12.9M\n",
      " 36 23.5G   36 8705M    0     0  12.9M      0  0:31:00  0:11:11  0:19:49 12.9M\n",
      " 36 23.5G   36 8718M    0     0  12.9M      0  0:31:00  0:11:12  0:19:48 13.0M\n",
      " 36 23.5G   36 8731M    0     0  12.9M      0  0:31:00  0:11:13  0:19:47 13.1M\n",
      " 36 23.5G   36 8744M    0     0  12.9M      0  0:31:00  0:11:14  0:19:46 13.0M\n",
      " 36 23.5G   36 8757M    0     0  12.9M      0  0:31:00  0:11:15  0:19:45 13.0M\n",
      " 36 23.5G   36 8767M    0     0  12.9M      0  0:31:00  0:11:17  0:19:43 12.3M\n",
      " 36 23.5G   36 8782M    0     0  12.9M      0  0:31:00  0:11:17  0:19:43 12.6M\n",
      " 36 23.5G   36 8794M    0     0  12.9M      0  0:31:00  0:11:18  0:19:42 12.6M\n",
      " 36 23.5G   36 8807M    0     0  12.9M      0  0:31:00  0:11:19  0:19:41 12.5M\n",
      " 36 23.5G   36 8820M    0     0  12.9M      0  0:31:00  0:11:20  0:19:40 12.5M\n",
      " 36 23.5G   36 8833M    0     0  12.9M      0  0:31:00  0:11:21  0:19:39 13.1M\n",
      " 36 23.5G   36 8846M    0     0  12.9M      0  0:31:00  0:11:22  0:19:38 12.8M\n",
      " 36 23.5G   36 8859M    0     0  12.9M      0  0:31:00  0:11:23  0:19:37 12.9M\n",
      " 36 23.5G   36 8872M    0     0  12.9M      0  0:31:00  0:11:24  0:19:36 13.0M\n",
      " 36 23.5G   36 8885M    0     0  12.9M      0  0:31:00  0:11:25  0:19:35 12.9M\n",
      " 36 23.5G   36 8898M    0     0  12.9M      0  0:31:00  0:11:26  0:19:34 13.0M\n",
      " 36 23.5G   36 8911M    0     0  12.9M      0  0:31:00  0:11:27  0:19:33 12.9M\n",
      " 37 23.5G   37 8923M    0     0  12.9M      0  0:31:00  0:11:28  0:19:32 12.8M\n",
      " 37 23.5G   37 8936M    0     0  12.9M      0  0:31:00  0:11:29  0:19:31 12.8M\n",
      " 37 23.5G   37 8950M    0     0  12.9M      0  0:31:00  0:11:30  0:19:30 12.9M\n",
      " 37 23.5G   37 8962M    0     0  12.9M      0  0:31:00  0:11:31  0:19:29 12.8M\n",
      " 37 23.5G   37 8975M    0     0  12.9M      0  0:31:00  0:11:32  0:19:28 12.9M\n",
      " 37 23.5G   37 8988M    0     0  12.9M      0  0:31:00  0:11:33  0:19:27 12.9M\n",
      " 37 23.5G   37 9001M    0     0  12.9M      0  0:31:00  0:11:34  0:19:26 12.9M\n",
      " 37 23.5G   37 9014M    0     0  12.9M      0  0:31:00  0:11:35  0:19:25 12.8M\n",
      " 37 23.5G   37 9027M    0     0  12.9M      0  0:31:00  0:11:36  0:19:24 12.9M\n",
      " 37 23.5G   37 9040M    0     0  12.9M      0  0:31:00  0:11:37  0:19:23 12.9M\n",
      " 37 23.5G   37 9053M    0     0  12.9M      0  0:31:00  0:11:38  0:19:22 13.0M\n",
      " 37 23.5G   37 9066M    0     0  12.9M      0  0:31:00  0:11:39  0:19:21 13.0M\n",
      " 37 23.5G   37 9079M    0     0  12.9M      0  0:31:00  0:11:40  0:19:20 13.0M\n",
      " 37 23.5G   37 9092M    0     0  12.9M      0  0:31:00  0:11:41  0:19:19 13.0M\n",
      " 37 23.5G   37 9105M    0     0  12.9M      0  0:31:00  0:11:42  0:19:18 13.0M\n",
      " 37 23.5G   37 9118M    0     0  12.9M      0  0:31:00  0:11:43  0:19:17 13.0M\n",
      " 37 23.5G   37 9131M    0     0  12.9M      0  0:31:00  0:11:44  0:19:16 12.9M\n",
      " 37 23.5G   37 9144M    0     0  12.9M      0  0:31:00  0:11:45  0:19:15 13.0M\n",
      " 37 23.5G   37 9157M    0     0  12.9M      0  0:31:00  0:11:46  0:19:14 13.0M\n",
      " 38 23.5G   38 9170M    0     0  12.9M      0  0:31:00  0:11:47  0:19:13 13.0M\n",
      " 38 23.5G   38 9183M    0     0  12.9M      0  0:31:00  0:11:48  0:19:12 12.9M\n",
      " 38 23.5G   38 9196M    0     0  12.9M      0  0:31:00  0:11:49  0:19:11 12.9M\n",
      " 38 23.5G   38 9209M    0     0  12.9M      0  0:31:00  0:11:50  0:19:10 12.9M\n",
      " 38 23.5G   38 9221M    0     0  12.9M      0  0:31:00  0:11:51  0:19:09 12.8M\n",
      " 38 23.5G   38 9233M    0     0  12.9M      0  0:31:00  0:11:52  0:19:08 12.6M\n",
      " 38 23.5G   38 9246M    0     0  12.9M      0  0:31:00  0:11:53  0:19:07 12.4M\n",
      " 38 23.5G   38 9259M    0     0  12.9M      0  0:31:00  0:11:54  0:19:06 12.5M\n",
      " 38 23.5G   38 9271M    0     0  12.9M      0  0:31:01  0:11:55  0:19:06 12.4M\n",
      " 38 23.5G   38 9284M    0     0  12.9M      0  0:31:01  0:11:56  0:19:05 12.5M\n",
      " 38 23.5G   38 9297M    0     0  12.9M      0  0:31:00  0:11:57  0:19:03 12.7M\n",
      " 38 23.5G   38 9311M    0     0  12.9M      0  0:31:00  0:11:58  0:19:02 12.9M\n",
      " 38 23.5G   38 9323M    0     0  12.9M      0  0:31:00  0:11:59  0:19:01 12.9M\n",
      " 38 23.5G   38 9337M    0     0  12.9M      0  0:31:00  0:12:00  0:19:00 13.0M\n",
      " 38 23.5G   38 9349M    0     0  12.9M      0  0:31:00  0:12:01  0:18:59 13.0M\n",
      " 38 23.5G   38 9362M    0     0  12.9M      0  0:31:00  0:12:02  0:18:58 13.0M\n",
      " 38 23.5G   38 9376M    0     0  12.9M      0  0:31:00  0:12:03  0:18:57 13.0M\n",
      " 38 23.5G   38 9389M    0     0  12.9M      0  0:31:00  0:12:04  0:18:56 13.0M\n",
      " 39 23.5G   39 9402M    0     0  12.9M      0  0:31:00  0:12:05  0:18:55 12.9M\n",
      " 39 23.5G   39 9414M    0     0  12.9M      0  0:31:00  0:12:06  0:18:54 12.9M\n",
      " 39 23.5G   39 9427M    0     0  12.9M      0  0:31:00  0:12:07  0:18:53 12.9M\n",
      " 39 23.5G   39 9440M    0     0  12.9M      0  0:31:00  0:12:08  0:18:52 12.9M\n",
      " 39 23.5G   39 9453M    0     0  12.9M      0  0:31:00  0:12:09  0:18:51 12.9M\n",
      " 39 23.5G   39 9467M    0     0  12.9M      0  0:31:00  0:12:10  0:18:50 13.0M\n",
      " 39 23.5G   39 9479M    0     0  12.9M      0  0:31:00  0:12:11  0:18:49 12.9M\n",
      " 39 23.5G   39 9492M    0     0  12.9M      0  0:31:00  0:12:12  0:18:48 12.9M\n",
      " 39 23.5G   39 9505M    0     0  12.9M      0  0:31:00  0:12:13  0:18:47 12.8M\n",
      " 39 23.5G   39 9518M    0     0  12.9M      0  0:31:00  0:12:14  0:18:46 12.8M\n",
      " 39 23.5G   39 9531M    0     0  12.9M      0  0:31:00  0:12:15  0:18:45 12.8M\n",
      " 39 23.5G   39 9543M    0     0  12.9M      0  0:31:00  0:12:16  0:18:44 12.8M\n",
      " 39 23.5G   39 9555M    0     0  12.9M      0  0:31:01  0:12:17  0:18:44 12.5M\n",
      " 39 23.5G   39 9564M    0     0  12.9M      0  0:31:02  0:12:18  0:18:44 11.7M\n",
      " 39 23.5G   39 9573M    0     0  12.9M      0  0:31:02  0:12:19  0:18:43 10.9M\n",
      " 39 23.5G   39 9577M    0     0  12.9M      0  0:31:04  0:12:20  0:18:44 9456k\n",
      " 39 23.5G   39 9585M    0     0  12.9M      0  0:31:05  0:12:21  0:18:44 8485k\n",
      " 39 23.5G   39 9586M    0     0  12.8M      0  0:31:09  0:12:23  0:18:46 5796k\n",
      " 39 23.5G   39 9587M    0     0  12.8M      0  0:31:10  0:12:24  0:18:46 4754k\n",
      " 39 23.5G   39 9588M    0     0  12.8M      0  0:31:13  0:12:25  0:18:48 2800k\n",
      " 39 23.5G   39 9592M    0     0  12.8M      0  0:31:14  0:12:26  0:18:48 3136k\n",
      " 39 23.5G   39 9599M    0     0  12.8M      0  0:31:15  0:12:26  0:18:49 2985k\n",
      " 39 23.5G   39 9607M    0     0  12.8M      0  0:31:16  0:12:27  0:18:49 4872k\n",
      " 39 23.5G   39 9614M    0     0  12.8M      0  0:31:17  0:12:28  0:18:49 5559k\n",
      " 39 23.5G   39 9619M    0     0  12.8M      0  0:31:18  0:12:29  0:18:49 6983k\n",
      " 39 23.5G   39 9625M    0     0  12.8M      0  0:31:20  0:12:30  0:18:50 6681k\n",
      " 39 23.5G   39 9631M    0     0  12.8M      0  0:31:21  0:12:31  0:18:50 6482k\n",
      " 39 23.5G   39 9638M    0     0  12.8M      0  0:31:22  0:12:32  0:18:50 6418k\n",
      " 40 23.5G   40 9646M    0     0  12.7M      0  0:31:23  0:12:33  0:18:50 6616k\n",
      " 40 23.5G   40 9655M    0     0  12.7M      0  0:31:24  0:12:34  0:18:50 7417k\n",
      " 40 23.5G   40 9664M    0     0  12.7M      0  0:31:25  0:12:35  0:18:50 7979k\n",
      " 40 23.5G   40 9673M    0     0  12.7M      0  0:31:25  0:12:36  0:18:49 8527k\n",
      " 40 23.5G   40 9682M    0     0  12.7M      0  0:31:26  0:12:37  0:18:49 8944k\n",
      " 40 23.5G   40 9692M    0     0  12.7M      0  0:31:27  0:12:38  0:18:49 9480k\n",
      " 40 23.5G   40 9703M    0     0  12.7M      0  0:31:27  0:12:39  0:18:48 9918k\n",
      " 40 23.5G   40 9715M    0     0  12.7M      0  0:31:27  0:12:40  0:18:47 10.2M\n",
      " 40 23.5G   40 9726M    0     0  12.7M      0  0:31:27  0:12:41  0:18:46 10.6M\n",
      " 40 23.5G   40 9738M    0     0  12.7M      0  0:31:28  0:12:42  0:18:46 11.2M\n",
      " 40 23.5G   40 9751M    0     0  12.7M      0  0:31:28  0:12:43  0:18:45 11.6M\n",
      " 40 23.5G   40 9763M    0     0  12.7M      0  0:31:28  0:12:44  0:18:44 11.8M\n",
      " 40 23.5G   40 9776M    0     0  12.7M      0  0:31:28  0:12:45  0:18:43 12.2M\n",
      " 40 23.5G   40 9788M    0     0  12.7M      0  0:31:28  0:12:46  0:18:42 12.3M\n",
      " 40 23.5G   40 9801M    0     0  12.7M      0  0:31:28  0:12:47  0:18:41 12.4M\n",
      " 40 23.5G   40 9813M    0     0  12.7M      0  0:31:28  0:12:48  0:18:40 12.5M\n",
      " 40 23.5G   40 9826M    0     0  12.7M      0  0:31:28  0:12:49  0:18:39 12.7M\n",
      " 40 23.5G   40 9839M    0     0  12.7M      0  0:31:28  0:12:50  0:18:38 12.6M\n",
      " 40 23.5G   40 9851M    0     0  12.7M      0  0:31:28  0:12:51  0:18:37 12.6M\n",
      " 40 23.5G   40 9864M    0     0  12.7M      0  0:31:28  0:12:52  0:18:36 12.7M\n",
      " 40 23.5G   40 9877M    0     0  12.7M      0  0:31:28  0:12:53  0:18:35 12.7M\n",
      " 41 23.5G   41 9890M    0     0  12.7M      0  0:31:28  0:12:54  0:18:34 12.7M\n",
      " 41 23.5G   41 9903M    0     0  12.7M      0  0:31:28  0:12:55  0:18:33 12.7M\n",
      " 41 23.5G   41 9915M    0     0  12.7M      0  0:31:28  0:12:57  0:18:31 12.3M\n",
      " 41 23.5G   41 9929M    0     0  12.7M      0  0:31:28  0:12:57  0:18:31 12.8M\n",
      " 41 23.5G   41 9942M    0     0  12.7M      0  0:31:28  0:12:58  0:18:30 13.0M\n",
      " 41 23.5G   41 9954M    0     0  12.7M      0  0:31:28  0:12:59  0:18:29 12.8M\n",
      " 41 23.5G   41 9967M    0     0  12.7M      0  0:31:28  0:13:00  0:18:28 12.9M\n",
      " 41 23.5G   41 9980M    0     0  12.7M      0  0:31:28  0:13:01  0:18:27 13.3M\n",
      " 41 23.5G   41 9993M    0     0  12.7M      0  0:31:28  0:13:02  0:18:26 12.8M\n",
      " 41 23.5G   41  9.7G    0     0  12.7M      0  0:31:28  0:13:03  0:18:25 12.7M\n",
      " 41 23.5G   41  9.7G    0     0  12.7M      0  0:31:28  0:13:04  0:18:24 12.8M\n",
      " 41 23.5G   41  9.7G    0     0  12.7M      0  0:31:28  0:13:05  0:18:23 12.8M\n",
      " 41 23.5G   41  9.8G    0     0  12.7M      0  0:31:28  0:13:06  0:18:22 12.9M\n",
      " 41 23.5G   41  9.8G    0     0  12.7M      0  0:31:28  0:13:07  0:18:21 12.9M\n",
      " 41 23.5G   41  9.8G    0     0  12.7M      0  0:31:27  0:13:08  0:18:19 12.9M\n",
      " 41 23.5G   41  9.8G    0     0  12.7M      0  0:31:27  0:13:09  0:18:18 13.0M\n",
      " 41 23.5G   41  9.8G    0     0  12.7M      0  0:31:27  0:13:10  0:18:17 13.0M\n",
      " 41 23.5G   41  9.8G    0     0  12.7M      0  0:31:27  0:13:11  0:18:16 12.9M\n",
      " 42 23.5G   42  9.8G    0     0  12.7M      0  0:31:27  0:13:12  0:18:15 13.0M\n",
      " 42 23.5G   42  9.8G    0     0  12.7M      0  0:31:27  0:13:13  0:18:14 13.0M\n",
      " 42 23.5G   42  9.9G    0     0  12.7M      0  0:31:28  0:13:15  0:18:13 12.5M\n",
      " 42 23.5G   42  9.9G    0     0  12.7M      0  0:31:27  0:13:15  0:18:12 12.9M\n",
      " 42 23.5G   42  9.9G    0     0  12.7M      0  0:31:27  0:13:16  0:18:11 12.9M\n",
      " 42 23.5G   42  9.9G    0     0  12.7M      0  0:31:27  0:13:17  0:18:10 12.7M\n",
      " 42 23.5G   42  9.9G    0     0  12.7M      0  0:31:27  0:13:18  0:18:09 12.6M\n",
      " 42 23.5G   42  9.9G    0     0  12.7M      0  0:31:27  0:13:19  0:18:08 12.9M\n",
      " 42 23.5G   42  9.9G    0     0  12.7M      0  0:31:27  0:13:20  0:18:07 12.4M\n",
      " 42 23.5G   42  9.9G    0     0  12.7M      0  0:31:28  0:13:21  0:18:07 12.3M\n",
      " 42 23.5G   42 10.0G    0     0  12.7M      0  0:31:28  0:13:22  0:18:06 12.4M\n",
      " 42 23.5G   42 10.0G    0     0  12.7M      0  0:31:28  0:13:23  0:18:05 12.5M\n",
      " 42 23.5G   42 10.0G    0     0  12.7M      0  0:31:28  0:13:24  0:18:04 12.6M\n",
      " 42 23.5G   42 10.0G    0     0  12.7M      0  0:31:28  0:13:25  0:18:03 12.6M\n",
      " 42 23.5G   42 10.0G    0     0  12.7M      0  0:31:28  0:13:26  0:18:02 12.8M\n",
      " 42 23.5G   42 10.0G    0     0  12.7M      0  0:31:28  0:13:27  0:18:01 12.8M\n",
      " 42 23.5G   42 10.0G    0     0  12.7M      0  0:31:27  0:13:28  0:17:59 12.9M\n",
      " 42 23.5G   42 10.0G    0     0  12.7M      0  0:31:27  0:13:29  0:17:58 12.9M\n",
      " 42 23.5G   42 10.1G    0     0  12.7M      0  0:31:27  0:13:30  0:17:57 13.0M\n",
      " 43 23.5G   43 10.1G    0     0  12.7M      0  0:31:27  0:13:31  0:17:56 12.8M\n",
      " 43 23.5G   43 10.1G    0     0  12.7M      0  0:31:27  0:13:32  0:17:55 12.8M\n",
      " 43 23.5G   43 10.1G    0     0  12.7M      0  0:31:27  0:13:33  0:17:54 12.7M\n",
      " 43 23.5G   43 10.1G    0     0  12.7M      0  0:31:28  0:13:34  0:17:54 12.5M\n",
      " 43 23.5G   43 10.1G    0     0  12.7M      0  0:31:28  0:13:35  0:17:53 12.3M\n",
      " 43 23.5G   43 10.1G    0     0  12.7M      0  0:31:28  0:13:36  0:17:52 12.4M\n",
      " 43 23.5G   43 10.1G    0     0  12.7M      0  0:31:28  0:13:37  0:17:51 12.3M\n",
      " 43 23.5G   43 10.2G    0     0  12.7M      0  0:31:28  0:13:38  0:17:50 12.2M\n",
      " 43 23.5G   43 10.2G    0     0  12.7M      0  0:31:28  0:13:39  0:17:49 12.2M\n",
      " 43 23.5G   43 10.2G    0     0  12.7M      0  0:31:28  0:13:40  0:17:48 12.3M\n",
      " 43 23.5G   43 10.2G    0     0  12.7M      0  0:31:28  0:13:41  0:17:47 12.3M\n",
      " 43 23.5G   43 10.2G    0     0  12.7M      0  0:31:28  0:13:42  0:17:46 12.4M\n",
      " 43 23.5G   43 10.2G    0     0  12.7M      0  0:31:28  0:13:43  0:17:45 12.4M\n",
      " 43 23.5G   43 10.2G    0     0  12.7M      0  0:31:28  0:13:44  0:17:44 12.4M\n",
      " 43 23.5G   43 10.2G    0     0  12.7M      0  0:31:28  0:13:45  0:17:43 12.5M\n",
      " 43 23.5G   43 10.3G    0     0  12.7M      0  0:31:28  0:13:46  0:17:42 12.3M\n",
      " 43 23.5G   43 10.3G    0     0  12.7M      0  0:31:28  0:13:47  0:17:41 12.4M\n",
      " 43 23.5G   43 10.3G    0     0  12.7M      0  0:31:28  0:13:48  0:17:40 12.4M\n",
      " 43 23.5G   43 10.3G    0     0  12.7M      0  0:31:28  0:13:49  0:17:39 12.6M\n",
      " 43 23.5G   43 10.3G    0     0  12.7M      0  0:31:28  0:13:50  0:17:38 12.6M\n",
      " 44 23.5G   44 10.3G    0     0  12.7M      0  0:31:28  0:13:51  0:17:37 12.7M\n",
      " 44 23.5G   44 10.3G    0     0  12.7M      0  0:31:28  0:13:52  0:17:36 12.7M\n",
      " 44 23.5G   44 10.3G    0     0  12.7M      0  0:31:28  0:13:53  0:17:35 12.8M\n",
      " 44 23.5G   44 10.4G    0     0  12.7M      0  0:31:28  0:13:54  0:17:34 12.9M\n",
      " 44 23.5G   44 10.4G    0     0  12.7M      0  0:31:28  0:13:55  0:17:33 13.0M\n",
      " 44 23.5G   44 10.4G    0     0  12.7M      0  0:31:28  0:13:56  0:17:32 13.0M\n",
      " 44 23.5G   44 10.4G    0     0  12.7M      0  0:31:28  0:13:57  0:17:31 13.0M\n",
      " 44 23.5G   44 10.4G    0     0  12.7M      0  0:31:28  0:13:58  0:17:30 13.0M\n",
      " 44 23.5G   44 10.4G    0     0  12.7M      0  0:31:28  0:13:59  0:17:29 13.0M\n",
      " 44 23.5G   44 10.4G    0     0  12.7M      0  0:31:28  0:14:00  0:17:28 12.9M\n",
      " 44 23.5G   44 10.4G    0     0  12.7M      0  0:31:28  0:14:01  0:17:27 13.0M\n",
      " 44 23.5G   44 10.5G    0     0  12.7M      0  0:31:28  0:14:02  0:17:26 12.9M\n",
      " 44 23.5G   44 10.5G    0     0  12.7M      0  0:31:28  0:14:03  0:17:25 12.9M\n",
      " 44 23.5G   44 10.5G    0     0  12.7M      0  0:31:28  0:14:04  0:17:24 12.9M\n",
      " 44 23.5G   44 10.5G    0     0  12.7M      0  0:31:28  0:14:05  0:17:23 12.8M\n",
      " 44 23.5G   44 10.5G    0     0  12.7M      0  0:31:28  0:14:06  0:17:22 12.9M\n",
      " 44 23.5G   44 10.5G    0     0  12.7M      0  0:31:28  0:14:07  0:17:21 12.9M\n",
      " 44 23.5G   44 10.5G    0     0  12.7M      0  0:31:28  0:14:08  0:17:20 12.9M\n",
      " 45 23.5G   45 10.5G    0     0  12.7M      0  0:31:28  0:14:09  0:17:19 12.9M\n",
      " 45 23.5G   45 10.6G    0     0  12.7M      0  0:31:28  0:14:10  0:17:18 12.9M\n",
      " 45 23.5G   45 10.6G    0     0  12.7M      0  0:31:28  0:14:11  0:17:17 12.9M\n",
      " 45 23.5G   45 10.6G    0     0  12.7M      0  0:31:28  0:14:12  0:17:16 12.9M\n",
      " 45 23.5G   45 10.6G    0     0  12.7M      0  0:31:27  0:14:13  0:17:14 12.9M\n",
      " 45 23.5G   45 10.6G    0     0  12.7M      0  0:31:27  0:14:14  0:17:13 13.0M\n",
      " 45 23.5G   45 10.6G    0     0  12.7M      0  0:31:27  0:14:15  0:17:12 13.0M\n",
      " 45 23.5G   45 10.6G    0     0  12.7M      0  0:31:27  0:14:16  0:17:11 13.0M\n",
      " 45 23.5G   45 10.6G    0     0  12.7M      0  0:31:27  0:14:17  0:17:10 13.0M\n",
      " 45 23.5G   45 10.7G    0     0  12.7M      0  0:31:27  0:14:18  0:17:09 13.0M\n",
      " 45 23.5G   45 10.7G    0     0  12.7M      0  0:31:27  0:14:19  0:17:08 13.0M\n",
      " 45 23.5G   45 10.7G    0     0  12.7M      0  0:31:27  0:14:20  0:17:07 13.0M\n",
      " 45 23.5G   45 10.7G    0     0  12.7M      0  0:31:27  0:14:21  0:17:06 12.9M\n",
      " 45 23.5G   45 10.7G    0     0  12.7M      0  0:31:27  0:14:22  0:17:05 13.0M\n",
      " 45 23.5G   45 10.7G    0     0  12.7M      0  0:31:27  0:14:23  0:17:04 12.9M\n",
      " 45 23.5G   45 10.7G    0     0  12.7M      0  0:31:27  0:14:24  0:17:03 12.9M\n",
      " 45 23.5G   45 10.7G    0     0  12.7M      0  0:31:27  0:14:25  0:17:02 12.9M\n",
      " 45 23.5G   45 10.8G    0     0  12.7M      0  0:31:27  0:14:26  0:17:01 12.9M\n",
      " 45 23.5G   45 10.8G    0     0  12.7M      0  0:31:27  0:14:27  0:17:00 13.0M\n",
      " 46 23.5G   46 10.8G    0     0  12.7M      0  0:31:27  0:14:28  0:16:59 13.0M\n",
      " 46 23.5G   46 10.8G    0     0  12.7M      0  0:31:27  0:14:29  0:16:58 12.9M\n",
      " 46 23.5G   46 10.8G    0     0  12.7M      0  0:31:27  0:14:30  0:16:57 12.7M\n",
      " 46 23.5G   46 10.8G    0     0  12.7M      0  0:31:27  0:14:31  0:16:56 12.5M\n",
      " 46 23.5G   46 10.8G    0     0  12.7M      0  0:31:27  0:14:32  0:16:55 12.2M\n",
      " 46 23.5G   46 10.8G    0     0  12.7M      0  0:31:28  0:14:33  0:16:55 11.7M\n",
      " 46 23.5G   46 10.9G    0     0  12.7M      0  0:31:28  0:14:34  0:16:54 11.1M\n",
      " 46 23.5G   46 10.9G    0     0  12.7M      0  0:31:29  0:14:35  0:16:54 10.8M\n",
      " 46 23.5G   46 10.9G    0     0  12.7M      0  0:31:29  0:14:36  0:16:53 10.5M\n",
      " 46 23.5G   46 10.9G    0     0  12.7M      0  0:31:29  0:14:37  0:16:52 10.3M\n",
      " 46 23.5G   46 10.9G    0     0  12.7M      0  0:31:29  0:14:38  0:16:51 10.7M\n",
      " 46 23.5G   46 10.9G    0     0  12.7M      0  0:31:29  0:14:39  0:16:50 11.3M\n",
      " 46 23.5G   46 10.9G    0     0  12.7M      0  0:31:29  0:14:40  0:16:49 11.9M\n",
      " 46 23.5G   46 10.9G    0     0  12.7M      0  0:31:29  0:14:41  0:16:48 12.3M\n",
      " 46 23.5G   46 10.9G    0     0  12.7M      0  0:31:29  0:14:42  0:16:47 12.6M\n",
      " 46 23.5G   46 11.0G    0     0  12.7M      0  0:31:29  0:14:43  0:16:46 12.8M\n",
      " 46 23.5G   46 11.0G    0     0  12.7M      0  0:31:29  0:14:44  0:16:45 12.8M\n",
      " 46 23.5G   46 11.0G    0     0  12.7M      0  0:31:29  0:14:45  0:16:44 12.8M\n",
      " 46 23.5G   46 11.0G    0     0  12.7M      0  0:31:29  0:14:46  0:16:43 12.9M\n",
      " 46 23.5G   46 11.0G    0     0  12.7M      0  0:31:29  0:14:47  0:16:42 12.5M\n",
      " 47 23.5G   47 11.0G    0     0  12.7M      0  0:31:29  0:14:48  0:16:41 12.5M\n",
      " 47 23.5G   47 11.0G    0     0  12.7M      0  0:31:29  0:14:49  0:16:40 12.6M\n",
      " 47 23.5G   47 11.0G    0     0  12.7M      0  0:31:29  0:14:50  0:16:39 12.5M\n",
      " 47 23.5G   47 11.1G    0     0  12.7M      0  0:31:29  0:14:51  0:16:38 12.5M\n",
      " 47 23.5G   47 11.1G    0     0  12.7M      0  0:31:29  0:14:52  0:16:37 12.8M\n",
      " 47 23.5G   47 11.1G    0     0  12.7M      0  0:31:30  0:14:53  0:16:37 12.4M\n",
      " 47 23.5G   47 11.1G    0     0  12.7M      0  0:31:29  0:14:54  0:16:35 12.7M\n",
      " 47 23.5G   47 11.1G    0     0  12.7M      0  0:31:29  0:14:55  0:16:34 12.7M\n",
      " 47 23.5G   47 11.1G    0     0  12.7M      0  0:31:29  0:14:56  0:16:33 12.9M\n",
      " 47 23.5G   47 11.1G    0     0  12.7M      0  0:31:29  0:14:57  0:16:32 12.8M\n",
      " 47 23.5G   47 11.1G    0     0  12.7M      0  0:31:29  0:14:58  0:16:31 13.1M\n",
      " 47 23.5G   47 11.2G    0     0  12.7M      0  0:31:29  0:14:59  0:16:30 12.8M\n",
      " 47 23.5G   47 11.2G    0     0  12.7M      0  0:31:29  0:15:00  0:16:29 12.9M\n",
      " 47 23.5G   47 11.2G    0     0  12.7M      0  0:31:29  0:15:01  0:16:28 12.8M\n",
      " 47 23.5G   47 11.2G    0     0  12.7M      0  0:31:29  0:15:02  0:16:27 12.9M\n",
      " 47 23.5G   47 11.2G    0     0  12.7M      0  0:31:29  0:15:03  0:16:26 12.9M\n",
      " 47 23.5G   47 11.2G    0     0  12.7M      0  0:31:29  0:15:04  0:16:25 12.9M\n",
      " 47 23.5G   47 11.2G    0     0  12.7M      0  0:31:29  0:15:05  0:16:24 12.8M\n",
      " 47 23.5G   47 11.2G    0     0  12.7M      0  0:31:29  0:15:06  0:16:23 12.8M\n",
      " 48 23.5G   48 11.3G    0     0  12.7M      0  0:31:29  0:15:07  0:16:22 12.8M\n",
      " 48 23.5G   48 11.3G    0     0  12.7M      0  0:31:29  0:15:08  0:16:21 12.8M\n",
      " 48 23.5G   48 11.3G    0     0  12.7M      0  0:31:29  0:15:09  0:16:20 12.8M\n",
      " 48 23.5G   48 11.3G    0     0  12.7M      0  0:31:29  0:15:10  0:16:19 12.8M\n",
      " 48 23.5G   48 11.3G    0     0  12.7M      0  0:31:29  0:15:11  0:16:18 12.8M\n",
      " 48 23.5G   48 11.3G    0     0  12.7M      0  0:31:29  0:15:12  0:16:17 12.8M\n",
      " 48 23.5G   48 11.3G    0     0  12.7M      0  0:31:29  0:15:13  0:16:16 12.7M\n",
      " 48 23.5G   48 11.3G    0     0  12.7M      0  0:31:29  0:15:14  0:16:15 12.7M\n",
      " 48 23.5G   48 11.4G    0     0  12.7M      0  0:31:29  0:15:15  0:16:14 12.7M\n",
      " 48 23.5G   48 11.4G    0     0  12.7M      0  0:31:29  0:15:16  0:16:13 12.7M\n",
      " 48 23.5G   48 11.4G    0     0  12.7M      0  0:31:29  0:15:17  0:16:12 12.7M\n",
      " 48 23.5G   48 11.4G    0     0  12.7M      0  0:31:29  0:15:18  0:16:11 12.8M\n",
      " 48 23.5G   48 11.4G    0     0  12.7M      0  0:31:29  0:15:19  0:16:10 12.8M\n",
      " 48 23.5G   48 11.4G    0     0  12.7M      0  0:31:29  0:15:20  0:16:09 12.8M\n",
      " 48 23.5G   48 11.4G    0     0  12.7M      0  0:31:29  0:15:21  0:16:08 12.7M\n",
      " 48 23.5G   48 11.4G    0     0  12.7M      0  0:31:29  0:15:22  0:16:07 12.7M\n",
      " 48 23.5G   48 11.5G    0     0  12.7M      0  0:31:29  0:15:23  0:16:06 12.5M\n",
      " 48 23.5G   48 11.5G    0     0  12.7M      0  0:31:29  0:15:24  0:16:05 12.6M\n",
      " 49 23.5G   49 11.5G    0     0  12.7M      0  0:31:29  0:15:25  0:16:04 12.5M\n",
      " 49 23.5G   49 11.5G    0     0  12.7M      0  0:31:29  0:15:26  0:16:03 12.6M\n",
      " 49 23.5G   49 11.5G    0     0  12.7M      0  0:31:29  0:15:27  0:16:02 12.6M\n",
      " 49 23.5G   49 11.5G    0     0  12.7M      0  0:31:29  0:15:28  0:16:01 12.7M\n",
      " 49 23.5G   49 11.5G    0     0  12.7M      0  0:31:29  0:15:29  0:16:00 12.6M\n",
      " 49 23.5G   49 11.5G    0     0  12.7M      0  0:31:29  0:15:30  0:15:59 12.7M\n",
      " 49 23.5G   49 11.6G    0     0  12.7M      0  0:31:29  0:15:31  0:15:58 12.8M\n",
      " 49 23.5G   49 11.6G    0     0  12.7M      0  0:31:29  0:15:32  0:15:57 12.8M\n",
      " 49 23.5G   49 11.6G    0     0  12.7M      0  0:31:29  0:15:33  0:15:56 12.9M\n",
      " 49 23.5G   49 11.6G    0     0  12.7M      0  0:31:29  0:15:34  0:15:55 13.0M\n",
      " 49 23.5G   49 11.6G    0     0  12.7M      0  0:31:29  0:15:35  0:15:54 12.9M\n",
      " 49 23.5G   49 11.6G    0     0  12.7M      0  0:31:29  0:15:36  0:15:53 12.9M\n",
      " 49 23.5G   49 11.6G    0     0  12.7M      0  0:31:29  0:15:37  0:15:52 12.9M\n",
      " 49 23.5G   49 11.6G    0     0  12.7M      0  0:31:29  0:15:38  0:15:51 12.7M\n",
      " 49 23.5G   49 11.7G    0     0  12.7M      0  0:31:29  0:15:39  0:15:50 12.7M\n",
      " 49 23.5G   49 11.7G    0     0  12.7M      0  0:31:29  0:15:40  0:15:49 12.7M\n",
      " 49 23.5G   49 11.7G    0     0  12.7M      0  0:31:29  0:15:41  0:15:48 12.7M\n",
      " 49 23.5G   49 11.7G    0     0  12.7M      0  0:31:29  0:15:42  0:15:47 12.8M\n",
      " 49 23.5G   49 11.7G    0     0  12.7M      0  0:31:29  0:15:43  0:15:46 13.0M\n",
      " 50 23.5G   50 11.7G    0     0  12.7M      0  0:31:29  0:15:44  0:15:45 12.9M\n",
      " 50 23.5G   50 11.7G    0     0  12.7M      0  0:31:29  0:15:45  0:15:44 13.0M\n",
      " 50 23.5G   50 11.7G    0     0  12.7M      0  0:31:29  0:15:46  0:15:43 13.0M\n",
      " 50 23.5G   50 11.8G    0     0  12.7M      0  0:31:29  0:15:47  0:15:42 12.8M\n",
      " 50 23.5G   50 11.8G    0     0  12.7M      0  0:31:29  0:15:48  0:15:41 12.8M\n",
      " 50 23.5G   50 11.8G    0     0  12.7M      0  0:31:29  0:15:49  0:15:40 12.9M\n",
      " 50 23.5G   50 11.8G    0     0  12.7M      0  0:31:29  0:15:50  0:15:39 12.8M\n",
      " 50 23.5G   50 11.8G    0     0  12.7M      0  0:31:29  0:15:51  0:15:38 12.9M\n",
      " 50 23.5G   50 11.8G    0     0  12.7M      0  0:31:28  0:15:52  0:15:36 13.0M\n",
      " 50 23.5G   50 11.8G    0     0  12.7M      0  0:31:28  0:15:53  0:15:35 13.0M\n",
      " 50 23.5G   50 11.8G    0     0  12.7M      0  0:31:28  0:15:54  0:15:34 12.9M\n",
      " 50 23.5G   50 11.9G    0     0  12.7M      0  0:31:28  0:15:55  0:15:33 12.9M\n",
      " 50 23.5G   50 11.9G    0     0  12.7M      0  0:31:28  0:15:56  0:15:32 12.8M\n",
      " 50 23.5G   50 11.9G    0     0  12.7M      0  0:31:28  0:15:57  0:15:31 12.8M\n",
      " 50 23.5G   50 11.9G    0     0  12.7M      0  0:31:28  0:15:58  0:15:30 12.8M\n",
      " 50 23.5G   50 11.9G    0     0  12.7M      0  0:31:28  0:15:59  0:15:29 12.8M\n",
      " 50 23.5G   50 11.9G    0     0  12.7M      0  0:31:28  0:16:00  0:15:28 12.9M\n",
      " 50 23.5G   50 11.9G    0     0  12.7M      0  0:31:28  0:16:01  0:15:27 12.7M\n",
      " 50 23.5G   50 11.9G    0     0  12.7M      0  0:31:29  0:16:02  0:15:27 12.5M\n",
      " 51 23.5G   51 12.0G    0     0  12.7M      0  0:31:29  0:16:03  0:15:26 12.4M\n",
      " 51 23.5G   51 12.0G    0     0  12.7M      0  0:31:29  0:16:04  0:15:25 12.3M\n",
      " 51 23.5G   51 12.0G    0     0  12.7M      0  0:31:29  0:16:05  0:15:24 11.9M\n",
      " 51 23.5G   51 12.0G    0     0  12.7M      0  0:31:29  0:16:06  0:15:23 11.8M\n",
      " 51 23.5G   51 12.0G    0     0  12.7M      0  0:31:29  0:16:07  0:15:22 11.8M\n",
      " 51 23.5G   51 12.0G    0     0  12.7M      0  0:31:29  0:16:08  0:15:21 11.9M\n",
      " 51 23.5G   51 12.0G    0     0  12.7M      0  0:31:29  0:16:09  0:15:20 12.0M\n",
      " 51 23.5G   51 12.0G    0     0  12.7M      0  0:31:29  0:16:10  0:15:19 12.2M\n",
      " 51 23.5G   51 12.1G    0     0  12.7M      0  0:31:29  0:16:11  0:15:18 12.6M\n",
      " 51 23.5G   51 12.1G    0     0  12.7M      0  0:31:29  0:16:12  0:15:17 12.7M\n",
      " 51 23.5G   51 12.1G    0     0  12.7M      0  0:31:29  0:16:13  0:15:16 12.7M\n",
      " 51 23.5G   51 12.1G    0     0  12.7M      0  0:31:29  0:16:14  0:15:15 12.8M\n",
      " 51 23.5G   51 12.1G    0     0  12.7M      0  0:31:29  0:16:15  0:15:14 12.5M\n",
      " 51 23.5G   51 12.1G    0     0  12.7M      0  0:31:29  0:16:16  0:15:13 12.4M\n",
      " 51 23.5G   51 12.1G    0     0  12.7M      0  0:31:30  0:16:17  0:15:13 12.2M\n",
      " 51 23.5G   51 12.1G    0     0  12.7M      0  0:31:30  0:16:18  0:15:12 12.3M\n",
      " 51 23.5G   51 12.2G    0     0  12.7M      0  0:31:30  0:16:19  0:15:11 12.2M\n",
      " 51 23.5G   51 12.2G    0     0  12.7M      0  0:31:30  0:16:20  0:15:10 12.5M\n",
      " 51 23.5G   51 12.2G    0     0  12.7M      0  0:31:30  0:16:21  0:15:09 12.6M\n",
      " 52 23.5G   52 12.2G    0     0  12.7M      0  0:31:30  0:16:22  0:15:08 12.8M\n",
      " 52 23.5G   52 12.2G    0     0  12.7M      0  0:31:30  0:16:23  0:15:07 12.9M\n",
      " 52 23.5G   52 12.2G    0     0  12.7M      0  0:31:29  0:16:24  0:15:05 12.9M\n",
      " 52 23.5G   52 12.2G    0     0  12.7M      0  0:31:30  0:16:25  0:15:05 12.8M\n",
      " 52 23.5G   52 12.2G    0     0  12.7M      0  0:31:30  0:16:26  0:15:04 12.7M\n",
      " 52 23.5G   52 12.3G    0     0  12.7M      0  0:31:30  0:16:27  0:15:03 12.6M\n",
      " 52 23.5G   52 12.3G    0     0  12.7M      0  0:31:30  0:16:28  0:15:02 12.4M\n",
      " 52 23.5G   52 12.3G    0     0  12.7M      0  0:31:30  0:16:29  0:15:01 12.3M\n",
      " 52 23.5G   52 12.3G    0     0  12.7M      0  0:31:30  0:16:30  0:15:00 12.4M\n",
      " 52 23.5G   52 12.3G    0     0  12.7M      0  0:31:30  0:16:31  0:14:59 12.3M\n",
      " 52 23.5G   52 12.3G    0     0  12.7M      0  0:31:30  0:16:32  0:14:58 12.4M\n",
      " 52 23.5G   52 12.3G    0     0  12.7M      0  0:31:30  0:16:33  0:14:57 12.5M\n",
      " 52 23.5G   52 12.3G    0     0  12.7M      0  0:31:30  0:16:34  0:14:56 12.6M\n",
      " 52 23.5G   52 12.4G    0     0  12.7M      0  0:31:30  0:16:35  0:14:55 12.7M\n",
      " 52 23.5G   52 12.4G    0     0  12.7M      0  0:31:30  0:16:36  0:14:54 12.8M\n",
      " 52 23.5G   52 12.4G    0     0  12.7M      0  0:31:30  0:16:37  0:14:53 12.7M\n",
      " 52 23.5G   52 12.4G    0     0  12.7M      0  0:31:30  0:16:38  0:14:52 12.8M\n",
      " 52 23.5G   52 12.4G    0     0  12.7M      0  0:31:30  0:16:39  0:14:51 12.9M\n",
      " 52 23.5G   52 12.4G    0     0  12.7M      0  0:31:30  0:16:40  0:14:50 12.8M\n",
      " 53 23.5G   53 12.4G    0     0  12.7M      0  0:31:30  0:16:41  0:14:49 12.9M\n",
      " 53 23.5G   53 12.4G    0     0  12.7M      0  0:31:30  0:16:42  0:14:48 12.9M\n",
      " 53 23.5G   53 12.5G    0     0  12.7M      0  0:31:30  0:16:43  0:14:47 12.9M\n",
      " 53 23.5G   53 12.5G    0     0  12.7M      0  0:31:30  0:16:44  0:14:46 12.9M\n",
      " 53 23.5G   53 12.5G    0     0  12.7M      0  0:31:30  0:16:45  0:14:45 12.9M\n",
      " 53 23.5G   53 12.5G    0     0  12.7M      0  0:31:30  0:16:46  0:14:44 12.8M\n",
      " 53 23.5G   53 12.5G    0     0  12.7M      0  0:31:30  0:16:47  0:14:43 12.7M\n",
      " 53 23.5G   53 12.5G    0     0  12.7M      0  0:31:30  0:16:48  0:14:42 12.6M\n",
      " 53 23.5G   53 12.5G    0     0  12.7M      0  0:31:30  0:16:49  0:14:41 12.6M\n",
      " 53 23.5G   53 12.5G    0     0  12.7M      0  0:31:30  0:16:50  0:14:40 12.7M\n",
      " 53 23.5G   53 12.6G    0     0  12.7M      0  0:31:30  0:16:51  0:14:39 12.7M\n",
      " 53 23.5G   53 12.6G    0     0  12.7M      0  0:31:30  0:16:52  0:14:38 12.9M\n",
      " 53 23.5G   53 12.6G    0     0  12.7M      0  0:31:30  0:16:53  0:14:37 12.8M\n",
      " 53 23.5G   53 12.6G    0     0  12.7M      0  0:31:30  0:16:54  0:14:36 12.3M\n",
      " 53 23.5G   53 12.6G    0     0  12.7M      0  0:31:30  0:16:55  0:14:35 12.0M\n",
      " 53 23.5G   53 12.6G    0     0  12.7M      0  0:31:30  0:16:56  0:14:34 11.8M\n",
      " 53 23.5G   53 12.6G    0     0  12.7M      0  0:31:30  0:16:57  0:14:33 11.5M\n",
      " 53 23.5G   53 12.6G    0     0  12.7M      0  0:31:30  0:16:58  0:14:32 11.7M\n",
      " 53 23.5G   53 12.6G    0     0  12.7M      0  0:31:30  0:16:59  0:14:31 12.2M\n",
      " 53 23.5G   53 12.7G    0     0  12.7M      0  0:31:30  0:17:00  0:14:30 12.5M\n",
      " 54 23.5G   54 12.7G    0     0  12.7M      0  0:31:30  0:17:01  0:14:29 12.7M\n",
      " 54 23.5G   54 12.7G    0     0  12.7M      0  0:31:30  0:17:02  0:14:28 12.7M\n",
      " 54 23.5G   54 12.7G    0     0  12.7M      0  0:31:30  0:17:03  0:14:27 12.6M\n",
      " 54 23.5G   54 12.7G    0     0  12.7M      0  0:31:30  0:17:04  0:14:26 12.6M\n",
      " 54 23.5G   54 12.7G    0     0  12.7M      0  0:31:30  0:17:05  0:14:25 12.5M\n",
      " 54 23.5G   54 12.7G    0     0  12.7M      0  0:31:30  0:17:06  0:14:24 12.7M\n",
      " 54 23.5G   54 12.7G    0     0  12.7M      0  0:31:30  0:17:07  0:14:23 12.9M\n",
      " 54 23.5G   54 12.8G    0     0  12.7M      0  0:31:30  0:17:08  0:14:22 12.9M\n",
      " 54 23.5G   54 12.8G    0     0  12.7M      0  0:31:31  0:17:10  0:14:21 11.9M\n",
      " 54 23.5G   54 12.8G    0     0  12.7M      0  0:31:32  0:17:11  0:14:21  9.9M\n",
      " 54 23.5G   54 12.8G    0     0  12.7M      0  0:31:33  0:17:11  0:14:22 9788k\n",
      " 54 23.5G   54 12.8G    0     0  12.7M      0  0:31:33  0:17:12  0:14:21 9693k\n",
      " 54 23.5G   54 12.8G    0     0  12.7M      0  0:31:33  0:17:13  0:14:20 9793k\n",
      " 54 23.5G   54 12.8G    0     0  12.7M      0  0:31:32  0:17:14  0:14:18 10.6M\n",
      " 54 23.5G   54 12.8G    0     0  12.7M      0  0:31:32  0:17:15  0:14:17 12.7M\n",
      " 54 23.5G   54 12.8G    0     0  12.7M      0  0:31:32  0:17:16  0:14:16 13.2M\n",
      " 54 23.5G   54 12.9G    0     0  12.7M      0  0:31:32  0:17:17  0:14:15 13.2M\n",
      " 54 23.5G   54 12.9G    0     0  12.7M      0  0:31:32  0:17:18  0:14:14 13.0M\n",
      " 54 23.5G   54 12.9G    0     0  12.7M      0  0:31:32  0:17:19  0:14:13 13.0M\n",
      " 55 23.5G   55 12.9G    0     0  12.7M      0  0:31:32  0:17:20  0:14:12 13.0M\n",
      " 55 23.5G   55 12.9G    0     0  12.7M      0  0:31:32  0:17:21  0:14:11 12.9M\n",
      " 55 23.5G   55 12.9G    0     0  12.7M      0  0:31:32  0:17:22  0:14:10 12.8M\n",
      " 55 23.5G   55 12.9G    0     0  12.7M      0  0:31:32  0:17:23  0:14:09 13.0M\n",
      " 55 23.5G   55 12.9G    0     0  12.7M      0  0:31:32  0:17:24  0:14:08 13.0M\n",
      " 55 23.5G   55 13.0G    0     0  12.7M      0  0:31:32  0:17:25  0:14:07 13.0M\n",
      " 55 23.5G   55 13.0G    0     0  12.7M      0  0:31:32  0:17:26  0:14:06 12.9M\n",
      " 55 23.5G   55 13.0G    0     0  12.7M      0  0:31:32  0:17:27  0:14:05 13.0M\n",
      " 55 23.5G   55 13.0G    0     0  12.7M      0  0:31:32  0:17:28  0:14:04 13.1M\n",
      " 55 23.5G   55 13.0G    0     0  12.7M      0  0:31:32  0:17:29  0:14:03 13.1M\n",
      " 55 23.5G   55 13.0G    0     0  12.7M      0  0:31:32  0:17:30  0:14:02 13.1M\n",
      " 55 23.5G   55 13.0G    0     0  12.7M      0  0:31:32  0:17:31  0:14:01 13.2M\n",
      " 55 23.5G   55 13.0G    0     0  12.7M      0  0:31:32  0:17:32  0:14:00 13.1M\n",
      " 55 23.5G   55 13.1G    0     0  12.7M      0  0:31:32  0:17:33  0:13:59 12.9M\n",
      " 55 23.5G   55 13.1G    0     0  12.7M      0  0:31:32  0:17:34  0:13:58 12.9M\n",
      " 55 23.5G   55 13.1G    0     0  12.7M      0  0:31:31  0:17:35  0:13:56 12.9M\n",
      " 55 23.5G   55 13.1G    0     0  12.7M      0  0:31:31  0:17:36  0:13:55 13.0M\n",
      " 55 23.5G   55 13.1G    0     0  12.7M      0  0:31:31  0:17:37  0:13:54 13.0M\n",
      " 55 23.5G   55 13.1G    0     0  12.7M      0  0:31:31  0:17:38  0:13:53 12.9M\n",
      " 56 23.5G   56 13.1G    0     0  12.7M      0  0:31:31  0:17:39  0:13:52 12.8M\n",
      " 56 23.5G   56 13.1G    0     0  12.7M      0  0:31:31  0:17:40  0:13:51 12.8M\n",
      " 56 23.5G   56 13.2G    0     0  12.7M      0  0:31:31  0:17:41  0:13:50 12.8M\n",
      " 56 23.5G   56 13.2G    0     0  12.7M      0  0:31:31  0:17:42  0:13:49 12.8M\n",
      " 56 23.5G   56 13.2G    0     0  12.7M      0  0:31:31  0:17:43  0:13:48 13.1M\n",
      " 56 23.5G   56 13.2G    0     0  12.7M      0  0:31:31  0:17:44  0:13:47 13.1M\n",
      " 56 23.5G   56 13.2G    0     0  12.7M      0  0:31:31  0:17:45  0:13:46 13.0M\n",
      " 56 23.5G   56 13.2G    0     0  12.7M      0  0:31:31  0:17:46  0:13:45 13.0M\n",
      " 56 23.5G   56 13.2G    0     0  12.7M      0  0:31:31  0:17:47  0:13:44 13.0M\n",
      " 56 23.5G   56 13.3G    0     0  12.7M      0  0:31:31  0:17:48  0:13:43 12.9M\n",
      " 56 23.5G   56 13.3G    0     0  12.7M      0  0:31:31  0:17:49  0:13:42 13.0M\n",
      " 56 23.5G   56 13.3G    0     0  12.7M      0  0:31:31  0:17:50  0:13:41 13.1M\n",
      " 56 23.5G   56 13.3G    0     0  12.7M      0  0:31:31  0:17:51  0:13:40 13.0M\n",
      " 56 23.5G   56 13.3G    0     0  12.7M      0  0:31:31  0:17:52  0:13:39 12.9M\n",
      " 56 23.5G   56 13.3G    0     0  12.7M      0  0:31:31  0:17:53  0:13:38 12.9M\n",
      " 56 23.5G   56 13.3G    0     0  12.7M      0  0:31:31  0:17:54  0:13:37 12.8M\n",
      " 56 23.5G   56 13.3G    0     0  12.7M      0  0:31:31  0:17:55  0:13:36 12.7M\n",
      " 56 23.5G   56 13.4G    0     0  12.7M      0  0:31:31  0:17:56  0:13:35 12.8M\n",
      " 56 23.5G   56 13.4G    0     0  12.7M      0  0:31:31  0:17:57  0:13:34 12.9M\n",
      " 57 23.5G   57 13.4G    0     0  12.7M      0  0:31:31  0:17:58  0:13:33 12.9M\n",
      " 57 23.5G   57 13.4G    0     0  12.7M      0  0:31:31  0:17:59  0:13:32 12.9M\n",
      " 57 23.5G   57 13.4G    0     0  12.7M      0  0:31:31  0:18:01  0:13:30 12.7M\n",
      " 57 23.5G   57 13.4G    0     0  12.7M      0  0:31:31  0:18:01  0:13:30 12.8M\n",
      " 57 23.5G   57 13.4G    0     0  12.7M      0  0:31:31  0:18:02  0:13:29 12.8M\n",
      " 57 23.5G   57 13.4G    0     0  12.7M      0  0:31:31  0:18:03  0:13:28 12.7M\n",
      " 57 23.5G   57 13.5G    0     0  12.7M      0  0:31:31  0:18:04  0:13:27 12.7M\n",
      " 57 23.5G   57 13.5G    0     0  12.7M      0  0:31:31  0:18:05  0:13:26 12.8M\n",
      " 57 23.5G   57 13.5G    0     0  12.7M      0  0:31:31  0:18:06  0:13:25 12.5M\n",
      " 57 23.5G   57 13.5G    0     0  12.7M      0  0:31:31  0:18:08  0:13:23 11.9M\n",
      " 57 23.5G   57 13.5G    0     0  12.7M      0  0:31:31  0:18:08  0:13:23 12.1M\n",
      " 57 23.5G   57 13.5G    0     0  12.7M      0  0:31:31  0:18:09  0:13:22 12.1M\n",
      " 57 23.5G   57 13.5G    0     0  12.7M      0  0:31:31  0:18:10  0:13:21 12.0M\n",
      " 57 23.5G   57 13.5G    0     0  12.7M      0  0:31:31  0:18:11  0:13:20 12.2M\n",
      " 57 23.5G   57 13.5G    0     0  12.7M      0  0:31:31  0:18:12  0:13:19 12.8M\n",
      " 57 23.5G   57 13.6G    0     0  12.7M      0  0:31:31  0:18:13  0:13:18 12.5M\n",
      " 57 23.5G   57 13.6G    0     0  12.7M      0  0:31:31  0:18:14  0:13:17 12.6M\n",
      " 57 23.5G   57 13.6G    0     0  12.7M      0  0:31:31  0:18:15  0:13:16 12.8M\n",
      " 57 23.5G   57 13.6G    0     0  12.7M      0  0:31:31  0:18:16  0:13:15 12.8M\n",
      " 58 23.5G   58 13.6G    0     0  12.7M      0  0:31:31  0:18:17  0:13:14 12.8M\n",
      " 58 23.5G   58 13.6G    0     0  12.7M      0  0:31:31  0:18:18  0:13:13 12.8M\n",
      " 58 23.5G   58 13.6G    0     0  12.7M      0  0:31:31  0:18:19  0:13:12 12.8M\n",
      " 58 23.5G   58 13.6G    0     0  12.7M      0  0:31:31  0:18:20  0:13:11 12.8M\n",
      " 58 23.5G   58 13.7G    0     0  12.7M      0  0:31:31  0:18:21  0:13:10 12.9M\n",
      " 58 23.5G   58 13.7G    0     0  12.7M      0  0:31:31  0:18:22  0:13:09 13.0M\n",
      " 58 23.5G   58 13.7G    0     0  12.7M      0  0:31:31  0:18:23  0:13:08 13.1M\n",
      " 58 23.5G   58 13.7G    0     0  12.7M      0  0:31:31  0:18:24  0:13:07 13.1M\n",
      " 58 23.5G   58 13.7G    0     0  12.7M      0  0:31:31  0:18:25  0:13:06 13.1M\n",
      " 58 23.5G   58 13.7G    0     0  12.7M      0  0:31:31  0:18:26  0:13:05 13.1M\n",
      " 58 23.5G   58 13.7G    0     0  12.7M      0  0:31:31  0:18:27  0:13:04 13.1M\n",
      " 58 23.5G   58 13.8G    0     0  12.7M      0  0:31:31  0:18:28  0:13:03 13.0M\n",
      " 58 23.5G   58 13.8G    0     0  12.7M      0  0:31:31  0:18:29  0:13:02 12.9M\n",
      " 58 23.5G   58 13.8G    0     0  12.7M      0  0:31:31  0:18:30  0:13:01 12.8M\n",
      " 58 23.5G   58 13.8G    0     0  12.7M      0  0:31:31  0:18:31  0:13:00 12.7M\n",
      " 58 23.5G   58 13.8G    0     0  12.7M      0  0:31:31  0:18:32  0:12:59 12.7M\n",
      " 58 23.5G   58 13.8G    0     0  12.7M      0  0:31:31  0:18:33  0:12:58 12.7M\n",
      " 58 23.5G   58 13.8G    0     0  12.7M      0  0:31:31  0:18:34  0:12:57 12.8M\n",
      " 59 23.5G   59 13.8G    0     0  12.7M      0  0:31:31  0:18:35  0:12:56 13.0M\n",
      " 59 23.5G   59 13.9G    0     0  12.7M      0  0:31:30  0:18:36  0:12:54 13.1M\n",
      " 59 23.5G   59 13.9G    0     0  12.7M      0  0:31:30  0:18:37  0:12:53 13.2M\n",
      " 59 23.5G   59 13.9G    0     0  12.7M      0  0:31:30  0:18:38  0:12:52 13.2M\n",
      " 59 23.5G   59 13.9G    0     0  12.7M      0  0:31:30  0:18:39  0:12:51 13.2M\n",
      " 59 23.5G   59 13.9G    0     0  12.7M      0  0:31:30  0:18:40  0:12:50 13.2M\n",
      " 59 23.5G   59 13.9G    0     0  12.7M      0  0:31:30  0:18:41  0:12:49 13.3M\n",
      " 59 23.5G   59 13.9G    0     0  12.7M      0  0:31:30  0:18:42  0:12:48 13.3M\n",
      " 59 23.5G   59 13.9G    0     0  12.7M      0  0:31:30  0:18:43  0:12:47 13.3M\n",
      " 59 23.5G   59 14.0G    0     0  12.7M      0  0:31:30  0:18:44  0:12:46 13.2M\n",
      " 59 23.5G   59 14.0G    0     0  12.7M      0  0:31:30  0:18:45  0:12:45 13.2M\n",
      " 59 23.5G   59 14.0G    0     0  12.7M      0  0:31:30  0:18:46  0:12:44 13.1M\n",
      " 59 23.5G   59 14.0G    0     0  12.7M      0  0:31:30  0:18:47  0:12:43 13.1M\n",
      " 59 23.5G   59 14.0G    0     0  12.7M      0  0:31:30  0:18:48  0:12:42 13.1M\n",
      " 59 23.5G   59 14.0G    0     0  12.7M      0  0:31:30  0:18:49  0:12:41 13.1M\n",
      " 59 23.5G   59 14.0G    0     0  12.7M      0  0:31:30  0:18:50  0:12:40 13.2M\n",
      " 59 23.5G   59 14.0G    0     0  12.7M      0  0:31:29  0:18:51  0:12:38 13.2M\n",
      " 59 23.5G   59 14.1G    0     0  12.7M      0  0:31:29  0:18:52  0:12:37 13.1M\n",
      " 60 23.5G   60 14.1G    0     0  12.7M      0  0:31:29  0:18:53  0:12:36 13.1M\n",
      " 60 23.5G   60 14.1G    0     0  12.7M      0  0:31:29  0:18:54  0:12:35 13.0M\n",
      " 60 23.5G   60 14.1G    0     0  12.7M      0  0:31:29  0:18:55  0:12:34 12.8M\n",
      " 60 23.5G   60 14.1G    0     0  12.7M      0  0:31:29  0:18:56  0:12:33 12.8M\n",
      " 60 23.5G   60 14.1G    0     0  12.7M      0  0:31:29  0:18:57  0:12:32 12.7M\n",
      " 60 23.5G   60 14.1G    0     0  12.7M      0  0:31:29  0:18:58  0:12:31 12.8M\n",
      " 60 23.5G   60 14.1G    0     0  12.7M      0  0:31:29  0:18:59  0:12:30 12.9M\n",
      " 60 23.5G   60 14.2G    0     0  12.7M      0  0:31:29  0:19:00  0:12:29 13.0M\n",
      " 60 23.5G   60 14.2G    0     0  12.7M      0  0:31:29  0:19:01  0:12:28 12.9M\n",
      " 60 23.5G   60 14.2G    0     0  12.7M      0  0:31:29  0:19:02  0:12:27 13.0M\n",
      " 60 23.5G   60 14.2G    0     0  12.7M      0  0:31:29  0:19:03  0:12:26 12.9M\n",
      " 60 23.5G   60 14.2G    0     0  12.7M      0  0:31:29  0:19:04  0:12:25 12.9M\n",
      " 60 23.5G   60 14.2G    0     0  12.7M      0  0:31:29  0:19:05  0:12:24 12.8M\n",
      " 60 23.5G   60 14.2G    0     0  12.7M      0  0:31:29  0:19:06  0:12:23 13.0M\n",
      " 60 23.5G   60 14.2G    0     0  12.7M      0  0:31:29  0:19:07  0:12:22 13.0M\n",
      " 60 23.5G   60 14.3G    0     0  12.7M      0  0:31:29  0:19:08  0:12:21 12.9M\n",
      " 60 23.5G   60 14.3G    0     0  12.7M      0  0:31:29  0:19:09  0:12:20 12.8M\n",
      " 60 23.5G   60 14.3G    0     0  12.7M      0  0:31:29  0:19:10  0:12:19 12.8M\n",
      " 60 23.5G   60 14.3G    0     0  12.7M      0  0:31:29  0:19:11  0:12:18 12.7M\n",
      " 61 23.5G   61 14.3G    0     0  12.7M      0  0:31:29  0:19:12  0:12:17 12.6M\n",
      " 61 23.5G   61 14.3G    0     0  12.7M      0  0:31:29  0:19:13  0:12:16 12.5M\n",
      " 61 23.5G   61 14.3G    0     0  12.7M      0  0:31:29  0:19:14  0:12:15 12.7M\n",
      " 61 23.5G   61 14.3G    0     0  12.7M      0  0:31:29  0:19:15  0:12:14 12.8M\n",
      " 61 23.5G   61 14.4G    0     0  12.7M      0  0:31:29  0:19:16  0:12:13 12.8M\n",
      " 61 23.5G   61 14.4G    0     0  12.7M      0  0:31:29  0:19:17  0:12:12 12.9M\n",
      " 61 23.5G   61 14.4G    0     0  12.7M      0  0:31:29  0:19:18  0:12:11 13.0M\n",
      " 61 23.5G   61 14.4G    0     0  12.7M      0  0:31:29  0:19:19  0:12:10 12.7M\n",
      " 61 23.5G   61 14.4G    0     0  12.7M      0  0:31:29  0:19:20  0:12:09 12.8M\n",
      " 61 23.5G   61 14.4G    0     0  12.7M      0  0:31:29  0:19:21  0:12:08 12.9M\n",
      " 61 23.5G   61 14.4G    0     0  12.7M      0  0:31:29  0:19:22  0:12:07 12.8M\n",
      " 61 23.5G   61 14.4G    0     0  12.7M      0  0:31:29  0:19:23  0:12:06 12.8M\n",
      " 61 23.5G   61 14.5G    0     0  12.7M      0  0:31:29  0:19:24  0:12:05 13.2M\n",
      " 61 23.5G   61 14.5G    0     0  12.7M      0  0:31:29  0:19:25  0:12:04 13.1M\n",
      " 61 23.5G   61 14.5G    0     0  12.7M      0  0:31:29  0:19:26  0:12:03 13.1M\n",
      " 61 23.5G   61 14.5G    0     0  12.7M      0  0:31:29  0:19:27  0:12:02 13.1M\n",
      " 61 23.5G   61 14.5G    0     0  12.7M      0  0:31:29  0:19:28  0:12:01 13.1M\n",
      " 61 23.5G   61 14.5G    0     0  12.7M      0  0:31:29  0:19:29  0:12:00 13.0M\n",
      " 61 23.5G   61 14.5G    0     0  12.7M      0  0:31:29  0:19:30  0:11:59 13.0M\n",
      " 62 23.5G   62 14.6G    0     0  12.7M      0  0:31:29  0:19:31  0:11:58 12.9M\n",
      " 62 23.5G   62 14.6G    0     0  12.7M      0  0:31:29  0:19:32  0:11:57 12.9M\n",
      " 62 23.5G   62 14.6G    0     0  12.7M      0  0:31:29  0:19:33  0:11:56 12.9M\n",
      " 62 23.5G   62 14.6G    0     0  12.7M      0  0:31:28  0:19:34  0:11:54 12.9M\n",
      " 62 23.5G   62 14.6G    0     0  12.7M      0  0:31:28  0:19:35  0:11:53 12.9M\n",
      " 62 23.5G   62 14.6G    0     0  12.7M      0  0:31:28  0:19:36  0:11:52 13.0M\n",
      " 62 23.5G   62 14.6G    0     0  12.7M      0  0:31:28  0:19:37  0:11:51 13.0M\n",
      " 62 23.5G   62 14.6G    0     0  12.7M      0  0:31:28  0:19:38  0:11:50 13.0M\n",
      " 62 23.5G   62 14.7G    0     0  12.7M      0  0:31:28  0:19:39  0:11:49 12.9M\n",
      " 62 23.5G   62 14.7G    0     0  12.7M      0  0:31:28  0:19:40  0:11:48 12.9M\n",
      " 62 23.5G   62 14.7G    0     0  12.7M      0  0:31:28  0:19:41  0:11:47 12.8M\n",
      " 62 23.5G   62 14.7G    0     0  12.7M      0  0:31:28  0:19:42  0:11:46 12.9M\n",
      " 62 23.5G   62 14.7G    0     0  12.7M      0  0:31:28  0:19:43  0:11:45 12.9M\n",
      " 62 23.5G   62 14.7G    0     0  12.7M      0  0:31:28  0:19:44  0:11:44 13.1M\n",
      " 62 23.5G   62 14.7G    0     0  12.7M      0  0:31:28  0:19:45  0:11:43 13.1M\n",
      " 62 23.5G   62 14.7G    0     0  12.7M      0  0:31:28  0:19:46  0:11:42 13.2M\n",
      " 62 23.5G   62 14.8G    0     0  12.7M      0  0:31:28  0:19:47  0:11:41 13.1M\n",
      " 62 23.5G   62 14.8G    0     0  12.7M      0  0:31:28  0:19:48  0:11:40 13.2M\n",
      " 63 23.5G   63 14.8G    0     0  12.7M      0  0:31:28  0:19:49  0:11:39 13.1M\n",
      " 63 23.5G   63 14.8G    0     0  12.7M      0  0:31:28  0:19:50  0:11:38 13.1M\n",
      " 63 23.5G   63 14.8G    0     0  12.7M      0  0:31:28  0:19:51  0:11:37 13.1M\n",
      " 63 23.5G   63 14.8G    0     0  12.7M      0  0:31:28  0:19:52  0:11:36 13.1M\n",
      " 63 23.5G   63 14.8G    0     0  12.7M      0  0:31:28  0:19:53  0:11:35 12.9M\n",
      " 63 23.5G   63 14.8G    0     0  12.7M      0  0:31:28  0:19:54  0:11:34 12.9M\n",
      " 63 23.5G   63 14.9G    0     0  12.7M      0  0:31:28  0:19:55  0:11:33 12.9M\n",
      " 63 23.5G   63 14.9G    0     0  12.7M      0  0:31:28  0:19:56  0:11:32 12.9M\n",
      " 63 23.5G   63 14.9G    0     0  12.7M      0  0:31:28  0:19:57  0:11:31 12.9M\n",
      " 63 23.5G   63 14.9G    0     0  12.7M      0  0:31:28  0:19:58  0:11:30 13.0M\n",
      " 63 23.5G   63 14.9G    0     0  12.7M      0  0:31:28  0:19:59  0:11:29 13.0M\n",
      " 63 23.5G   63 14.9G    0     0  12.7M      0  0:31:28  0:20:00  0:11:28 13.1M\n",
      " 63 23.5G   63 14.9G    0     0  12.7M      0  0:31:28  0:20:01  0:11:27 13.0M\n",
      " 63 23.5G   63 14.9G    0     0  12.7M      0  0:31:27  0:20:02  0:11:25 13.1M\n",
      " 63 23.5G   63 15.0G    0     0  12.7M      0  0:31:27  0:20:03  0:11:24 13.1M\n",
      " 63 23.5G   63 15.0G    0     0  12.7M      0  0:31:27  0:20:04  0:11:23 13.1M\n",
      " 63 23.5G   63 15.0G    0     0  12.7M      0  0:31:27  0:20:05  0:11:22 13.1M\n",
      " 63 23.5G   63 15.0G    0     0  12.7M      0  0:31:27  0:20:06  0:11:21 13.3M\n",
      " 63 23.5G   63 15.0G    0     0  12.7M      0  0:31:27  0:20:07  0:11:20 13.3M\n",
      " 64 23.5G   64 15.0G    0     0  12.7M      0  0:31:27  0:20:08  0:11:19 13.3M\n",
      " 64 23.5G   64 15.0G    0     0  12.7M      0  0:31:27  0:20:09  0:11:18 13.2M\n",
      " 64 23.5G   64 15.0G    0     0  12.7M      0  0:31:27  0:20:10  0:11:17 13.3M\n",
      " 64 23.5G   64 15.1G    0     0  12.7M      0  0:31:27  0:20:11  0:11:16 13.2M\n",
      " 64 23.5G   64 15.1G    0     0  12.7M      0  0:31:27  0:20:12  0:11:15 13.1M\n",
      " 64 23.5G   64 15.1G    0     0  12.7M      0  0:31:27  0:20:13  0:11:14 13.1M\n",
      " 64 23.5G   64 15.1G    0     0  12.7M      0  0:31:27  0:20:14  0:11:13 13.2M\n",
      " 64 23.5G   64 15.1G    0     0  12.7M      0  0:31:27  0:20:15  0:11:12 13.1M\n",
      " 64 23.5G   64 15.1G    0     0  12.7M      0  0:31:27  0:20:16  0:11:11 13.1M\n",
      " 64 23.5G   64 15.1G    0     0  12.7M      0  0:31:27  0:20:17  0:11:10 13.1M\n",
      " 64 23.5G   64 15.2G    0     0  12.7M      0  0:31:27  0:20:18  0:11:09 13.0M\n",
      " 64 23.5G   64 15.2G    0     0  12.7M      0  0:31:27  0:20:19  0:11:08 12.9M\n",
      " 64 23.5G   64 15.2G    0     0  12.7M      0  0:31:27  0:20:20  0:11:07 12.7M\n",
      " 64 23.5G   64 15.2G    0     0  12.7M      0  0:31:27  0:20:21  0:11:06 12.9M\n",
      " 64 23.5G   64 15.2G    0     0  12.7M      0  0:31:27  0:20:22  0:11:05 12.8M\n",
      " 64 23.5G   64 15.2G    0     0  12.7M      0  0:31:27  0:20:23  0:11:04 12.8M\n",
      " 64 23.5G   64 15.2G    0     0  12.7M      0  0:31:26  0:20:24  0:11:02 13.0M\n",
      " 64 23.5G   64 15.2G    0     0  12.7M      0  0:31:26  0:20:25  0:11:01 13.3M\n",
      " 65 23.5G   65 15.3G    0     0  12.7M      0  0:31:26  0:20:26  0:11:00 13.0M\n",
      " 65 23.5G   65 15.3G    0     0  12.7M      0  0:31:26  0:20:27  0:10:59 13.0M\n",
      " 65 23.5G   65 15.3G    0     0  12.7M      0  0:31:26  0:20:28  0:10:58 13.0M\n",
      " 65 23.5G   65 15.3G    0     0  12.7M      0  0:31:26  0:20:29  0:10:57 12.8M\n",
      " 65 23.5G   65 15.3G    0     0  12.7M      0  0:31:26  0:20:30  0:10:56 12.8M\n",
      " 65 23.5G   65 15.3G    0     0  12.7M      0  0:31:26  0:20:31  0:10:55 12.9M\n",
      " 65 23.5G   65 15.3G    0     0  12.7M      0  0:31:26  0:20:32  0:10:54 12.9M\n",
      " 65 23.5G   65 15.3G    0     0  12.7M      0  0:31:26  0:20:33  0:10:53 12.9M\n",
      " 65 23.5G   65 15.4G    0     0  12.7M      0  0:31:26  0:20:34  0:10:52 13.0M\n",
      " 65 23.5G   65 15.4G    0     0  12.7M      0  0:31:26  0:20:35  0:10:51 13.1M\n",
      " 65 23.5G   65 15.4G    0     0  12.7M      0  0:31:26  0:20:36  0:10:50 13.1M\n",
      " 65 23.5G   65 15.4G    0     0  12.7M      0  0:31:26  0:20:37  0:10:49 13.1M\n",
      " 65 23.5G   65 15.4G    0     0  12.7M      0  0:31:26  0:20:38  0:10:48 13.1M\n",
      " 65 23.5G   65 15.4G    0     0  12.7M      0  0:31:26  0:20:39  0:10:47 13.1M\n",
      " 65 23.5G   65 15.4G    0     0  12.7M      0  0:31:26  0:20:40  0:10:46 13.1M\n",
      " 65 23.5G   65 15.4G    0     0  12.7M      0  0:31:26  0:20:41  0:10:45 13.1M\n",
      " 65 23.5G   65 15.5G    0     0  12.7M      0  0:31:26  0:20:42  0:10:44 13.2M\n",
      " 65 23.5G   65 15.5G    0     0  12.7M      0  0:31:26  0:20:43  0:10:43 13.2M\n",
      " 66 23.5G   66 15.5G    0     0  12.7M      0  0:31:26  0:20:44  0:10:42 13.2M\n",
      " 66 23.5G   66 15.5G    0     0  12.7M      0  0:31:26  0:20:45  0:10:41 13.1M\n",
      " 66 23.5G   66 15.5G    0     0  12.7M      0  0:31:26  0:20:46  0:10:40 12.9M\n",
      " 66 23.5G   66 15.5G    0     0  12.7M      0  0:31:26  0:20:47  0:10:39 12.9M\n",
      " 66 23.5G   66 15.5G    0     0  12.7M      0  0:31:26  0:20:48  0:10:38 12.9M\n",
      " 66 23.5G   66 15.5G    0     0  12.7M      0  0:31:26  0:20:49  0:10:37 12.9M\n",
      " 66 23.5G   66 15.6G    0     0  12.7M      0  0:31:26  0:20:50  0:10:36 13.1M\n",
      " 66 23.5G   66 15.6G    0     0  12.7M      0  0:31:25  0:20:51  0:10:34 13.2M\n",
      " 66 23.5G   66 15.6G    0     0  12.7M      0  0:31:25  0:20:52  0:10:33 13.2M\n",
      " 66 23.5G   66 15.6G    0     0  12.7M      0  0:31:25  0:20:53  0:10:32 13.1M\n",
      " 66 23.5G   66 15.6G    0     0  12.7M      0  0:31:25  0:20:54  0:10:31 13.0M\n",
      " 66 23.5G   66 15.6G    0     0  12.7M      0  0:31:26  0:20:56  0:10:30 12.4M\n",
      " 66 23.5G   66 15.6G    0     0  12.7M      0  0:31:25  0:20:56  0:10:29 12.8M\n",
      " 66 23.5G   66 15.6G    0     0  12.7M      0  0:31:25  0:20:57  0:10:28 12.8M\n",
      " 66 23.5G   66 15.7G    0     0  12.7M      0  0:31:25  0:20:58  0:10:27 12.8M\n",
      " 66 23.5G   66 15.7G    0     0  12.7M      0  0:31:25  0:20:59  0:10:26 13.0M\n",
      " 66 23.5G   66 15.7G    0     0  12.7M      0  0:31:25  0:21:00  0:10:25 13.5M\n",
      " 66 23.5G   66 15.7G    0     0  12.7M      0  0:31:25  0:21:01  0:10:24 12.9M\n",
      " 66 23.5G   66 15.7G    0     0  12.7M      0  0:31:25  0:21:02  0:10:23 12.9M\n",
      " 67 23.5G   67 15.7G    0     0  12.7M      0  0:31:25  0:21:03  0:10:22 12.8M\n",
      " 67 23.5G   67 15.7G    0     0  12.7M      0  0:31:25  0:21:04  0:10:21 12.8M\n",
      " 67 23.5G   67 15.7G    0     0  12.7M      0  0:31:25  0:21:05  0:10:20 12.7M\n",
      " 67 23.5G   67 15.8G    0     0  12.7M      0  0:31:25  0:21:06  0:10:19 12.7M\n",
      " 67 23.5G   67 15.8G    0     0  12.7M      0  0:31:25  0:21:07  0:10:18 12.6M\n",
      " 67 23.5G   67 15.8G    0     0  12.7M      0  0:31:25  0:21:08  0:10:17 12.6M\n",
      " 67 23.5G   67 15.8G    0     0  12.7M      0  0:31:25  0:21:09  0:10:16 12.5M\n",
      " 67 23.5G   67 15.8G    0     0  12.7M      0  0:31:25  0:21:10  0:10:15 12.6M\n",
      " 67 23.5G   67 15.8G    0     0  12.7M      0  0:31:25  0:21:11  0:10:14 12.7M\n",
      " 67 23.5G   67 15.8G    0     0  12.7M      0  0:31:25  0:21:12  0:10:13 12.7M\n",
      " 67 23.5G   67 15.8G    0     0  12.7M      0  0:31:25  0:21:13  0:10:12 12.7M\n",
      " 67 23.5G   67 15.9G    0     0  12.7M      0  0:31:25  0:21:14  0:10:11 12.7M\n",
      " 67 23.5G   67 15.9G    0     0  12.7M      0  0:31:25  0:21:15  0:10:10 12.7M\n",
      " 67 23.5G   67 15.9G    0     0  12.7M      0  0:31:25  0:21:16  0:10:09 12.6M\n",
      " 67 23.5G   67 15.9G    0     0  12.7M      0  0:31:25  0:21:17  0:10:08 12.7M\n",
      " 67 23.5G   67 15.9G    0     0  12.7M      0  0:31:25  0:21:18  0:10:07 12.8M\n",
      " 67 23.5G   67 15.9G    0     0  12.7M      0  0:31:25  0:21:19  0:10:06 12.8M\n",
      " 67 23.5G   67 15.9G    0     0  12.7M      0  0:31:25  0:21:20  0:10:05 12.8M\n",
      " 67 23.5G   67 15.9G    0     0  12.7M      0  0:31:25  0:21:21  0:10:04 13.0M\n",
      " 68 23.5G   68 16.0G    0     0  12.7M      0  0:31:25  0:21:22  0:10:03 13.0M\n",
      " 68 23.5G   68 16.0G    0     0  12.7M      0  0:31:25  0:21:23  0:10:02 13.1M\n",
      " 68 23.5G   68 16.0G    0     0  12.7M      0  0:31:25  0:21:24  0:10:01 13.0M\n",
      " 68 23.5G   68 16.0G    0     0  12.7M      0  0:31:25  0:21:25  0:10:00 13.2M\n",
      " 68 23.5G   68 16.0G    0     0  12.7M      0  0:31:25  0:21:26  0:09:59 13.2M\n",
      " 68 23.5G   68 16.0G    0     0  12.7M      0  0:31:25  0:21:27  0:09:58 13.1M\n",
      " 68 23.5G   68 16.0G    0     0  12.7M      0  0:31:25  0:21:28  0:09:57 13.0M\n",
      " 68 23.5G   68 16.1G    0     0  12.7M      0  0:31:25  0:21:29  0:09:56 13.0M\n",
      " 68 23.5G   68 16.1G    0     0  12.7M      0  0:31:25  0:21:30  0:09:55 13.0M\n",
      " 68 23.5G   68 16.1G    0     0  12.7M      0  0:31:25  0:21:31  0:09:54 12.9M\n",
      " 68 23.5G   68 16.1G    0     0  12.7M      0  0:31:25  0:21:32  0:09:53 13.0M\n",
      " 68 23.5G   68 16.1G    0     0  12.7M      0  0:31:25  0:21:33  0:09:52 13.1M\n",
      " 68 23.5G   68 16.1G    0     0  12.7M      0  0:31:25  0:21:34  0:09:51 13.2M\n",
      " 68 23.5G   68 16.1G    0     0  12.7M      0  0:31:25  0:21:35  0:09:50 13.1M\n",
      " 68 23.5G   68 16.1G    0     0  12.7M      0  0:31:25  0:21:36  0:09:49 13.2M\n",
      " 68 23.5G   68 16.2G    0     0  12.7M      0  0:31:25  0:21:37  0:09:48 13.0M\n",
      " 68 23.5G   68 16.2G    0     0  12.7M      0  0:31:25  0:21:38  0:09:47 12.9M\n",
      " 68 23.5G   68 16.2G    0     0  12.7M      0  0:31:25  0:21:39  0:09:46 12.8M\n",
      " 69 23.5G   69 16.2G    0     0  12.7M      0  0:31:25  0:21:40  0:09:45 12.8M\n",
      " 69 23.5G   69 16.2G    0     0  12.7M      0  0:31:25  0:21:41  0:09:44 12.9M\n",
      " 69 23.5G   69 16.2G    0     0  12.7M      0  0:31:25  0:21:42  0:09:43 12.9M\n",
      " 69 23.5G   69 16.2G    0     0  12.7M      0  0:31:25  0:21:43  0:09:42 12.9M\n",
      " 69 23.5G   69 16.2G    0     0  12.7M      0  0:31:25  0:21:44  0:09:41 12.9M\n",
      " 69 23.5G   69 16.3G    0     0  12.7M      0  0:31:25  0:21:45  0:09:40 12.9M\n",
      " 69 23.5G   69 16.3G    0     0  12.7M      0  0:31:25  0:21:46  0:09:39 12.8M\n",
      " 69 23.5G   69 16.3G    0     0  12.7M      0  0:31:25  0:21:47  0:09:38 13.0M\n",
      " 69 23.5G   69 16.3G    0     0  12.7M      0  0:31:24  0:21:48  0:09:36 13.1M\n",
      " 69 23.5G   69 16.3G    0     0  12.7M      0  0:31:24  0:21:49  0:09:35 13.2M\n",
      " 69 23.5G   69 16.3G    0     0  12.7M      0  0:31:24  0:21:50  0:09:34 13.2M\n",
      " 69 23.5G   69 16.3G    0     0  12.7M      0  0:31:24  0:21:51  0:09:33 13.3M\n",
      " 69 23.5G   69 16.3G    0     0  12.7M      0  0:31:24  0:21:52  0:09:32 13.2M\n",
      " 69 23.5G   69 16.4G    0     0  12.7M      0  0:31:24  0:21:53  0:09:31 13.1M\n",
      " 69 23.5G   69 16.4G    0     0  12.7M      0  0:31:24  0:21:54  0:09:30 13.1M\n",
      " 69 23.5G   69 16.4G    0     0  12.7M      0  0:31:24  0:21:55  0:09:29 13.1M\n",
      " 69 23.5G   69 16.4G    0     0  12.7M      0  0:31:24  0:21:56  0:09:28 13.1M\n",
      " 69 23.5G   69 16.4G    0     0  12.7M      0  0:31:24  0:21:57  0:09:27 13.1M\n",
      " 69 23.5G   69 16.4G    0     0  12.7M      0  0:31:24  0:21:58  0:09:26 13.2M\n",
      " 70 23.5G   70 16.4G    0     0  12.7M      0  0:31:24  0:21:59  0:09:25 13.1M\n",
      " 70 23.5G   70 16.4G    0     0  12.7M      0  0:31:24  0:22:00  0:09:24 13.2M\n",
      " 70 23.5G   70 16.5G    0     0  12.7M      0  0:31:24  0:22:01  0:09:23 13.2M\n",
      " 70 23.5G   70 16.5G    0     0  12.7M      0  0:31:24  0:22:02  0:09:22 13.2M\n",
      " 70 23.5G   70 16.5G    0     0  12.7M      0  0:31:24  0:22:03  0:09:21 13.2M\n",
      " 70 23.5G   70 16.5G    0     0  12.7M      0  0:31:24  0:22:04  0:09:20 13.2M\n",
      " 70 23.5G   70 16.5G    0     0  12.7M      0  0:31:24  0:22:05  0:09:19 13.0M\n",
      " 70 23.5G   70 16.5G    0     0  12.7M      0  0:31:24  0:22:06  0:09:18 13.1M\n",
      " 70 23.5G   70 16.5G    0     0  12.7M      0  0:31:24  0:22:07  0:09:17 13.1M\n",
      " 70 23.5G   70 16.6G    0     0  12.7M      0  0:31:24  0:22:08  0:09:16 13.1M\n",
      " 70 23.5G   70 16.6G    0     0  12.7M      0  0:31:23  0:22:09  0:09:14 13.1M\n",
      " 70 23.5G   70 16.6G    0     0  12.7M      0  0:31:23  0:22:10  0:09:13 13.2M\n",
      " 70 23.5G   70 16.6G    0     0  12.7M      0  0:31:23  0:22:11  0:09:12 13.2M\n",
      " 70 23.5G   70 16.6G    0     0  12.7M      0  0:31:23  0:22:12  0:09:11 13.2M\n",
      " 70 23.5G   70 16.6G    0     0  12.7M      0  0:31:23  0:22:13  0:09:10 13.2M\n",
      " 70 23.5G   70 16.6G    0     0  12.7M      0  0:31:23  0:22:14  0:09:09 13.3M\n",
      " 70 23.5G   70 16.6G    0     0  12.7M      0  0:31:23  0:22:15  0:09:08 13.2M\n",
      " 70 23.5G   70 16.7G    0     0  12.7M      0  0:31:23  0:22:16  0:09:07 13.1M\n",
      " 71 23.5G   71 16.7G    0     0  12.7M      0  0:31:23  0:22:17  0:09:06 13.0M\n",
      " 71 23.5G   71 16.7G    0     0  12.7M      0  0:31:23  0:22:18  0:09:05 13.0M\n",
      " 71 23.5G   71 16.7G    0     0  12.7M      0  0:31:23  0:22:19  0:09:04 13.1M\n",
      " 71 23.5G   71 16.7G    0     0  12.7M      0  0:31:23  0:22:20  0:09:03 13.1M\n",
      " 71 23.5G   71 16.7G    0     0  12.7M      0  0:31:23  0:22:21  0:09:02 13.1M\n",
      " 71 23.5G   71 16.7G    0     0  12.7M      0  0:31:23  0:22:22  0:09:01 13.0M\n",
      " 71 23.5G   71 16.7G    0     0  12.7M      0  0:31:23  0:22:23  0:09:00 12.9M\n",
      " 71 23.5G   71 16.8G    0     0  12.7M      0  0:31:23  0:22:24  0:08:59 12.8M\n",
      " 71 23.5G   71 16.8G    0     0  12.7M      0  0:31:23  0:22:25  0:08:58 12.6M\n",
      " 71 23.5G   71 16.8G    0     0  12.7M      0  0:31:23  0:22:26  0:08:57 12.6M\n",
      " 71 23.5G   71 16.8G    0     0  12.7M      0  0:31:23  0:22:27  0:08:56 12.7M\n",
      " 71 23.5G   71 16.8G    0     0  12.7M      0  0:31:23  0:22:28  0:08:55 12.8M\n",
      " 71 23.5G   71 16.8G    0     0  12.7M      0  0:31:23  0:22:29  0:08:54 12.8M\n",
      " 71 23.5G   71 16.8G    0     0  12.7M      0  0:31:23  0:22:30  0:08:53 12.9M\n",
      " 71 23.5G   71 16.8G    0     0  12.7M      0  0:31:23  0:22:31  0:08:52 12.8M\n",
      " 71 23.5G   71 16.9G    0     0  12.7M      0  0:31:23  0:22:32  0:08:51 12.9M\n",
      " 71 23.5G   71 16.9G    0     0  12.7M      0  0:31:23  0:22:33  0:08:50 12.8M\n",
      " 71 23.5G   71 16.9G    0     0  12.7M      0  0:31:23  0:22:34  0:08:49 12.8M\n",
      " 71 23.5G   71 16.9G    0     0  12.7M      0  0:31:23  0:22:35  0:08:48 12.7M\n",
      " 72 23.5G   72 16.9G    0     0  12.7M      0  0:31:23  0:22:36  0:08:47 12.6M\n",
      " 72 23.5G   72 16.9G    0     0  12.7M      0  0:31:23  0:22:37  0:08:46 12.4M\n",
      " 72 23.5G   72 16.9G    0     0  12.7M      0  0:31:23  0:22:38  0:08:45 12.4M\n",
      " 72 23.5G   72 16.9G    0     0  12.7M      0  0:31:23  0:22:39  0:08:44 12.4M\n",
      " 72 23.5G   72 17.0G    0     0  12.7M      0  0:31:23  0:22:40  0:08:43 12.2M\n",
      " 72 23.5G   72 17.0G    0     0  12.7M      0  0:31:23  0:22:41  0:08:42 12.4M\n",
      " 72 23.5G   72 17.0G    0     0  12.7M      0  0:31:23  0:22:42  0:08:41 12.5M\n",
      " 72 23.5G   72 17.0G    0     0  12.7M      0  0:31:23  0:22:43  0:08:40 12.5M\n",
      " 72 23.5G   72 17.0G    0     0  12.7M      0  0:31:23  0:22:44  0:08:39 12.5M\n",
      " 72 23.5G   72 17.0G    0     0  12.7M      0  0:31:23  0:22:45  0:08:38 12.7M\n",
      " 72 23.5G   72 17.0G    0     0  12.7M      0  0:31:23  0:22:46  0:08:37 12.7M\n",
      " 72 23.5G   72 17.0G    0     0  12.7M      0  0:31:23  0:22:47  0:08:36 12.8M\n",
      " 72 23.5G   72 17.1G    0     0  12.7M      0  0:31:23  0:22:48  0:08:35 13.0M\n",
      " 72 23.5G   72 17.1G    0     0  12.7M      0  0:31:23  0:22:49  0:08:34 13.1M\n",
      " 72 23.5G   72 17.1G    0     0  12.7M      0  0:31:23  0:22:50  0:08:33 13.1M\n",
      " 72 23.5G   72 17.1G    0     0  12.7M      0  0:31:23  0:22:51  0:08:32 13.1M\n",
      " 72 23.5G   72 17.1G    0     0  12.7M      0  0:31:23  0:22:52  0:08:31 13.1M\n",
      " 72 23.5G   72 17.1G    0     0  12.7M      0  0:31:23  0:22:53  0:08:30 13.0M\n",
      " 73 23.5G   73 17.1G    0     0  12.7M      0  0:31:23  0:22:54  0:08:29 13.0M\n",
      " 73 23.5G   73 17.1G    0     0  12.7M      0  0:31:23  0:22:55  0:08:28 13.1M\n",
      " 73 23.5G   73 17.2G    0     0  12.7M      0  0:31:23  0:22:56  0:08:27 13.0M\n",
      " 73 23.5G   73 17.2G    0     0  12.7M      0  0:31:23  0:22:57  0:08:26 12.9M\n",
      " 73 23.5G   73 17.2G    0     0  12.7M      0  0:31:23  0:22:58  0:08:25 12.9M\n",
      " 73 23.5G   73 17.2G    0     0  12.7M      0  0:31:23  0:22:59  0:08:24 12.7M\n",
      " 73 23.5G   73 17.2G    0     0  12.7M      0  0:31:23  0:23:00  0:08:23 12.7M\n",
      " 73 23.5G   73 17.2G    0     0  12.7M      0  0:31:23  0:23:01  0:08:22 12.7M\n",
      " 73 23.5G   73 17.2G    0     0  12.7M      0  0:31:23  0:23:02  0:08:21 12.7M\n",
      " 73 23.5G   73 17.2G    0     0  12.7M      0  0:31:23  0:23:03  0:08:20 12.8M\n",
      " 73 23.5G   73 17.3G    0     0  12.7M      0  0:31:23  0:23:04  0:08:19 12.9M\n",
      " 73 23.5G   73 17.3G    0     0  12.7M      0  0:31:23  0:23:05  0:08:18 12.9M\n",
      " 73 23.5G   73 17.3G    0     0  12.7M      0  0:31:23  0:23:06  0:08:17 13.0M\n",
      " 73 23.5G   73 17.3G    0     0  12.7M      0  0:31:23  0:23:07  0:08:16 13.1M\n",
      " 73 23.5G   73 17.3G    0     0  12.7M      0  0:31:23  0:23:08  0:08:15 13.1M\n",
      " 73 23.5G   73 17.3G    0     0  12.7M      0  0:31:23  0:23:09  0:08:14 13.2M\n",
      " 73 23.5G   73 17.3G    0     0  12.7M      0  0:31:23  0:23:10  0:08:13 13.2M\n",
      " 73 23.5G   73 17.3G    0     0  12.7M      0  0:31:23  0:23:11  0:08:12 12.9M\n",
      " 73 23.5G   73 17.4G    0     0  12.7M      0  0:31:23  0:23:12  0:08:11 13.0M\n",
      " 74 23.5G   74 17.4G    0     0  12.7M      0  0:31:23  0:23:13  0:08:10 12.9M\n",
      " 74 23.5G   74 17.4G    0     0  12.7M      0  0:31:23  0:23:14  0:08:09 12.9M\n",
      " 74 23.5G   74 17.4G    0     0  12.7M      0  0:31:23  0:23:15  0:08:08 12.9M\n",
      " 74 23.5G   74 17.4G    0     0  12.7M      0  0:31:23  0:23:16  0:08:07 13.1M\n",
      " 74 23.5G   74 17.4G    0     0  12.7M      0  0:31:22  0:23:17  0:08:05 13.1M\n",
      " 74 23.5G   74 17.4G    0     0  12.7M      0  0:31:22  0:23:18  0:08:04 13.2M\n",
      " 74 23.5G   74 17.4G    0     0  12.7M      0  0:31:22  0:23:19  0:08:03 13.1M\n",
      " 74 23.5G   74 17.5G    0     0  12.7M      0  0:31:22  0:23:20  0:08:02 13.0M\n",
      " 74 23.5G   74 17.5G    0     0  12.8M      0  0:31:22  0:23:21  0:08:01 13.1M\n",
      " 74 23.5G   74 17.5G    0     0  12.8M      0  0:31:22  0:23:22  0:08:00 13.1M\n",
      " 74 23.5G   74 17.5G    0     0  12.8M      0  0:31:22  0:23:23  0:07:59 13.0M\n",
      " 74 23.5G   74 17.5G    0     0  12.8M      0  0:31:22  0:23:24  0:07:58 13.1M\n",
      " 74 23.5G   74 17.5G    0     0  12.8M      0  0:31:22  0:23:25  0:07:57 13.1M\n",
      " 74 23.5G   74 17.5G    0     0  12.8M      0  0:31:22  0:23:26  0:07:56 13.0M\n",
      " 74 23.5G   74 17.6G    0     0  12.8M      0  0:31:22  0:23:27  0:07:55 13.0M\n",
      " 74 23.5G   74 17.6G    0     0  12.8M      0  0:31:22  0:23:28  0:07:54 13.0M\n",
      " 74 23.5G   74 17.6G    0     0  12.8M      0  0:31:22  0:23:29  0:07:53 13.0M\n",
      " 74 23.5G   74 17.6G    0     0  12.8M      0  0:31:22  0:23:30  0:07:52 13.0M\n",
      " 75 23.5G   75 17.6G    0     0  12.8M      0  0:31:22  0:23:31  0:07:51 13.0M\n",
      " 75 23.5G   75 17.6G    0     0  12.8M      0  0:31:22  0:23:32  0:07:50 13.0M\n",
      " 75 23.5G   75 17.6G    0     0  12.8M      0  0:31:22  0:23:33  0:07:49 13.0M\n",
      " 75 23.5G   75 17.6G    0     0  12.8M      0  0:31:22  0:23:34  0:07:48 13.0M\n",
      " 75 23.5G   75 17.7G    0     0  12.8M      0  0:31:22  0:23:35  0:07:47 12.9M\n",
      " 75 23.5G   75 17.7G    0     0  12.8M      0  0:31:22  0:23:36  0:07:46 12.9M\n",
      " 75 23.5G   75 17.7G    0     0  12.8M      0  0:31:22  0:23:37  0:07:45 12.9M\n",
      " 75 23.5G   75 17.7G    0     0  12.8M      0  0:31:22  0:23:38  0:07:44 12.8M\n",
      " 75 23.5G   75 17.7G    0     0  12.8M      0  0:31:22  0:23:39  0:07:43 12.8M\n",
      " 75 23.5G   75 17.7G    0     0  12.8M      0  0:31:22  0:23:40  0:07:42 13.0M\n",
      " 75 23.5G   75 17.7G    0     0  12.8M      0  0:31:22  0:23:41  0:07:41 13.1M\n",
      " 75 23.5G   75 17.7G    0     0  12.8M      0  0:31:22  0:23:42  0:07:40 13.1M\n",
      " 75 23.5G   75 17.8G    0     0  12.8M      0  0:31:22  0:23:43  0:07:39 13.2M\n",
      " 75 23.5G   75 17.8G    0     0  12.8M      0  0:31:22  0:23:44  0:07:38 13.2M\n",
      " 75 23.5G   75 17.8G    0     0  12.8M      0  0:31:22  0:23:45  0:07:37 13.1M\n",
      " 75 23.5G   75 17.8G    0     0  12.8M      0  0:31:22  0:23:46  0:07:36 13.1M\n",
      " 75 23.5G   75 17.8G    0     0  12.8M      0  0:31:22  0:23:47  0:07:35 13.0M\n",
      " 75 23.5G   75 17.8G    0     0  12.8M      0  0:31:22  0:23:48  0:07:34 13.0M\n",
      " 75 23.5G   75 17.8G    0     0  12.8M      0  0:31:22  0:23:49  0:07:33 13.1M\n",
      " 76 23.5G   76 17.8G    0     0  12.8M      0  0:31:22  0:23:50  0:07:32 13.1M\n",
      " 76 23.5G   76 17.9G    0     0  12.8M      0  0:31:22  0:23:51  0:07:31 13.1M\n",
      " 76 23.5G   76 17.9G    0     0  12.8M      0  0:31:21  0:23:52  0:07:29 13.1M\n",
      " 76 23.5G   76 17.9G    0     0  12.8M      0  0:31:21  0:23:53  0:07:28 13.1M\n",
      " 76 23.5G   76 17.9G    0     0  12.8M      0  0:31:21  0:23:54  0:07:27 13.0M\n",
      " 76 23.5G   76 17.9G    0     0  12.8M      0  0:31:21  0:23:55  0:07:26 13.1M\n",
      " 76 23.5G   76 17.9G    0     0  12.8M      0  0:31:21  0:23:56  0:07:25 13.1M\n",
      " 76 23.5G   76 17.9G    0     0  12.8M      0  0:31:21  0:23:57  0:07:24 13.3M\n",
      " 76 23.5G   76 17.9G    0     0  12.8M      0  0:31:21  0:23:58  0:07:23 13.2M\n",
      " 76 23.5G   76 18.0G    0     0  12.8M      0  0:31:21  0:23:59  0:07:22 13.3M\n",
      " 76 23.5G   76 18.0G    0     0  12.8M      0  0:31:21  0:24:00  0:07:21 13.2M\n",
      " 76 23.5G   76 18.0G    0     0  12.8M      0  0:31:21  0:24:01  0:07:20 13.0M\n",
      " 76 23.5G   76 18.0G    0     0  12.8M      0  0:31:22  0:24:03  0:07:19 12.2M\n",
      " 76 23.5G   76 18.0G    0     0  12.8M      0  0:31:21  0:24:03  0:07:18 12.9M\n",
      " 76 23.5G   76 18.0G    0     0  12.8M      0  0:31:21  0:24:04  0:07:17 12.9M\n",
      " 76 23.5G   76 18.0G    0     0  12.8M      0  0:31:21  0:24:05  0:07:16 12.8M\n",
      " 76 23.5G   76 18.0G    0     0  12.8M      0  0:31:21  0:24:06  0:07:15 13.0M\n",
      " 76 23.5G   76 18.1G    0     0  12.8M      0  0:31:21  0:24:07  0:07:14 13.7M\n",
      " 77 23.5G   77 18.1G    0     0  12.8M      0  0:31:21  0:24:08  0:07:13 12.9M\n",
      " 77 23.5G   77 18.1G    0     0  12.8M      0  0:31:21  0:24:09  0:07:12 12.8M\n",
      " 77 23.5G   77 18.1G    0     0  12.8M      0  0:31:21  0:24:10  0:07:11 12.8M\n",
      " 77 23.5G   77 18.1G    0     0  12.8M      0  0:31:21  0:24:11  0:07:10 12.8M\n",
      " 77 23.5G   77 18.1G    0     0  12.8M      0  0:31:21  0:24:12  0:07:09 12.9M\n",
      " 77 23.5G   77 18.1G    0     0  12.8M      0  0:31:21  0:24:13  0:07:08 12.5M\n",
      " 77 23.5G   77 18.1G    0     0  12.8M      0  0:31:22  0:24:14  0:07:08 11.8M\n",
      " 77 23.5G   77 18.2G    0     0  12.8M      0  0:31:22  0:24:15  0:07:07 11.1M\n",
      " 77 23.5G   77 18.2G    0     0  12.7M      0  0:31:22  0:24:16  0:07:06 10.1M\n",
      " 77 23.5G   77 18.2G    0     0  12.7M      0  0:31:23  0:24:17  0:07:06 9356k\n",
      " 77 23.5G   77 18.2G    0     0  12.7M      0  0:31:23  0:24:18  0:07:05 8726k\n",
      " 77 23.5G   77 18.2G    0     0  12.7M      0  0:31:24  0:24:19  0:07:05 8434k\n",
      " 77 23.5G   77 18.2G    0     0  12.7M      0  0:31:24  0:24:20  0:07:04 8269k\n",
      " 77 23.5G   77 18.2G    0     0  12.7M      0  0:31:25  0:24:21  0:07:04 8287k\n",
      " 77 23.5G   77 18.2G    0     0  12.7M      0  0:31:25  0:24:22  0:07:03 8292k\n",
      " 77 23.5G   77 18.2G    0     0  12.7M      0  0:31:26  0:24:23  0:07:03 8277k\n",
      " 77 23.5G   77 18.2G    0     0  12.7M      0  0:31:26  0:24:24  0:07:02 8181k\n",
      " 77 23.5G   77 18.2G    0     0  12.7M      0  0:31:27  0:24:25  0:07:02 8125k\n",
      " 77 23.5G   77 18.2G    0     0  12.7M      0  0:31:27  0:24:26  0:07:01 8085k\n",
      " 77 23.5G   77 18.2G    0     0  12.7M      0  0:31:28  0:24:27  0:07:01 8132k\n",
      " 77 23.5G   77 18.3G    0     0  12.7M      0  0:31:28  0:24:28  0:07:00 8105k\n",
      " 77 23.5G   77 18.3G    0     0  12.7M      0  0:31:29  0:24:29  0:07:00 8186k\n",
      " 77 23.5G   77 18.3G    0     0  12.7M      0  0:31:29  0:24:30  0:06:59 8178k\n",
      " 77 23.5G   77 18.3G    0     0  12.7M      0  0:31:30  0:24:31  0:06:59 8109k\n",
      " 77 23.5G   77 18.3G    0     0  12.7M      0  0:31:30  0:24:32  0:06:58 8016k\n",
      " 77 23.5G   77 18.3G    0     0  12.7M      0  0:31:31  0:24:33  0:06:58 7888k\n",
      " 77 23.5G   77 18.3G    0     0  12.7M      0  0:31:31  0:24:34  0:06:57 7847k\n",
      " 78 23.5G   78 18.3G    0     0  12.7M      0  0:31:32  0:24:35  0:06:57 7770k\n",
      " 78 23.5G   78 18.3G    0     0  12.7M      0  0:31:32  0:24:36  0:06:56 7724k\n",
      " 78 23.5G   78 18.3G    0     0  12.7M      0  0:31:33  0:24:37  0:06:56 7640k\n",
      " 78 23.5G   78 18.3G    0     0  12.7M      0  0:31:33  0:24:38  0:06:55 7645k\n",
      " 78 23.5G   78 18.3G    0     0  12.7M      0  0:31:34  0:24:39  0:06:55 7569k\n",
      " 78 23.5G   78 18.3G    0     0  12.7M      0  0:31:35  0:24:40  0:06:55 7362k\n",
      " 78 23.5G   78 18.3G    0     0  12.7M      0  0:31:35  0:24:41  0:06:54 7154k\n",
      " 78 23.5G   78 18.4G    0     0  12.7M      0  0:31:36  0:24:42  0:06:54 6981k\n",
      " 78 23.5G   78 18.4G    0     0  12.7M      0  0:31:36  0:24:43  0:06:53 6858k\n",
      " 78 23.5G   78 18.4G    0     0  12.7M      0  0:31:37  0:24:44  0:06:53 6737k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:38  0:24:45  0:06:53 6724k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:38  0:24:46  0:06:52 6587k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:39  0:24:47  0:06:52 6384k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:40  0:24:48  0:06:52 6242k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:40  0:24:49  0:06:51 6019k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:41  0:24:50  0:06:51 5903k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:42  0:24:51  0:06:51 5880k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:43  0:24:52  0:06:51 5860k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:43  0:24:53  0:06:50 5907k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:44  0:24:54  0:06:50 5957k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:45  0:24:55  0:06:50 5920k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:45  0:24:56  0:06:49 5930k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:46  0:24:57  0:06:49 5921k\n",
      " 78 23.5G   78 18.4G    0     0  12.6M      0  0:31:47  0:24:58  0:06:49 5850k\n",
      " 78 23.5G   78 18.5G    0     0  12.6M      0  0:31:47  0:24:59  0:06:48 5802k\n",
      " 78 23.5G   78 18.5G    0     0  12.6M      0  0:31:48  0:25:00  0:06:48 6537k\n",
      " 78 23.5G   78 18.5G    0     0  12.6M      0  0:31:48  0:25:01  0:06:47 7942k\n",
      " 78 23.5G   78 18.5G    0     0  12.6M      0  0:31:48  0:25:02  0:06:46 9421k\n",
      " 78 23.5G   78 18.5G    0     0  12.6M      0  0:31:48  0:25:03  0:06:45 10.6M\n",
      " 78 23.5G   78 18.5G    0     0  12.6M      0  0:31:48  0:25:04  0:06:44 12.1M\n",
      " 78 23.5G   78 18.5G    0     0  12.6M      0  0:31:48  0:25:05  0:06:43 12.8M\n",
      " 78 23.5G   78 18.5G    0     0  12.6M      0  0:31:48  0:25:06  0:06:42 12.8M\n",
      " 79 23.5G   79 18.5G    0     0  12.6M      0  0:31:48  0:25:07  0:06:41 12.7M\n",
      " 79 23.5G   79 18.6G    0     0  12.6M      0  0:31:48  0:25:08  0:06:40 12.8M\n",
      " 79 23.5G   79 18.6G    0     0  12.6M      0  0:31:48  0:25:09  0:06:39 12.6M\n",
      " 79 23.5G   79 18.6G    0     0  12.6M      0  0:31:48  0:25:10  0:06:38 12.7M\n",
      " 79 23.5G   79 18.6G    0     0  12.6M      0  0:31:47  0:25:11  0:06:36 12.8M\n",
      " 79 23.5G   79 18.6G    0     0  12.6M      0  0:31:47  0:25:12  0:06:35 12.8M\n",
      " 79 23.5G   79 18.6G    0     0  12.6M      0  0:31:47  0:25:13  0:06:34 12.8M\n",
      " 79 23.5G   79 18.6G    0     0  12.6M      0  0:31:47  0:25:14  0:06:33 13.0M\n",
      " 79 23.5G   79 18.7G    0     0  12.6M      0  0:31:47  0:25:15  0:06:32 13.0M\n",
      " 79 23.5G   79 18.7G    0     0  12.6M      0  0:31:47  0:25:16  0:06:31 13.1M\n",
      " 79 23.5G   79 18.7G    0     0  12.6M      0  0:31:47  0:25:17  0:06:30 13.1M\n",
      " 79 23.5G   79 18.7G    0     0  12.6M      0  0:31:47  0:25:18  0:06:29 13.2M\n",
      " 79 23.5G   79 18.7G    0     0  12.6M      0  0:31:47  0:25:19  0:06:28 13.1M\n",
      " 79 23.5G   79 18.7G    0     0  12.6M      0  0:31:47  0:25:20  0:06:27 13.1M\n",
      " 79 23.5G   79 18.7G    0     0  12.6M      0  0:31:47  0:25:21  0:06:26 13.0M\n",
      " 79 23.5G   79 18.7G    0     0  12.6M      0  0:31:47  0:25:22  0:06:25 13.1M\n",
      " 79 23.5G   79 18.8G    0     0  12.6M      0  0:31:47  0:25:23  0:06:24 13.0M\n",
      " 79 23.5G   79 18.8G    0     0  12.6M      0  0:31:47  0:25:24  0:06:23 13.1M\n",
      " 80 23.5G   80 18.8G    0     0  12.6M      0  0:31:47  0:25:26  0:06:21 12.6M\n",
      " 80 23.5G   80 18.8G    0     0  12.6M      0  0:31:47  0:25:26  0:06:21 13.1M\n",
      " 80 23.5G   80 18.8G    0     0  12.6M      0  0:31:47  0:25:27  0:06:20 13.1M\n",
      " 80 23.5G   80 18.8G    0     0  12.6M      0  0:31:47  0:25:28  0:06:19 13.1M\n",
      " 80 23.5G   80 18.8G    0     0  12.6M      0  0:31:47  0:25:29  0:06:18 13.2M\n",
      " 80 23.5G   80 18.8G    0     0  12.6M      0  0:31:47  0:25:30  0:06:17 13.6M\n",
      " 80 23.5G   80 18.9G    0     0  12.6M      0  0:31:46  0:25:31  0:06:15 13.1M\n",
      " 80 23.5G   80 18.9G    0     0  12.6M      0  0:31:46  0:25:32  0:06:14 13.1M\n",
      " 80 23.5G   80 18.9G    0     0  12.6M      0  0:31:46  0:25:33  0:06:13 13.1M\n",
      " 80 23.5G   80 18.9G    0     0  12.6M      0  0:31:46  0:25:34  0:06:12 13.1M\n",
      " 80 23.5G   80 18.9G    0     0  12.6M      0  0:31:46  0:25:35  0:06:11 13.1M\n",
      " 80 23.5G   80 18.9G    0     0  12.6M      0  0:31:46  0:25:36  0:06:10 13.2M\n",
      " 80 23.5G   80 18.9G    0     0  12.6M      0  0:31:46  0:25:37  0:06:09 13.2M\n",
      " 80 23.5G   80 18.9G    0     0  12.6M      0  0:31:46  0:25:38  0:06:08 13.2M\n",
      " 80 23.5G   80 19.0G    0     0  12.6M      0  0:31:46  0:25:39  0:06:07 13.2M\n",
      " 80 23.5G   80 19.0G    0     0  12.6M      0  0:31:46  0:25:40  0:06:06 13.2M\n",
      " 80 23.5G   80 19.0G    0     0  12.6M      0  0:31:46  0:25:41  0:06:05 13.1M\n",
      " 80 23.5G   80 19.0G    0     0  12.6M      0  0:31:46  0:25:42  0:06:04 13.2M\n",
      " 80 23.5G   80 19.0G    0     0  12.6M      0  0:31:46  0:25:43  0:06:03 12.7M\n",
      " 81 23.5G   81 19.0G    0     0  12.6M      0  0:31:46  0:25:44  0:06:02 12.8M\n",
      " 81 23.5G   81 19.0G    0     0  12.6M      0  0:31:46  0:25:45  0:06:01 12.7M\n",
      " 81 23.5G   81 19.0G    0     0  12.6M      0  0:31:46  0:25:46  0:06:00 12.7M\n",
      " 81 23.5G   81 19.1G    0     0  12.6M      0  0:31:46  0:25:47  0:05:59 12.5M\n",
      " 81 23.5G   81 19.1G    0     0  12.6M      0  0:31:46  0:25:48  0:05:58 12.9M\n",
      " 81 23.5G   81 19.1G    0     0  12.6M      0  0:31:46  0:25:49  0:05:57 12.8M\n",
      " 81 23.5G   81 19.1G    0     0  12.6M      0  0:31:46  0:25:50  0:05:56 12.9M\n",
      " 81 23.5G   81 19.1G    0     0  12.6M      0  0:31:46  0:25:51  0:05:55 12.9M\n",
      " 81 23.5G   81 19.1G    0     0  12.6M      0  0:31:46  0:25:52  0:05:54 13.1M\n",
      " 81 23.5G   81 19.1G    0     0  12.6M      0  0:31:46  0:25:53  0:05:53 13.0M\n",
      " 81 23.5G   81 19.1G    0     0  12.6M      0  0:31:46  0:25:54  0:05:52 12.8M\n",
      " 81 23.5G   81 19.2G    0     0  12.6M      0  0:31:46  0:25:55  0:05:51 12.6M\n",
      " 81 23.5G   81 19.2G    0     0  12.6M      0  0:31:46  0:25:56  0:05:50 12.4M\n",
      " 81 23.5G   81 19.2G    0     0  12.6M      0  0:31:46  0:25:57  0:05:49 12.2M\n",
      " 81 23.5G   81 19.2G    0     0  12.6M      0  0:31:46  0:25:58  0:05:48 11.9M\n",
      " 81 23.5G   81 19.2G    0     0  12.6M      0  0:31:46  0:25:59  0:05:47 11.8M\n",
      " 81 23.5G   81 19.2G    0     0  12.6M      0  0:31:46  0:26:00  0:05:46 11.8M\n",
      " 81 23.5G   81 19.2G    0     0  12.6M      0  0:31:46  0:26:01  0:05:45 11.6M\n",
      " 81 23.5G   81 19.2G    0     0  12.6M      0  0:31:46  0:26:02  0:05:44 11.4M\n",
      " 82 23.5G   82 19.3G    0     0  12.6M      0  0:31:47  0:26:03  0:05:44 11.4M\n",
      " 82 23.5G   82 19.3G    0     0  12.6M      0  0:31:47  0:26:04  0:05:43 11.4M\n",
      " 82 23.5G   82 19.3G    0     0  12.6M      0  0:31:47  0:26:05  0:05:42 11.4M\n",
      " 82 23.5G   82 19.3G    0     0  12.6M      0  0:31:47  0:26:06  0:05:41 11.0M\n",
      " 82 23.5G   82 19.3G    0     0  12.6M      0  0:31:47  0:26:07  0:05:40 11.1M\n",
      " 82 23.5G   82 19.3G    0     0  12.6M      0  0:31:47  0:26:08  0:05:39 11.3M\n",
      " 82 23.5G   82 19.3G    0     0  12.6M      0  0:31:47  0:26:09  0:05:38 11.6M\n",
      " 82 23.5G   82 19.3G    0     0  12.6M      0  0:31:47  0:26:10  0:05:37 12.0M\n",
      " 82 23.5G   82 19.3G    0     0  12.6M      0  0:31:47  0:26:11  0:05:36 12.7M\n",
      " 82 23.5G   82 19.4G    0     0  12.6M      0  0:31:47  0:26:12  0:05:35 13.0M\n",
      " 82 23.5G   82 19.4G    0     0  12.6M      0  0:31:47  0:26:13  0:05:34 13.2M\n",
      " 82 23.5G   82 19.4G    0     0  12.6M      0  0:31:47  0:26:14  0:05:33 13.2M\n",
      " 82 23.5G   82 19.4G    0     0  12.6M      0  0:31:47  0:26:15  0:05:32 13.0M\n",
      " 82 23.5G   82 19.4G    0     0  12.6M      0  0:31:47  0:26:16  0:05:31 12.8M\n",
      " 82 23.5G   82 19.4G    0     0  12.6M      0  0:31:47  0:26:17  0:05:30 12.7M\n",
      " 82 23.5G   82 19.4G    0     0  12.6M      0  0:31:47  0:26:18  0:05:29 12.7M\n",
      " 82 23.5G   82 19.4G    0     0  12.6M      0  0:31:47  0:26:19  0:05:28 12.7M\n",
      " 82 23.5G   82 19.5G    0     0  12.6M      0  0:31:47  0:26:20  0:05:27 12.9M\n",
      " 82 23.5G   82 19.5G    0     0  12.6M      0  0:31:47  0:26:21  0:05:26 13.1M\n",
      " 83 23.5G   83 19.5G    0     0  12.6M      0  0:31:47  0:26:22  0:05:25 13.1M\n",
      " 83 23.5G   83 19.5G    0     0  12.6M      0  0:31:47  0:26:24  0:05:23 12.7M\n",
      " 83 23.5G   83 19.5G    0     0  12.6M      0  0:31:47  0:26:24  0:05:23 13.1M\n",
      " 83 23.5G   83 19.5G    0     0  12.6M      0  0:31:46  0:26:25  0:05:21 13.0M\n",
      " 83 23.5G   83 19.5G    0     0  12.6M      0  0:31:46  0:26:26  0:05:20 13.0M\n",
      " 83 23.5G   83 19.5G    0     0  12.6M      0  0:31:46  0:26:27  0:05:19 13.0M\n",
      " 83 23.5G   83 19.6G    0     0  12.6M      0  0:31:46  0:26:28  0:05:18 13.4M\n",
      " 83 23.5G   83 19.6G    0     0  12.6M      0  0:31:46  0:26:29  0:05:17 13.0M\n",
      " 83 23.5G   83 19.6G    0     0  12.6M      0  0:31:46  0:26:30  0:05:16 13.1M\n",
      " 83 23.5G   83 19.6G    0     0  12.6M      0  0:31:46  0:26:31  0:05:15 13.1M\n",
      " 83 23.5G   83 19.6G    0     0  12.6M      0  0:31:46  0:26:32  0:05:14 13.1M\n",
      " 83 23.5G   83 19.6G    0     0  12.6M      0  0:31:46  0:26:33  0:05:13 13.1M\n",
      " 83 23.5G   83 19.6G    0     0  12.6M      0  0:31:46  0:26:34  0:05:12 13.1M\n",
      " 83 23.5G   83 19.7G    0     0  12.6M      0  0:31:46  0:26:35  0:05:11 13.1M\n",
      " 83 23.5G   83 19.7G    0     0  12.6M      0  0:31:46  0:26:36  0:05:10 13.1M\n",
      " 83 23.5G   83 19.7G    0     0  12.6M      0  0:31:46  0:26:37  0:05:09 13.0M\n",
      " 83 23.5G   83 19.7G    0     0  12.6M      0  0:31:46  0:26:38  0:05:08 13.0M\n",
      " 83 23.5G   83 19.7G    0     0  12.6M      0  0:31:46  0:26:39  0:05:07 12.9M\n",
      " 83 23.5G   83 19.7G    0     0  12.6M      0  0:31:46  0:26:40  0:05:06 12.9M\n",
      " 84 23.5G   84 19.7G    0     0  12.6M      0  0:31:46  0:26:41  0:05:05 13.0M\n",
      " 84 23.5G   84 19.7G    0     0  12.6M      0  0:31:46  0:26:42  0:05:04 13.0M\n",
      " 84 23.5G   84 19.8G    0     0  12.6M      0  0:31:46  0:26:43  0:05:03 13.1M\n",
      " 84 23.5G   84 19.8G    0     0  12.6M      0  0:31:46  0:26:44  0:05:02 13.1M\n",
      " 84 23.5G   84 19.8G    0     0  12.6M      0  0:31:46  0:26:45  0:05:01 13.1M\n",
      " 84 23.5G   84 19.8G    0     0  12.6M      0  0:31:46  0:26:46  0:05:00 13.1M\n",
      " 84 23.5G   84 19.8G    0     0  12.6M      0  0:31:46  0:26:47  0:04:59 13.0M\n",
      " 84 23.5G   84 19.8G    0     0  12.6M      0  0:31:46  0:26:48  0:04:58 13.0M\n",
      " 84 23.5G   84 19.8G    0     0  12.6M      0  0:31:45  0:26:49  0:04:56 13.1M\n",
      " 84 23.5G   84 19.8G    0     0  12.6M      0  0:31:45  0:26:50  0:04:55 13.1M\n",
      " 84 23.5G   84 19.9G    0     0  12.6M      0  0:31:45  0:26:51  0:04:54 13.1M\n",
      " 84 23.5G   84 19.9G    0     0  12.6M      0  0:31:45  0:26:52  0:04:53 13.0M\n",
      " 84 23.5G   84 19.9G    0     0  12.6M      0  0:31:45  0:26:53  0:04:52 12.9M\n",
      " 84 23.5G   84 19.9G    0     0  12.6M      0  0:31:45  0:26:54  0:04:51 12.9M\n",
      " 84 23.5G   84 19.9G    0     0  12.6M      0  0:31:45  0:26:55  0:04:50 12.8M\n",
      " 84 23.5G   84 19.9G    0     0  12.6M      0  0:31:45  0:26:56  0:04:49 12.9M\n",
      " 84 23.5G   84 19.9G    0     0  12.6M      0  0:31:45  0:26:57  0:04:48 12.9M\n",
      " 84 23.5G   84 19.9G    0     0  12.6M      0  0:31:45  0:26:58  0:04:47 13.0M\n",
      " 85 23.5G   85 20.0G    0     0  12.6M      0  0:31:45  0:26:59  0:04:46 13.1M\n",
      " 85 23.5G   85 20.0G    0     0  12.6M      0  0:31:45  0:27:00  0:04:45 13.1M\n",
      " 85 23.5G   85 20.0G    0     0  12.6M      0  0:31:45  0:27:01  0:04:44 13.0M\n",
      " 85 23.5G   85 20.0G    0     0  12.6M      0  0:31:45  0:27:02  0:04:43 13.0M\n",
      " 85 23.5G   85 20.0G    0     0  12.6M      0  0:31:45  0:27:03  0:04:42 12.9M\n",
      " 85 23.5G   85 20.0G    0     0  12.6M      0  0:31:45  0:27:04  0:04:41 12.8M\n",
      " 85 23.5G   85 20.0G    0     0  12.6M      0  0:31:45  0:27:05  0:04:40 12.8M\n",
      " 85 23.5G   85 20.0G    0     0  12.6M      0  0:31:45  0:27:06  0:04:39 12.8M\n",
      " 85 23.5G   85 20.1G    0     0  12.6M      0  0:31:45  0:27:07  0:04:38 12.8M\n",
      " 85 23.5G   85 20.1G    0     0  12.6M      0  0:31:45  0:27:08  0:04:37 12.8M\n",
      " 85 23.5G   85 20.1G    0     0  12.6M      0  0:31:45  0:27:09  0:04:36 12.8M\n",
      " 85 23.5G   85 20.1G    0     0  12.6M      0  0:31:45  0:27:10  0:04:35 12.7M\n",
      " 85 23.5G   85 20.1G    0     0  12.6M      0  0:31:45  0:27:12  0:04:33 11.9M\n",
      " 85 23.5G   85 20.1G    0     0  12.6M      0  0:31:46  0:27:12  0:04:34 11.3M\n",
      " 85 23.5G   85 20.1G    0     0  12.6M      0  0:31:46  0:27:13  0:04:33 10.5M\n",
      " 85 23.5G   85 20.1G    0     0  12.6M      0  0:31:46  0:27:14  0:04:32 9808k\n",
      " 85 23.5G   85 20.1G    0     0  12.6M      0  0:31:47  0:27:15  0:04:32 8494k\n",
      " 85 23.5G   85 20.1G    0     0  12.6M      0  0:31:47  0:27:16  0:04:31 8809k\n",
      " 85 23.5G   85 20.2G    0     0  12.6M      0  0:31:47  0:27:17  0:04:30 8843k\n",
      " 85 23.5G   85 20.2G    0     0  12.6M      0  0:31:48  0:27:18  0:04:30 9034k\n",
      " 85 23.5G   85 20.2G    0     0  12.6M      0  0:31:48  0:27:19  0:04:29 9756k\n",
      " 85 23.5G   85 20.2G    0     0  12.6M      0  0:31:48  0:27:20  0:04:28 10.6M\n",
      " 86 23.5G   86 20.2G    0     0  12.6M      0  0:31:48  0:27:21  0:04:27 10.8M\n",
      " 86 23.5G   86 20.2G    0     0  12.6M      0  0:31:48  0:27:22  0:04:26 11.1M\n",
      " 86 23.5G   86 20.2G    0     0  12.6M      0  0:31:48  0:27:23  0:04:25 11.3M\n",
      " 86 23.5G   86 20.2G    0     0  12.6M      0  0:31:48  0:27:24  0:04:24 11.4M\n",
      " 86 23.5G   86 20.2G    0     0  12.6M      0  0:31:48  0:27:25  0:04:23 11.4M\n",
      " 86 23.5G   86 20.3G    0     0  12.6M      0  0:31:48  0:27:26  0:04:22 11.7M\n",
      " 86 23.5G   86 20.3G    0     0  12.6M      0  0:31:48  0:27:27  0:04:21 12.2M\n",
      " 86 23.5G   86 20.3G    0     0  12.6M      0  0:31:48  0:27:28  0:04:20 12.6M\n",
      " 86 23.5G   86 20.3G    0     0  12.6M      0  0:31:48  0:27:29  0:04:19 13.0M\n",
      " 86 23.5G   86 20.3G    0     0  12.6M      0  0:31:48  0:27:30  0:04:18 13.1M\n",
      " 86 23.5G   86 20.3G    0     0  12.6M      0  0:31:48  0:27:31  0:04:17 13.0M\n",
      " 86 23.5G   86 20.3G    0     0  12.6M      0  0:31:48  0:27:32  0:04:16 13.0M\n",
      " 86 23.5G   86 20.3G    0     0  12.6M      0  0:31:48  0:27:33  0:04:15 13.0M\n",
      " 86 23.5G   86 20.4G    0     0  12.6M      0  0:31:48  0:27:34  0:04:14 13.0M\n",
      " 86 23.5G   86 20.4G    0     0  12.6M      0  0:31:48  0:27:35  0:04:13 13.1M\n",
      " 86 23.5G   86 20.4G    0     0  12.6M      0  0:31:48  0:27:36  0:04:12 13.2M\n",
      " 86 23.5G   86 20.4G    0     0  12.6M      0  0:31:48  0:27:37  0:04:11 13.2M\n",
      " 86 23.5G   86 20.4G    0     0  12.6M      0  0:31:48  0:27:38  0:04:10 13.2M\n",
      " 86 23.5G   86 20.4G    0     0  12.6M      0  0:31:48  0:27:39  0:04:09 13.2M\n",
      " 87 23.5G   87 20.4G    0     0  12.6M      0  0:31:48  0:27:40  0:04:08 13.2M\n",
      " 87 23.5G   87 20.4G    0     0  12.6M      0  0:31:48  0:27:41  0:04:07 13.0M\n",
      " 87 23.5G   87 20.5G    0     0  12.6M      0  0:31:48  0:27:42  0:04:06 13.0M\n",
      " 87 23.5G   87 20.5G    0     0  12.6M      0  0:31:48  0:27:43  0:04:05 13.0M\n",
      " 87 23.5G   87 20.5G    0     0  12.6M      0  0:31:47  0:27:44  0:04:03 13.1M\n",
      " 87 23.5G   87 20.5G    0     0  12.6M      0  0:31:47  0:27:45  0:04:02 13.1M\n",
      " 87 23.5G   87 20.5G    0     0  12.6M      0  0:31:47  0:27:46  0:04:01 13.2M\n",
      " 87 23.5G   87 20.5G    0     0  12.6M      0  0:31:47  0:27:47  0:04:00 13.2M\n",
      " 87 23.5G   87 20.5G    0     0  12.6M      0  0:31:47  0:27:48  0:03:59 13.1M\n",
      " 87 23.5G   87 20.6G    0     0  12.6M      0  0:31:47  0:27:49  0:03:58 13.0M\n",
      " 87 23.5G   87 20.6G    0     0  12.6M      0  0:31:47  0:27:50  0:03:57 12.9M\n",
      " 87 23.5G   87 20.6G    0     0  12.6M      0  0:31:47  0:27:51  0:03:56 13.0M\n",
      " 87 23.5G   87 20.6G    0     0  12.6M      0  0:31:47  0:27:52  0:03:55 13.0M\n",
      " 87 23.5G   87 20.6G    0     0  12.6M      0  0:31:47  0:27:53  0:03:54 13.1M\n",
      " 87 23.5G   87 20.6G    0     0  12.6M      0  0:31:47  0:27:54  0:03:53 13.0M\n",
      " 87 23.5G   87 20.6G    0     0  12.6M      0  0:31:47  0:27:55  0:03:52 13.1M\n",
      " 87 23.5G   87 20.6G    0     0  12.6M      0  0:31:47  0:27:56  0:03:51 13.1M\n",
      " 87 23.5G   87 20.7G    0     0  12.6M      0  0:31:47  0:27:57  0:03:50 13.1M\n",
      " 88 23.5G   88 20.7G    0     0  12.6M      0  0:31:47  0:27:58  0:03:49 13.1M\n",
      " 88 23.5G   88 20.7G    0     0  12.6M      0  0:31:47  0:27:59  0:03:48 13.1M\n",
      " 88 23.5G   88 20.7G    0     0  12.6M      0  0:31:47  0:28:00  0:03:47 13.0M\n",
      " 88 23.5G   88 20.7G    0     0  12.6M      0  0:31:47  0:28:01  0:03:46 12.9M\n",
      " 88 23.5G   88 20.7G    0     0  12.6M      0  0:31:47  0:28:02  0:03:45 12.9M\n",
      " 88 23.5G   88 20.7G    0     0  12.6M      0  0:31:47  0:28:03  0:03:44 13.0M\n",
      " 88 23.5G   88 20.7G    0     0  12.6M      0  0:31:47  0:28:04  0:03:43 13.0M\n",
      " 88 23.5G   88 20.8G    0     0  12.6M      0  0:31:47  0:28:05  0:03:42 13.0M\n",
      " 88 23.5G   88 20.8G    0     0  12.6M      0  0:31:47  0:28:06  0:03:41 13.0M\n",
      " 88 23.5G   88 20.8G    0     0  12.6M      0  0:31:47  0:28:07  0:03:40 13.0M\n",
      " 88 23.5G   88 20.8G    0     0  12.6M      0  0:31:47  0:28:08  0:03:39 13.0M\n",
      " 88 23.5G   88 20.8G    0     0  12.6M      0  0:31:46  0:28:09  0:03:37 13.0M\n",
      " 88 23.5G   88 20.8G    0     0  12.6M      0  0:31:46  0:28:10  0:03:36 13.0M\n",
      " 88 23.5G   88 20.8G    0     0  12.6M      0  0:31:46  0:28:11  0:03:35 13.1M\n",
      " 88 23.5G   88 20.8G    0     0  12.6M      0  0:31:46  0:28:12  0:03:34 13.1M\n",
      " 88 23.5G   88 20.9G    0     0  12.6M      0  0:31:46  0:28:13  0:03:33 13.2M\n",
      " 88 23.5G   88 20.9G    0     0  12.6M      0  0:31:46  0:28:14  0:03:32 13.1M\n",
      " 88 23.5G   88 20.9G    0     0  12.6M      0  0:31:46  0:28:15  0:03:31 13.1M\n",
      " 89 23.5G   89 20.9G    0     0  12.6M      0  0:31:46  0:28:16  0:03:30 13.1M\n",
      " 89 23.5G   89 20.9G    0     0  12.6M      0  0:31:46  0:28:17  0:03:29 13.1M\n",
      " 89 23.5G   89 20.9G    0     0  12.6M      0  0:31:46  0:28:18  0:03:28 13.0M\n",
      " 89 23.5G   89 20.9G    0     0  12.6M      0  0:31:46  0:28:19  0:03:27 13.1M\n",
      " 89 23.5G   89 20.9G    0     0  12.6M      0  0:31:46  0:28:20  0:03:26 13.1M\n",
      " 89 23.5G   89 21.0G    0     0  12.6M      0  0:31:46  0:28:21  0:03:25 13.2M\n",
      " 89 23.5G   89 21.0G    0     0  12.6M      0  0:31:46  0:28:22  0:03:24 13.1M\n",
      " 89 23.5G   89 21.0G    0     0  12.6M      0  0:31:46  0:28:23  0:03:23 13.2M\n",
      " 89 23.5G   89 21.0G    0     0  12.6M      0  0:31:46  0:28:24  0:03:22 13.2M\n",
      " 89 23.5G   89 21.0G    0     0  12.6M      0  0:31:46  0:28:25  0:03:21 13.1M\n",
      " 89 23.5G   89 21.0G    0     0  12.6M      0  0:31:46  0:28:26  0:03:20 13.1M\n",
      " 89 23.5G   89 21.0G    0     0  12.6M      0  0:31:46  0:28:27  0:03:19 13.1M\n",
      " 89 23.5G   89 21.1G    0     0  12.6M      0  0:31:46  0:28:28  0:03:18 13.1M\n",
      " 89 23.5G   89 21.1G    0     0  12.6M      0  0:31:46  0:28:29  0:03:17 13.1M\n",
      " 89 23.5G   89 21.1G    0     0  12.6M      0  0:31:45  0:28:30  0:03:15 13.2M\n",
      " 89 23.5G   89 21.1G    0     0  12.6M      0  0:31:45  0:28:31  0:03:14 13.2M\n",
      " 89 23.5G   89 21.1G    0     0  12.6M      0  0:31:45  0:28:32  0:03:13 13.1M\n",
      " 89 23.5G   89 21.1G    0     0  12.6M      0  0:31:45  0:28:33  0:03:12 13.0M\n",
      " 89 23.5G   89 21.1G    0     0  12.6M      0  0:31:45  0:28:34  0:03:11 13.0M\n",
      " 90 23.5G   90 21.1G    0     0  12.6M      0  0:31:45  0:28:35  0:03:10 12.9M\n",
      " 90 23.5G   90 21.2G    0     0  12.6M      0  0:31:45  0:28:36  0:03:09 12.9M\n",
      " 90 23.5G   90 21.2G    0     0  12.6M      0  0:31:45  0:28:37  0:03:08 13.0M\n",
      " 90 23.5G   90 21.2G    0     0  12.6M      0  0:31:45  0:28:38  0:03:07 13.1M\n",
      " 90 23.5G   90 21.2G    0     0  12.6M      0  0:31:45  0:28:39  0:03:06 13.0M\n",
      " 90 23.5G   90 21.2G    0     0  12.6M      0  0:31:45  0:28:40  0:03:05 13.1M\n",
      " 90 23.5G   90 21.2G    0     0  12.6M      0  0:31:45  0:28:41  0:03:04 13.1M\n",
      " 90 23.5G   90 21.2G    0     0  12.6M      0  0:31:45  0:28:42  0:03:03 13.1M\n",
      " 90 23.5G   90 21.2G    0     0  12.6M      0  0:31:45  0:28:43  0:03:02 13.2M\n",
      " 90 23.5G   90 21.3G    0     0  12.6M      0  0:31:45  0:28:44  0:03:01 13.2M\n",
      " 90 23.5G   90 21.3G    0     0  12.6M      0  0:31:45  0:28:45  0:03:00 13.2M\n",
      " 90 23.5G   90 21.3G    0     0  12.6M      0  0:31:45  0:28:46  0:02:59 13.1M\n",
      " 90 23.5G   90 21.3G    0     0  12.6M      0  0:31:45  0:28:47  0:02:58 13.0M\n",
      " 90 23.5G   90 21.3G    0     0  12.6M      0  0:31:45  0:28:48  0:02:57 13.0M\n",
      " 90 23.5G   90 21.3G    0     0  12.6M      0  0:31:45  0:28:49  0:02:56 12.9M\n",
      " 90 23.5G   90 21.3G    0     0  12.6M      0  0:31:45  0:28:50  0:02:55 12.9M\n",
      " 90 23.5G   90 21.3G    0     0  12.6M      0  0:31:45  0:28:51  0:02:54 12.8M\n",
      " 90 23.5G   90 21.4G    0     0  12.6M      0  0:31:45  0:28:52  0:02:53 12.9M\n",
      " 91 23.5G   91 21.4G    0     0  12.6M      0  0:31:45  0:28:53  0:02:52 12.8M\n",
      " 91 23.5G   91 21.4G    0     0  12.6M      0  0:31:45  0:28:54  0:02:51 12.8M\n",
      " 91 23.5G   91 21.4G    0     0  12.6M      0  0:31:45  0:28:55  0:02:50 12.5M\n",
      " 91 23.5G   91 21.4G    0     0  12.6M      0  0:31:45  0:28:56  0:02:49 12.5M\n",
      " 91 23.5G   91 21.4G    0     0  12.6M      0  0:31:45  0:28:57  0:02:48 12.5M\n",
      " 91 23.5G   91 21.4G    0     0  12.6M      0  0:31:45  0:28:58  0:02:47 12.6M\n",
      " 91 23.5G   91 21.4G    0     0  12.6M      0  0:31:45  0:28:59  0:02:46 12.6M\n",
      " 91 23.5G   91 21.5G    0     0  12.6M      0  0:31:45  0:29:00  0:02:45 13.1M\n",
      " 91 23.5G   91 21.5G    0     0  12.6M      0  0:31:45  0:29:01  0:02:44 13.1M\n",
      " 91 23.5G   91 21.5G    0     0  12.6M      0  0:31:45  0:29:02  0:02:43 13.1M\n",
      " 91 23.5G   91 21.5G    0     0  12.6M      0  0:31:44  0:29:03  0:02:41 13.1M\n",
      " 91 23.5G   91 21.5G    0     0  12.6M      0  0:31:44  0:29:04  0:02:40 13.1M\n",
      " 91 23.5G   91 21.5G    0     0  12.6M      0  0:31:44  0:29:05  0:02:39 12.8M\n",
      " 91 23.5G   91 21.5G    0     0  12.6M      0  0:31:44  0:29:06  0:02:38 12.9M\n",
      " 91 23.5G   91 21.5G    0     0  12.6M      0  0:31:45  0:29:07  0:02:38 12.4M\n",
      " 91 23.5G   91 21.6G    0     0  12.6M      0  0:31:45  0:29:08  0:02:37 11.6M\n",
      " 91 23.5G   91 21.6G    0     0  12.6M      0  0:31:45  0:29:09  0:02:36 10.8M\n",
      " 91 23.5G   91 21.6G    0     0  12.6M      0  0:31:46  0:29:10  0:02:36 10.0M\n",
      " 91 23.5G   91 21.6G    0     0  12.6M      0  0:31:46  0:29:11  0:02:35  9.9M\n",
      " 91 23.5G   91 21.6G    0     0  12.6M      0  0:31:46  0:29:12  0:02:34 10.4M\n",
      " 92 23.5G   92 21.6G    0     0  12.6M      0  0:31:46  0:29:13  0:02:33 11.1M\n",
      " 92 23.5G   92 21.6G    0     0  12.6M      0  0:31:45  0:29:14  0:02:31 11.9M\n",
      " 92 23.5G   92 21.6G    0     0  12.6M      0  0:31:45  0:29:15  0:02:30 13.0M\n",
      " 92 23.5G   92 21.6G    0     0  12.6M      0  0:31:45  0:29:16  0:02:29 13.1M\n",
      " 92 23.5G   92 21.7G    0     0  12.6M      0  0:31:45  0:29:17  0:02:28 13.1M\n",
      " 92 23.5G   92 21.7G    0     0  12.6M      0  0:31:45  0:29:18  0:02:27 13.2M\n",
      " 92 23.5G   92 21.7G    0     0  12.6M      0  0:31:45  0:29:19  0:02:26 13.1M\n",
      " 92 23.5G   92 21.7G    0     0  12.6M      0  0:31:45  0:29:20  0:02:25 13.2M\n",
      " 92 23.5G   92 21.7G    0     0  12.6M      0  0:31:45  0:29:21  0:02:24 13.1M\n",
      " 92 23.5G   92 21.7G    0     0  12.6M      0  0:31:45  0:29:22  0:02:23 13.2M\n",
      " 92 23.5G   92 21.7G    0     0  12.6M      0  0:31:45  0:29:23  0:02:22 13.2M\n",
      " 92 23.5G   92 21.7G    0     0  12.6M      0  0:31:45  0:29:24  0:02:21 13.2M\n",
      " 92 23.5G   92 21.8G    0     0  12.6M      0  0:31:45  0:29:25  0:02:20 13.2M\n",
      " 92 23.5G   92 21.8G    0     0  12.6M      0  0:31:45  0:29:26  0:02:19 13.3M\n",
      " 92 23.5G   92 21.8G    0     0  12.6M      0  0:31:45  0:29:27  0:02:18 13.3M\n",
      " 92 23.5G   92 21.8G    0     0  12.6M      0  0:31:45  0:29:28  0:02:17 13.2M\n",
      " 92 23.5G   92 21.8G    0     0  12.6M      0  0:31:45  0:29:29  0:02:16 13.1M\n",
      " 92 23.5G   92 21.8G    0     0  12.6M      0  0:31:45  0:29:30  0:02:15 12.9M\n",
      " 93 23.5G   93 21.8G    0     0  12.6M      0  0:31:45  0:29:31  0:02:14 12.9M\n",
      " 93 23.5G   93 21.9G    0     0  12.6M      0  0:31:45  0:29:32  0:02:13 12.8M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93 23.5G   93 21.9G    0     0  12.6M      0  0:31:45  0:29:33  0:02:12 12.9M\n",
      " 93 23.5G   93 21.9G    0     0  12.6M      0  0:31:45  0:29:34  0:02:11 13.1M\n",
      " 93 23.5G   93 21.9G    0     0  12.6M      0  0:31:45  0:29:35  0:02:10 13.3M\n",
      " 93 23.5G   93 21.9G    0     0  12.6M      0  0:31:44  0:29:36  0:02:08 13.3M\n",
      " 93 23.5G   93 21.9G    0     0  12.6M      0  0:31:44  0:29:37  0:02:07 13.3M\n",
      " 93 23.5G   93 21.9G    0     0  12.6M      0  0:31:44  0:29:38  0:02:06 13.3M\n",
      " 93 23.5G   93 21.9G    0     0  12.6M      0  0:31:44  0:29:39  0:02:05 13.3M\n",
      " 93 23.5G   93 22.0G    0     0  12.6M      0  0:31:44  0:29:40  0:02:04 13.3M\n",
      " 93 23.5G   93 22.0G    0     0  12.6M      0  0:31:44  0:29:41  0:02:03 13.4M\n",
      " 93 23.5G   93 22.0G    0     0  12.6M      0  0:31:44  0:29:42  0:02:02 13.4M\n",
      " 93 23.5G   93 22.0G    0     0  12.6M      0  0:31:44  0:29:43  0:02:01 13.4M\n",
      " 93 23.5G   93 22.0G    0     0  12.6M      0  0:31:44  0:29:44  0:02:00 13.4M\n",
      " 93 23.5G   93 22.0G    0     0  12.6M      0  0:31:44  0:29:45  0:01:59 13.4M\n",
      " 93 23.5G   93 22.0G    0     0  12.6M      0  0:31:44  0:29:46  0:01:58 13.4M\n",
      " 93 23.5G   93 22.0G    0     0  12.6M      0  0:31:44  0:29:47  0:01:57 13.3M\n",
      " 93 23.5G   93 22.1G    0     0  12.6M      0  0:31:44  0:29:48  0:01:56 13.1M\n",
      " 93 23.5G   93 22.1G    0     0  12.6M      0  0:31:44  0:29:49  0:01:55 13.0M\n",
      " 94 23.5G   94 22.1G    0     0  12.6M      0  0:31:44  0:29:50  0:01:54 13.1M\n",
      " 94 23.5G   94 22.1G    0     0  12.6M      0  0:31:44  0:29:51  0:01:53 13.2M\n",
      " 94 23.5G   94 22.1G    0     0  12.6M      0  0:31:44  0:29:52  0:01:52 13.3M\n",
      " 94 23.5G   94 22.1G    0     0  12.6M      0  0:31:43  0:29:53  0:01:50 13.4M\n",
      " 94 23.5G   94 22.1G    0     0  12.6M      0  0:31:43  0:29:54  0:01:49 13.4M\n",
      " 94 23.5G   94 22.2G    0     0  12.6M      0  0:31:43  0:29:55  0:01:48 13.3M\n",
      " 94 23.5G   94 22.2G    0     0  12.6M      0  0:31:43  0:29:56  0:01:47 13.3M\n",
      " 94 23.5G   94 22.2G    0     0  12.6M      0  0:31:43  0:29:57  0:01:46 13.3M\n",
      " 94 23.5G   94 22.2G    0     0  12.6M      0  0:31:43  0:29:58  0:01:45 13.2M\n",
      " 94 23.5G   94 22.2G    0     0  12.6M      0  0:31:43  0:29:59  0:01:44 13.1M\n",
      " 94 23.5G   94 22.2G    0     0  12.6M      0  0:31:43  0:30:00  0:01:43 13.2M\n",
      " 94 23.5G   94 22.2G    0     0  12.6M      0  0:31:43  0:30:01  0:01:42 13.1M\n",
      " 94 23.5G   94 22.2G    0     0  12.6M      0  0:31:43  0:30:02  0:01:41 13.0M\n",
      " 94 23.5G   94 22.3G    0     0  12.6M      0  0:31:43  0:30:03  0:01:40 13.1M\n",
      " 94 23.5G   94 22.3G    0     0  12.6M      0  0:31:43  0:30:04  0:01:39 13.2M\n",
      " 94 23.5G   94 22.3G    0     0  12.6M      0  0:31:43  0:30:05  0:01:38 13.2M\n",
      " 94 23.5G   94 22.3G    0     0  12.6M      0  0:31:43  0:30:06  0:01:37 13.3M\n",
      " 94 23.5G   94 22.3G    0     0  12.6M      0  0:31:43  0:30:07  0:01:36 13.3M\n",
      " 95 23.5G   95 22.3G    0     0  12.6M      0  0:31:43  0:30:08  0:01:35 13.3M\n",
      " 95 23.5G   95 22.3G    0     0  12.6M      0  0:31:43  0:30:09  0:01:34 13.2M\n",
      " 95 23.5G   95 22.3G    0     0  12.6M      0  0:31:43  0:30:10  0:01:33 13.1M\n",
      " 95 23.5G   95 22.4G    0     0  12.6M      0  0:31:43  0:30:11  0:01:32 13.1M\n",
      " 95 23.5G   95 22.4G    0     0  12.6M      0  0:31:43  0:30:12  0:01:31 13.1M\n",
      " 95 23.5G   95 22.4G    0     0  12.6M      0  0:31:43  0:30:13  0:01:30 13.1M\n",
      " 95 23.5G   95 22.4G    0     0  12.6M      0  0:31:42  0:30:14  0:01:28 13.2M\n",
      " 95 23.5G   95 22.4G    0     0  12.6M      0  0:31:42  0:30:15  0:01:27 13.2M\n",
      " 95 23.5G   95 22.4G    0     0  12.6M      0  0:31:42  0:30:16  0:01:26 13.3M\n",
      " 95 23.5G   95 22.4G    0     0  12.6M      0  0:31:42  0:30:17  0:01:25 13.2M\n",
      " 95 23.5G   95 22.4G    0     0  12.6M      0  0:31:42  0:30:18  0:01:24 13.2M\n",
      " 95 23.5G   95 22.5G    0     0  12.6M      0  0:31:42  0:30:19  0:01:23 13.2M\n",
      " 95 23.5G   95 22.5G    0     0  12.6M      0  0:31:42  0:30:20  0:01:22 13.1M\n",
      " 95 23.5G   95 22.5G    0     0  12.6M      0  0:31:42  0:30:21  0:01:21 13.1M\n",
      " 95 23.5G   95 22.5G    0     0  12.6M      0  0:31:42  0:30:22  0:01:20 13.1M\n",
      " 95 23.5G   95 22.5G    0     0  12.6M      0  0:31:42  0:30:23  0:01:19 13.0M\n",
      " 95 23.5G   95 22.5G    0     0  12.6M      0  0:31:42  0:30:24  0:01:18 13.0M\n",
      " 95 23.5G   95 22.5G    0     0  12.6M      0  0:31:42  0:30:25  0:01:17 13.0M\n",
      " 96 23.5G   96 22.6G    0     0  12.6M      0  0:31:42  0:30:26  0:01:16 12.9M\n",
      " 96 23.5G   96 22.6G    0     0  12.6M      0  0:31:42  0:30:27  0:01:15 12.9M\n",
      " 96 23.5G   96 22.6G    0     0  12.6M      0  0:31:42  0:30:28  0:01:14 12.9M\n",
      " 96 23.5G   96 22.6G    0     0  12.6M      0  0:31:42  0:30:29  0:01:13 12.8M\n",
      " 96 23.5G   96 22.6G    0     0  12.6M      0  0:31:42  0:30:30  0:01:12 12.9M\n",
      " 96 23.5G   96 22.6G    0     0  12.6M      0  0:31:42  0:30:31  0:01:11 12.8M\n",
      " 96 23.5G   96 22.6G    0     0  12.6M      0  0:31:42  0:30:32  0:01:10 12.7M\n",
      " 96 23.5G   96 22.6G    0     0  12.6M      0  0:31:42  0:30:33  0:01:09 12.8M\n",
      " 96 23.5G   96 22.7G    0     0  12.6M      0  0:31:42  0:30:34  0:01:08 12.9M\n",
      " 96 23.5G   96 22.7G    0     0  12.6M      0  0:31:42  0:30:35  0:01:07 12.8M\n",
      " 96 23.5G   96 22.7G    0     0  12.6M      0  0:31:42  0:30:36  0:01:06 13.0M\n",
      " 96 23.5G   96 22.7G    0     0  12.6M      0  0:31:42  0:30:37  0:01:05 13.0M\n",
      " 96 23.5G   96 22.7G    0     0  12.6M      0  0:31:42  0:30:39  0:01:03 12.3M\n",
      " 96 23.5G   96 22.7G    0     0  12.6M      0  0:31:42  0:30:39  0:01:03 12.6M\n",
      " 96 23.5G   96 22.7G    0     0  12.6M      0  0:31:42  0:30:40  0:01:02 12.7M\n",
      " 96 23.5G   96 22.7G    0     0  12.6M      0  0:31:42  0:30:41  0:01:01 12.6M\n",
      " 96 23.5G   96 22.8G    0     0  12.6M      0  0:31:42  0:30:42  0:01:00 12.8M\n",
      " 96 23.5G   96 22.8G    0     0  12.6M      0  0:31:42  0:30:43  0:00:59 13.5M\n",
      " 96 23.5G   96 22.8G    0     0  12.6M      0  0:31:42  0:30:44  0:00:58 13.2M\n",
      " 97 23.5G   97 22.8G    0     0  12.6M      0  0:31:42  0:30:45  0:00:57 13.2M\n",
      " 97 23.5G   97 22.8G    0     0  12.6M      0  0:31:42  0:30:46  0:00:56 13.2M\n",
      " 97 23.5G   97 22.8G    0     0  12.6M      0  0:31:42  0:30:47  0:00:55 13.1M\n",
      " 97 23.5G   97 22.8G    0     0  12.6M      0  0:31:42  0:30:48  0:00:54 13.1M\n",
      " 97 23.5G   97 22.8G    0     0  12.6M      0  0:31:41  0:30:49  0:00:52 13.1M\n",
      " 97 23.5G   97 22.9G    0     0  12.6M      0  0:31:41  0:30:50  0:00:51 13.1M\n",
      " 97 23.5G   97 22.9G    0     0  12.6M      0  0:31:41  0:30:51  0:00:50 13.2M\n",
      " 97 23.5G   97 22.9G    0     0  12.6M      0  0:31:41  0:30:52  0:00:49 13.2M\n",
      " 97 23.5G   97 22.9G    0     0  12.6M      0  0:31:41  0:30:53  0:00:48 13.1M\n",
      " 97 23.5G   97 22.9G    0     0  12.6M      0  0:31:41  0:30:54  0:00:47 13.1M\n",
      " 97 23.5G   97 22.9G    0     0  12.6M      0  0:31:41  0:30:55  0:00:46 13.2M\n",
      " 97 23.5G   97 22.9G    0     0  12.6M      0  0:31:41  0:30:56  0:00:45 13.3M\n",
      " 97 23.5G   97 22.9G    0     0  12.6M      0  0:31:41  0:30:57  0:00:44 13.5M\n",
      " 97 23.5G   97 23.0G    0     0  12.6M      0  0:31:41  0:30:58  0:00:43 13.6M\n",
      " 97 23.5G   97 23.0G    0     0  12.6M      0  0:31:41  0:30:59  0:00:42 13.6M\n",
      " 97 23.5G   97 23.0G    0     0  12.6M      0  0:31:41  0:31:00  0:00:41 13.6M\n",
      " 97 23.5G   97 23.0G    0     0  12.6M      0  0:31:41  0:31:01  0:00:40 13.4M\n",
      " 97 23.5G   97 23.0G    0     0  12.6M      0  0:31:41  0:31:02  0:00:39 13.3M\n",
      " 98 23.5G   98 23.0G    0     0  12.6M      0  0:31:41  0:31:03  0:00:38 13.2M\n",
      " 98 23.5G   98 23.0G    0     0  12.6M      0  0:31:41  0:31:04  0:00:37 13.1M\n",
      " 98 23.5G   98 23.1G    0     0  12.6M      0  0:31:41  0:31:05  0:00:36 13.0M\n",
      " 98 23.5G   98 23.1G    0     0  12.6M      0  0:31:41  0:31:06  0:00:35 13.0M\n",
      " 98 23.5G   98 23.1G    0     0  12.6M      0  0:31:41  0:31:07  0:00:34 12.9M\n",
      " 98 23.5G   98 23.1G    0     0  12.6M      0  0:31:41  0:31:08  0:00:33 12.9M\n",
      " 98 23.5G   98 23.1G    0     0  12.6M      0  0:31:41  0:31:09  0:00:32 12.8M\n",
      " 98 23.5G   98 23.1G    0     0  12.6M      0  0:31:41  0:31:10  0:00:31 12.7M\n",
      " 98 23.5G   98 23.1G    0     0  12.6M      0  0:31:41  0:31:11  0:00:30 12.8M\n",
      " 98 23.5G   98 23.1G    0     0  12.6M      0  0:31:41  0:31:12  0:00:29 12.4M\n",
      " 98 23.5G   98 23.2G    0     0  12.6M      0  0:31:41  0:31:13  0:00:28 12.8M\n",
      " 98 23.5G   98 23.2G    0     0  12.6M      0  0:31:40  0:31:14  0:00:26 12.8M\n",
      " 98 23.5G   98 23.2G    0     0  12.6M      0  0:31:40  0:31:15  0:00:25 12.9M\n",
      " 98 23.5G   98 23.2G    0     0  12.6M      0  0:31:40  0:31:16  0:00:24 12.9M\n",
      " 98 23.5G   98 23.2G    0     0  12.6M      0  0:31:40  0:31:17  0:00:23 13.3M\n",
      " 98 23.5G   98 23.2G    0     0  12.6M      0  0:31:40  0:31:18  0:00:22 12.9M\n",
      " 98 23.5G   98 23.2G    0     0  12.6M      0  0:31:40  0:31:19  0:00:21 12.9M\n",
      " 98 23.5G   98 23.2G    0     0  12.6M      0  0:31:40  0:31:20  0:00:20 13.0M\n",
      " 99 23.5G   99 23.3G    0     0  12.6M      0  0:31:40  0:31:21  0:00:19 12.9M\n",
      " 99 23.5G   99 23.3G    0     0  12.6M      0  0:31:40  0:31:22  0:00:18 13.1M\n",
      " 99 23.5G   99 23.3G    0     0  12.6M      0  0:31:40  0:31:23  0:00:17 13.1M\n",
      " 99 23.5G   99 23.3G    0     0  12.6M      0  0:31:40  0:31:24  0:00:16 13.1M\n",
      " 99 23.5G   99 23.3G    0     0  12.6M      0  0:31:40  0:31:25  0:00:15 13.2M\n",
      " 99 23.5G   99 23.3G    0     0  12.6M      0  0:31:40  0:31:26  0:00:14 13.3M\n",
      " 99 23.5G   99 23.3G    0     0  12.6M      0  0:31:40  0:31:27  0:00:13 13.2M\n",
      " 99 23.5G   99 23.3G    0     0  12.6M      0  0:31:40  0:31:28  0:00:12 13.2M\n",
      " 99 23.5G   99 23.4G    0     0  12.6M      0  0:31:40  0:31:29  0:00:11 13.3M\n",
      " 99 23.5G   99 23.4G    0     0  12.6M      0  0:31:40  0:31:30  0:00:10 13.1M\n",
      " 99 23.5G   99 23.4G    0     0  12.6M      0  0:31:40  0:31:31  0:00:09 13.1M\n",
      " 99 23.5G   99 23.4G    0     0  12.6M      0  0:31:40  0:31:32  0:00:08 13.2M\n",
      " 99 23.5G   99 23.4G    0     0  12.6M      0  0:31:40  0:31:33  0:00:07 13.2M\n",
      " 99 23.5G   99 23.4G    0     0  12.6M      0  0:31:40  0:31:34  0:00:06 13.1M\n",
      " 99 23.5G   99 23.4G    0     0  12.6M      0  0:31:40  0:31:35  0:00:05 13.2M\n",
      " 99 23.5G   99 23.4G    0     0  12.6M      0  0:31:40  0:31:36  0:00:04 13.1M\n",
      " 99 23.5G   99 23.5G    0     0  12.6M      0  0:31:40  0:31:37  0:00:03 13.1M\n",
      " 99 23.5G   99 23.5G    0     0  12.6M      0  0:31:40  0:31:38  0:00:02 13.1M\n",
      " 99 23.5G   99 23.5G    0     0  12.6M      0  0:31:40  0:31:39  0:00:01 13.2M\n",
      "100 23.5G  100 23.5G    0     0  12.6M      0  0:31:40  0:31:40 --:--:-- 13.1M\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "!curl -O http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/All_Amazon_Review_5.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "pJirMTJK7N3L"
   },
   "outputs": [],
   "source": [
    "balanced_data = []\n",
    "with gzip.open('All_Amazon_Review_5.json.gz') as f:\n",
    "    pos_limit = 500000\n",
    "    neg_limit = 500000\n",
    "    for l in f:\n",
    "        #Using first 1M records. All records used up all the RAM. \n",
    "        if pos_limit == 0 and neg_limit == 0:\n",
    "          break\n",
    "        json_data = json.loads(l.strip())\n",
    "        overall = json_data['overall']\n",
    "        if overall < 4.0 and neg_limit > 0: \n",
    "          balanced_data.append(json_data)\n",
    "          neg_limit -= 1\n",
    "        elif overall >= 4.0 and pos_limit > 0:\n",
    "          balanced_data.append(json_data)\n",
    "          pos_limit -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "AtUnYuVK7bve"
   },
   "outputs": [],
   "source": [
    "balanced_df = pd.DataFrame.from_dict(balanced_data)\n",
    "b_sentiment_df = pd.concat([balanced_df['unixReviewTime'], balanced_df['reviewText'], balanced_df['overall']], axis=1)\n",
    "b_sentiment_df['sentiment'] = np.where(b_sentiment_df['overall'] < 4.0, 0, 1)\n",
    "b_sentiment_df['datetimeReviewTime'] = pd.to_datetime(b_sentiment_df['unixReviewTime'], unit='s')\n",
    "# b_sentiment_df = b_sentiment_df.set_index(['datetimeReviewTime'])\n",
    "b_sentiment_df['reviewText'] = np.where(pd.isnull(b_sentiment_df['reviewText']), b_sentiment_df['reviewText'], b_sentiment_df['reviewText'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_sentiment_df['Original_text'] = b_sentiment_df['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "C1xla-pmxR-c",
    "outputId": "c18d5ba9-7db6-4f01-d487-8890d4a3ae7e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1449273600</td>\n",
       "      <td>Can only control one of two bulbs from one of two echos</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-05</td>\n",
       "      <td>Can only control one of two bulbs from one of two echos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1515974400</td>\n",
       "      <td>Great skill</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>Great skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1515024000</td>\n",
       "      <td>Not happy. Can not connect to Alexa regardless.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>Not happy. Can not connect to Alexa regardless.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1514592000</td>\n",
       "      <td>Can not connect a hue lights to Alexa. Linked the LIFX in the Amazon Alexa app. Can not located the smart hue bulbs. It should not be this hard to connect to Alexa. Even watched a you tube video and still</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>Can not connect a hue lights to Alexa. Linked the LIFX in the Amazon Alexa app. Can not located the smart hue bulbs. It should not be this hard to connect to Alexa. Even watched a you tube video and still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1514505600</td>\n",
       "      <td>The service works with google home, but doesn't work with alexa. I'm getting rid of the \"I'm  not sure\" machine.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>The service works with google home, but doesn't work with alexa. I'm getting rid of the \"I'm  not sure\" machine.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unixReviewTime  \\\n",
       "0      1449273600   \n",
       "1      1515974400   \n",
       "2      1515024000   \n",
       "3      1514592000   \n",
       "4      1514505600   \n",
       "\n",
       "                                                                                                                                                                                                     reviewText  \\\n",
       "0                                                                                                                                                       Can only control one of two bulbs from one of two echos   \n",
       "1                                                                                                                                                                                                   Great skill   \n",
       "2                                                                                                                                                               Not happy. Can not connect to Alexa regardless.   \n",
       "3  Can not connect a hue lights to Alexa. Linked the LIFX in the Amazon Alexa app. Can not located the smart hue bulbs. It should not be this hard to connect to Alexa. Even watched a you tube video and still   \n",
       "4                                                                                              The service works with google home, but doesn't work with alexa. I'm getting rid of the \"I'm  not sure\" machine.   \n",
       "\n",
       "   overall  sentiment datetimeReviewTime  \\\n",
       "0      2.0          0         2015-12-05   \n",
       "1      5.0          1         2018-01-15   \n",
       "2      1.0          0         2018-01-04   \n",
       "3      1.0          0         2017-12-30   \n",
       "4      1.0          0         2017-12-29   \n",
       "\n",
       "                                                                                                                                                                                                  Original_text  \n",
       "0                                                                                                                                                       Can only control one of two bulbs from one of two echos  \n",
       "1                                                                                                                                                                                                   Great skill  \n",
       "2                                                                                                                                                               Not happy. Can not connect to Alexa regardless.  \n",
       "3  Can not connect a hue lights to Alexa. Linked the LIFX in the Amazon Alexa app. Can not located the smart hue bulbs. It should not be this hard to connect to Alexa. Even watched a you tube video and still  \n",
       "4                                                                                              The service works with google home, but doesn't work with alexa. I'm getting rid of the \"I'm  not sure\" machine.  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['Original_text'] = balanced_df['reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "iPhHXTcCG0QC",
    "outputId": "b139cff7-556d-477c-ea02-4d1f7fe08e97"
   },
   "source": [
    "## Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 13)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "      <th>style</th>\n",
       "      <th>Original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>12 5, 2015</td>\n",
       "      <td>A3KUPJ396OQF78</td>\n",
       "      <td>B017O9P72A</td>\n",
       "      <td>Larry Russlin</td>\n",
       "      <td>Can only control one of two bulbs from one of two echos</td>\n",
       "      <td>Buggy</td>\n",
       "      <td>1449273600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can only control one of two bulbs from one of two echos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>01 15, 2018</td>\n",
       "      <td>A3TXR8GLKS19RE</td>\n",
       "      <td>B017O9P72A</td>\n",
       "      <td>Nello</td>\n",
       "      <td>Great skill</td>\n",
       "      <td>Great</td>\n",
       "      <td>1515974400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Great skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>01 4, 2018</td>\n",
       "      <td>A1FOHYK23FJ6CN</td>\n",
       "      <td>B017O9P72A</td>\n",
       "      <td>L. Ray Humphreys</td>\n",
       "      <td>Not happy. Can not connect to Alexa regardless.</td>\n",
       "      <td>Can not connect to ECHO</td>\n",
       "      <td>1515024000</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not happy. Can not connect to Alexa regardless.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0      2.0     False   12 5, 2015  A3KUPJ396OQF78  B017O9P72A   \n",
       "1      5.0     False  01 15, 2018  A3TXR8GLKS19RE  B017O9P72A   \n",
       "2      1.0     False   01 4, 2018  A1FOHYK23FJ6CN  B017O9P72A   \n",
       "\n",
       "       reviewerName                                               reviewText  \\\n",
       "0     Larry Russlin  Can only control one of two bulbs from one of two echos   \n",
       "1             Nello                                              Great skill   \n",
       "2  L. Ray Humphreys          Not happy. Can not connect to Alexa regardless.   \n",
       "\n",
       "                   summary  unixReviewTime vote image style  \\\n",
       "0                    Buggy      1449273600  NaN   NaN   NaN   \n",
       "1                    Great      1515974400  NaN   NaN   NaN   \n",
       "2  Can not connect to ECHO      1515024000    2   NaN   NaN   \n",
       "\n",
       "                                             Original_text  \n",
       "0  Can only control one of two bulbs from one of two echos  \n",
       "1                                              Great skill  \n",
       "2          Not happy. Can not connect to Alexa regardless.  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfKwawobrKKJ",
    "outputId": "06ce3ca1-b6e7-4f85-f51f-704cb4377944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average words in a sentence is :  53.235652\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "req_df =b_sentiment_df\n",
    "req_df['reviewText'] = copy.deepcopy(b_sentiment_df['reviewText'])\n",
    "#calculate word count\n",
    "\n",
    "req_df['words'] = [len(str(x).split()) for x in req_df['reviewText'].tolist()]\n",
    "word_count = req_df['words']\n",
    "avg_word_count = sum(word_count) / len(word_count)\n",
    "print(\"Average words in a sentence is : \", avg_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "to1PS6HrQGcc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEICAYAAACXo2mmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdr0lEQVR4nO3df5RfdX3n8eerwYBu1QCJGkLCBB10E3VjGBNaheWsskxyWEdt1aRWfshuyJ6khe3pWYL2COs5WNaKrtQ0Y5QcSGsJtKk41exGdBWqkpKkpCETCAwhwMhsCKQNbBPDTnjvH/czcPPN9zvfOzP3JnyT1+Oce773fn7dzyf3MG/ur89VRGBmZlaGXzvWHTAzs+OHg4qZmZXGQcXMzErjoGJmZqVxUDEzs9I4qJiZWWkqDSqSOiXtkNQnaVmdfEm6JeVvlTQ7pU+V9BNJD0vqlXR1rs5pku6R9Fj6PTWXd11qa4eki6scm5mZHUlVvaciaRzwKHAR0A9sBBZGxPZcmfnA7wHzgbnA1yNirqTJwOSI+AdJbwQ2Ax+NiO2SvgzsjYibUqA6NSKulTQDuAOYA5wB/Ag4JyIONerjxIkTo62trfzBm5kdxzZv3vxcREyql3dShfudA/RFxE4ASWuALmB7rkwXsDqyyLZB0gRJkyNiABgAiIgXJT0MTEl1u4ALU/3bgZ8C16b0NRFxEHhCUl/qw/2NOtjW1samTZtKGq6Z2YlB0pON8qq8/DUFeDq33Z/SRlRGUhvwPuDvU9JbU9Ah/b5lBPszM7MKVRlUVCet9lrbsGUk/TqwFrgmIl4oYX9IWiRpk6RNe/bsadKkmZmNRJVBpR+Ymts+E3imaBlJryMLKN+JiL/Jldmd7rmQfp8dwf6IiJUR0RERHZMm1b0kaGZmo1RlUNkItEuaLmk8sADoqSnTA1yangI7D9gXEQOSBNwKPBwRX61T57K0fhnwvVz6AkknS5oOtAMPlD8sMzNrpLIb9RExKGkpsB4YB6yKiF5Ji1N+N7CO7MmvPmA/cEWq/gHgM8BDkraktM9FxDrgJuAuSVcCTwGfSO31SrqL7Gb+ILBkuCe/zMysfJU9UtwKOjo6wk9/mZmNjKTNEdFRL89v1JuZWWkcVMzMrDQOKmZmVpoq36g/7i1e/Dl27Xq+1Dbb2k6nu/tLpbZpZna0OKiMwa5dz3PWWd8suc2rSm3PzOxo8uUvMzMrjYOKmZmVxkHFzMxK46BiZmalcVAxM7PSOKiYmVlpHFTMzKw0DipmZlYaBxUzMyuNg4qZmZXGQcXMzErjoGJmZqVxUDEzs9JUGlQkdUraIalP0rI6+ZJ0S8rfKml2Lm+VpGclbaupc6ekLWnZNfQNe0ltkg7k8rqrHJuZmR2psqnvJY0DlgMXAf3ARkk9EbE9V2we0J6WucCK9AtwG/ANYHW+3Yj4VG4fNwP7ctmPR8SsUgdiZmaFVXmmMgfoi4idEfESsAboqinTBayOzAZggqTJABFxH7C3UeOSBHwSuKOS3puZ2YhVGVSmAE/ntvtT2kjLNHI+sDsiHsulTZf0oKR7JZ0/0g6bmdnYVPnlR9VJi1GUaWQhh5+lDADTIuJ5SecCd0uaGREvHLZDaRGwCGDatGkFd2VmZkVUeabSD0zNbZ8JPDOKMkeQdBLwceDOobSIOBgRz6f1zcDjwDm1dSNiZUR0RETHpEmTCg7FzMyKqDKobATaJU2XNB5YAPTUlOkBLk1PgZ0H7IuIgQJtfxh4JCL6hxIkTUoPByDpbLKb/zvLGIiZmRVT2eWviBiUtBRYD4wDVkVEr6TFKb8bWAfMB/qA/cAVQ/Ul3QFcCEyU1A9cHxG3puwFHHmD/gLgi5IGgUPA4ohoeKPfzMzKV+U9FSJiHVngyKd159YDWNKg7sJh2r28TtpaYO1o+2pmZmPnN+rNzKw0DipmZlYaBxUzMyuNg4qZmZXGQcXMzErjoGJmZqVxUDEzs9I4qJiZWWkcVMzMrDQOKmZmVhoHFTMzK42DipmZlcZBxczMSuOgYmZmpXFQMTOz0jiomJlZaRxUzMysNA4qZmZWmkqDiqROSTsk9UlaVidfkm5J+Vslzc7lrZL0rKRtNXVukPRLSVvSMj+Xd11qa4eki6scm5mZHamyoCJpHLAcmAfMABZKmlFTbB7QnpZFwIpc3m1AZ4PmvxYRs9KyLu1vBrAAmJnq/Vnqg5mZHSVVnqnMAfoiYmdEvASsAbpqynQBqyOzAZggaTJARNwH7B3B/rqANRFxMCKeAPpSH8zM7CipMqhMAZ7ObfentJGWqWdpuly2StKpY2zLzMxKUmVQUZ20GEWZWiuAtwOzgAHg5pG0JWmRpE2SNu3Zs6fJrszMbCSqDCr9wNTc9pnAM6Moc5iI2B0RhyLiZeBbvHqJq1BbEbEyIjoiomPSpEmFBmJmZsVUGVQ2Au2SpksaT3YTvaemTA9waXoK7DxgX0QMDNfo0D2X5GPA0NNhPcACSSdLmk528/+BMgZiZmbFnFRVwxExKGkpsB4YB6yKiF5Ji1N+N7AOmE92U30/cMVQfUl3ABcCEyX1A9dHxK3AlyXNIru0tQu4KrXXK+kuYDswCCyJiENVjc/MzI5UWVABSI/7rqtJ686tB7CkQd2FDdI/M8z+bgRuHFVnzcxszPxGvZmZlcZBxczMSuOgYmZmpXFQMTOz0jiomJlZaRxUzMysNA4qZmZWGgcVMzMrjYOKmZmVxkHFzMxK46BiZmalcVAxM7PSOKiYmVlpHFTMzKw0DipmZlYaBxUzMyuNg4qZmZXGQcXMzEpTaVCR1Clph6Q+Scvq5EvSLSl/q6TZubxVkp6VtK2mzp9IeiSV/66kCSm9TdIBSVvS0o2ZmR1VlQUVSeOA5cA8YAawUNKMmmLzgPa0LAJW5PJuAzrrNH0P8O6IeC/wKHBdLu/xiJiVlsWlDMTMzAo7qUghSZOA/wS05etExGeHqTYH6IuInamNNUAXsD1XpgtYHREBbJA0QdLkiBiIiPsktdU2GhE/zG1uAH67yBjMzKx6hYIK8D3g74AfAYcK1pkCPJ3b7gfmFigzBRgouI/PAnfmtqdLehB4AfijiPi7gu2YmVkJigaVN0TEtSNsW3XSYhRl6jcufR4YBL6TkgaAaRHxvKRzgbslzYyIF2rqLSK71Ma0adOK7Oqo6u3dRmfnVaW329Z2Ot3dXyq9XTOzvKJB5fuS5kfEuhG03Q9MzW2fCTwzijJHkHQZcAnwoXTpjIg4CBxM65slPQ6cA2zK142IlcBKgI6OjkIB7Gg6cECcddY3S293167yA5WZWa2iN+qvJgssv5L0YlpeaFJnI9Auabqk8cACoKemTA9waXoK7DxgX0QMe+lLUidwLfCRiNifS5+UHg5A0tlkN/93FhyfmZmVoNCZSkS8caQNR8SgpKXAemAcsCoieiUtTvndwDpgPtAH7AeuGKov6Q7gQmCipH7g+oi4FfgGcDJwjySADelJrwuAL0oaJLvvszgi9o6032ZmNnpFL38h6SNkf7gBfhoR329WJ10uW1eT1p1bD2BJg7oLG6S/o0H6WmBtsz6ZmVl1Cl3+knQT2SWw7Wm5OqWZmZm9ouiZynxgVkS8DCDpduBB4Ii35M3M7MQ1kjfqJ+TW31xyP8zM7DhQ9Ezlj4EHJf2E7N2SCzh8ehQzM7PCT3/dIemnwPvJgsq1EfF/quyYmZm1nmEvf0l6V/qdDUwme1nxaeCM/IzCZmZm0PxM5Q/IpjS5uU5eAP+u9B6ZmVnLGjaoRMSitDovIn6Vz5N0SmW9MjOzllT06a9fFEwzM7MT2LBnKpLeRjYV/eslvY9XZxV+E/CGivtmZmYtptk9lYuBy8lmD/5qLv1F4HMV9cnMzFpUs3sqtwO3S/qtNLeWmZlZQ0Vffny3pJm1iRHxxZL7Y2ZmLaxoUPm/ufVTyD6Q9XD53TEzs1ZW9I36w95TkfQVjvzglpmZneBGMqFk3huAs8vsiJmZtb5CZyqSHiJ7gx6yrzhOAnw/xczMDlP0nsolufVBYHdEDFbQHzMza2GFLn9FxJPA6UAX8HHgPUXqSeqUtENSn6QjPuilzC0pf2t+kkpJqyQ9K2lbTZ3TJN0j6bH0e2ou77rU1g5JFxfpo5mZlafo54S/ANxOFlgmArdJ+qMmdcYBy4F5wAxgoaQZNcXmAe1pWQSsyOXdBnTWaXoZ8OOIaAd+nLZJbS8AZqZ6f5b6YGZmR0nRG/ULgfdHxPURcT1wHvDpJnXmAH0RsTMiXgLWkJ3p5HUBqyOzAZggaTJARNwH7K3TbhdZgCP9fjSXviYiDkbEE0Bf6oOZmR0lRYPKLrL3U4acDDzepM4Usm+vDOlPaSMtU+utETEAkH7fMpK2JC2StEnSpj179jTZlZmZjUSzCSX/lOypr4NAr6R70vZFwM+atK06aTGKMkUVaisiVgIrATo6Oka7LzMzq6PZ01+b0u9m4Lu59J8WaLsfmJrbPhN4ZhRlau2WNDkiBtKlsmfH0JaZmZWoyISSo7URaJc0Hfgl2U3036kp0wMslbQGmAvsG7q0NYwe4DLgpvT7vVz6X0r6KnAG2c3/B8bQfzMzG6Fml7/uiohP1rz8+IqIeG+juhExKGkpsJ7shclVEdEraXHK7wbWAfPJbqrvB67I7fsO4EJgoqR+4PqIuJUsmNwl6UrgKeATqb1eSXcB28nepVkSEYeK/TOYmVkZml3+ujr9XjJsqQYiYh1Z4MindefWA1jSoO7CBunPAx9qkHcjcONo+mpmZmPX7PLXQHrX49aI+PBR6pOZmbWopo8Up0tI+yW9+Sj0x8zMWljRub9+BTyUHin+l6HEiPj9SnplZmYtqWhQ+UFa8vyOh5mZHaZoUJkQEV/PJ0i6ulFhMzM7MRWdpuWyOmmXl9gPMzM7DjR7T2Uh2QuL0yXlPx/8JuD5KjtmZmatp9nlr18AA2TT3ee/U/8isLWqTpmZWWtq9p7Kk8CTkj4MHIiIlyWdA7wLeOhodNDMzFpH0Xsq9wGnSJpC9mGsK8g+omVmZvaKokFFEbGf7FPCfxoRHyP7mqOZmdkrCgcVSb9B9rXHofdVij6ObGZmJ4iiQeUa4Drgu2k24LOBn1TWKzMza0mFzjYi4l7g3tz2TsBTtJiZ2WGavafyPyLiGkl/S/3vqXyksp6ZmVnLaXam8ufp9ytVd8TMzFpfs/dUNqffeyVNSut7jkbHzMys9Qx7o16ZGyQ9BzwCPCppj6QvFGlcUqekHZL6JC1r0P4tKX+rpNnN6kq6U9KWtOyStCWlt0k6kMvrrt2fmZlVq9nlr2uADwDvj4gnANKTXysk/ZeI+FqjiumLkcuBi4B+YKOknojYnis2D2hPy1xgBTB3uLoR8ancPm4G9uXaezwiZjUftpmZVaHZI8WXAguHAgq88uTX76a84cwB+iJiZ0S8BKwBumrKdAGrI7MBmCBpcpG6kgR8ErijST/MzOwoaRZUXhcRz9Umpvsqr2tSdwrwdG67P6UVKVOk7vnA7oh4LJc2XdKDku6VdH6T/pmZWcmaXf56aZR5AKqTVvtYcqMyReou5PCzlAFgWkQ8L+lc4G5JMyPihcN2KC0CFgFMmzZtmO6bmdlINQsq/0bSC3XSBZzSpG4/MDW3fSbwTMEy44erK+kksnnIzh1Ki4iDwMG0vlnS48A5wKb8DiNiJbASoKOjw59ENjMr0bCXvyJiXES8qc7yxohodvlrI9Auabqk8cACoKemTA9waXoK7DxgX0QMFKj7YeCRiOgfSpA0Kd3gH3qYoB3Y2fRfwMzMSlPZpJARMShpKbAeGAesSvOGLU753cA6YD7QB+wnm1K/Yd1c8ws48gb9BcAXJQ0Ch4DFEbG3qvGZmdmRKp1pOCLWkQWOfFp3bj2AJUXr5vIur5O2Flg7hu6amdkYFZ2l2MzMrCkHFTMzK42DipmZlcZBxczMSuOgYmZmpXFQMTOz0jiomJlZaRxUzMysNA4qZmZWmkrfqLfXjt7ebXR2XlV6u21tp9Pd/aXS2zWz1uSgcoI4cECcddY3S293167yA5WZtS5f/jIzs9I4qJiZWWkcVMzMrDQOKmZmVhoHFTMzK42DipmZlcZBxczMSlNpUJHUKWmHpD5Jy+rkS9ItKX+rpNnN6kq6QdIvJW1Jy/xc3nWp/A5JF1c5NjMzO1JlLz9KGgcsBy4C+oGNknoiYnuu2DygPS1zgRXA3AJ1vxYRX6nZ3wxgATATOAP4kaRzIuJQVWM0M7PDVXmmMgfoi4idEfESsAboqinTBayOzAZggqTJBevW6gLWRMTBiHgC6EvtmJnZUVJlUJkCPJ3b7k9pRco0q7s0XS5bJenUEezPzMwqVGVQUZ20KFhmuLorgLcDs4AB4OYR7A9JiyRtkrRpz549daqYmdloVRlU+oGpue0zgWcKlmlYNyJ2R8ShiHgZ+BavXuIqsj8iYmVEdEREx6RJk0Y8KDMza6zKoLIRaJc0XdJ4spvoPTVleoBL01Ng5wH7ImJguLrpnsuQjwHbcm0tkHSypOlkN/8fqGpwZmZ2pMqe/oqIQUlLgfXAOGBVRPRKWpzyu4F1wHyym+r7gSuGq5ua/rKkWWSXtnYBV6U6vZLuArYDg8ASP/llZnZ0Vfo9lYhYRxY48mndufUAlhStm9I/M8z+bgRuHG1/zcxsbPxGvZmZlcZBxczMSuOgYmZmpXFQMTOz0jiomJlZaRxUzMysNA4qZmZWGgcVMzMrjYOKmZmVptI36u3419u7jc7Oq0pvt63tdLq7v1R6u2ZWLQcVG5MDB8RZZ32z9HZ37So/UJlZ9Xz5y8zMSuOgYmZmpXFQMTOz0jiomJlZaRxUzMysNA4qZmZWGgcVMzMrTaVBRVKnpB2S+iQtq5MvSbek/K2SZjerK+lPJD2Syn9X0oSU3ibpgKQtaemu3Z+ZmVWrsqAiaRywHJgHzAAWSppRU2we0J6WRcCKAnXvAd4dEe8FHgWuy7X3eETMSsviakZmZmaNVHmmMgfoi4idEfESsAboqinTBayOzAZggqTJw9WNiB9GxGCqvwE4s8IxmJnZCFQZVKYAT+e2+1NakTJF6gJ8Fvifue3pkh6UdK+k80fbcTMzG50q5/5SnbQoWKZpXUmfBwaB76SkAWBaRDwv6VzgbkkzI+KFmnqLyC61MW3atKaDMDOz4qoMKv3A1Nz2mcAzBcuMH66upMuAS4APRUQARMRB4GBa3yzpceAcYFN+hxGxElgJ0NHRURvk7DWiitmPPfOxWfWqDCobgXZJ04FfAguA36kp0wMslbQGmAvsi4gBSXsa1ZXUCVwL/NuI2D/UkKRJwN6IOCTpbLKb/zsrHJ9VqIrZjz3zsVn1KgsqETEoaSmwHhgHrIqIXkmLU343sA6YD/QB+4Erhqubmv4GcDJwjySADelJrwuAL0oaBA4BiyNib1XjMzOzI1X6PZWIWEcWOPJp3bn1AJYUrZvS39Gg/Fpg7Vj6a2ZmY+M36s3MrDQOKmZmVhoHFTMzK42DipmZlabSG/VmryVVvPsCfv/FLM9BxU4YVbz7An7/xSzPl7/MzKw0DipmZlYaBxUzMyuN76mYjZEfADB7lYOK2Rj5AQCzV/nyl5mZlcZBxczMSuPLX2avUf5QmbUiBxWz1yh/qMxakYOK2QnET6pZ1RxUzE4gflLNquagYmZj5jMgG1JpUJHUCXyd7Dvz346Im2rylfLnk32j/vKI+Ifh6ko6DbgTaAN2AZ+MiH9KedcBV5J9o/73I2J9leMzs0xVZ0A/+MEHHKxaTGVBRdI4YDlwEdAPbJTUExHbc8XmAe1pmQusAOY2qbsM+HFE3CRpWdq+VtIMYAEwEzgD+JGkcyLiUFVjNLNqtVKweuqpPqZNe0epbULrBcAqz1TmAH0RsRNA0hqgC8gHlS5gdUQEsEHSBEmTyc5CGtXtAi5M9W8Hfgpcm9LXRMRB4AlJfakP91c4RjNrQVUEq02bPsj557dGAITqglWVQWUK8HRuu5/sbKRZmSlN6r41IgYAImJA0ltybW2o05aZWctqtYcrlJ0kVNCw9Ang4oj4j2n7M8CciPi9XJkfAH8cET9L2z8G/itwdqO6kv45Iibk2viniDhV0nLg/oj4i5R+K7AuItbW9GsRsChtvhPYMYZhTgSeG0P91yqPq/Ucr2PzuF6bzoqISfUyqjxT6Qem5rbPBJ4pWGb8MHV3S5qczlImA8+OYH9ExEpg5ciGUp+kTRHRUUZbryUeV+s5XsfmcbWeKuf+2gi0S5ouaTzZTfSemjI9wKXKnAfsS5e2hqvbA1yW1i8DvpdLXyDpZEnTyW7+P1DV4MzM7EiVnalExKCkpcB6sseCV0VEr6TFKb8bWEf2OHEf2SPFVwxXNzV9E3CXpCuBp4BPpDq9ku4iu5k/CCzxk19mZkdXZfdUTgSSFqXLaccVj6v1HK9j87haj4OKmZmVxt9TMTOz0jiojIKkTkk7JPWlt/pblqRdkh6StEXSppR2mqR7JD2Wfk891v0sQtIqSc9K2pZLazgWSdelY7hD0sXHptfNNRjXDZJ+mY7bFknzc3mtMq6pkn4i6WFJvZKuTuktfcyGGVfLH7NCIsLLCBayBwceJ3uXZjzwj8CMY92vMYxnFzCxJu3LwLK0vgz478e6nwXHcgEwG9jWbCzAjHTsTgamp2M67liPYQTjugH4wzplW2lck4HZaf2NwKOp/y19zIYZV8sfsyKLz1RG7pXpZyLiJWBoCpnjSRfZFDik348eu64UFxH3AXtrkhuN5ZVpfSLiCbInEOccjX6OVINxNdJK4xqINIFsRLwIPEw2C0ZLH7NhxtVIS4yrKAeVkWs0tUyrCuCHkjan2QagZioc4C0Na7/2NRrL8XAcl0rami6PDV0iaslxSWoD3gf8PcfRMasZFxxHx6wRB5WRU520Vn6E7gMRMZtsxuglki441h06Slr9OK4A3g7MAgaAm1N6y41L0q8Da4FrIuKF4YrWSXvNjq3OuI6bYzYcB5WRKzQdTKuIiGfS77PAd8lOu3enKXComQqnFTUaS0sfx4jYHRGHIuJl4Fu8ermkpcYl6XVkf3i/ExF/k5Jb/pjVG9fxcsyacVAZuSLTz7QESf9K0huH1oF/D2yj8VQ4rei4nNZn6I9u8jGy4wYtNC5JAm4FHo6Ir+ayWvqYNRrX8XDMCjnWTwq04kI2tcyjZE9pfP5Y92cM4zib7KmTfwR6h8YCnA78GHgs/Z52rPtacDx3kF1W+H9k//d35XBjAT6fjuEOYN6x7v8Ix/XnwEPAVrI/SpNbcFwfJLvMsxXYkpb5rX7MhhlXyx+zIovfqDczs9L48peZmZXGQcXMzErjoGJmZqVxUDEzs9I4qJiZWWkcVMwKkPQ1SdfkttdL+nZu+2ZJfzDKti+U9P0GeXMk3Zdmr31E0rclvWE0+xlm/5dLOqPMNu3E5aBiVswvgN8EkPRrwERgZi7/N4GfF2lI0riC5d4K/BVwbUS8E/jXwP8im/m2TJcDDipWCgcVs2J+TgoqZMFkG/CipFMlnUz2B/9BSR+S9GD6Rs2qlDf03ZovSPoZ8All3+R5JG1/vME+lwC3R8T9AJH564jYnb45cneanHCDpPem/dwg6Q+HGpC0TVJbWh6W9K30jY8fSnq9pN8GOoDvpG98vL78fzo7kTiomBUQ2Rxpg5KmkQWX+8lmnv0Nsj/KW8n+e7oN+FREvAc4CfjPuWZ+FREfBO4mm/vpPwDnA29rsNt3A5sb5P034MGIeC/wOWB1gWG0A8sjYibwz8BvRcRfA5uAT0fErIg4UKAds4YcVMyKGzpbGQoq9+e2fwG8E3giIh5N5W8n+8DWkDvT77tSuccim9LiL0bRlw+STftBRPxv4HRJb25S54mI2JLWNwNto9iv2bAcVMyKG7qv8h6yy18byM5Uhu6n1JvCPO9fcutF5kfqBc5tkNdouvRBDv/v+pTc+sHc+iGyMymzUjmomBX3c+ASYG9kU5jvBSaQBZb7gUeANknvSOU/A9xbp51HgOmS3p62FzbY3zeAyyTNHUqQ9LuS3gbcB3w6pV0IPBfZNzt2kX16GEmzyT5P28yLlH/z305QDipmxT1E9tTXhpq0fRHxXET8CrgC+CtJDwEvA921jaRyi4AfpBv1T9bbWUTsJvu0wlfSI8UPk92DeYHse+cdkrYCN/HqVPFrgdMkbSG7n/Nobbt13AZ0+0a9lcGzFJuZWWl8pmJmZqVxUDEzs9I4qJiZWWkcVMzMrDQOKmZmVhoHFTMzK42DipmZlcZBxczMSvP/AYt53vZr4Zq3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "# fixed bin size\n",
    "bins=range(0,300,20) # fixed number of bins \n",
    "plt.ylabel(\"Distribution\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.hist(word_count, bins=bins, density=True, alpha=0.6, color='b', edgecolor='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "nexMSWMB5UzA"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "Label_overall = req_df['overall']\n",
    "plt.ylabel(\"Distribution\")\n",
    "plt.xlabel(\"Overall Rating\")\n",
    "plt.hist(Label_overall, density=True, color='b', edgecolor='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z9gni5Fy0Cdl",
    "outputId": "9070091e-53c7-4f3e-eac3-5745e0451f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 95% confidence interval for the population mean : (53.0079293431603, 53.463374656839704)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "#Calculate the sample parameters\n",
    "confidenceLevel = 0.99   # 95% CI given\n",
    "degrees_freedom = len(word_count)-1  #degree of freedom = sample size-1\n",
    "sampleMean = np.mean(word_count)    #sample mean\n",
    "sampleStandardError = st.sem(word_count)  #sample standard error\n",
    "#create 95% confidence interval for the population mean\n",
    "confidenceInterval = st.t.interval(alpha=confidenceLevel, df=degrees_freedom, loc=sampleMean, scale=sampleStandardError)\n",
    "print('The 95% confidence interval for the population mean :',confidenceInterval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "Yd2ls_Ef7wkn"
   },
   "outputs": [],
   "source": [
    "# df_13 = b_sentiment_df.loc['2013-01-01':'2014-01-01']\n",
    "\n",
    "df_13 = b_sentiment_df[(b_sentiment_df['datetimeReviewTime'] >= '2013-01-01') & (b_sentiment_df['datetimeReviewTime'] < '2014-01-01')]\n",
    "df_14 = b_sentiment_df[(b_sentiment_df['datetimeReviewTime'] >= '2014-01-01') & (b_sentiment_df['datetimeReviewTime'] < '2015-01-01')]\n",
    "df_15 = b_sentiment_df[(b_sentiment_df['datetimeReviewTime'] >= '2015-01-01') & (b_sentiment_df['datetimeReviewTime'] < '2016-01-01')]\n",
    "df_16 = b_sentiment_df[(b_sentiment_df['datetimeReviewTime'] >= '2016-01-01') & (b_sentiment_df['datetimeReviewTime'] < '2017-01-01')]\n",
    "df_17 = b_sentiment_df[(b_sentiment_df['datetimeReviewTime'] >= '2017-01-01') & (b_sentiment_df['datetimeReviewTime'] < '2018-01-01')]\n",
    "df_18 = b_sentiment_df[(b_sentiment_df['datetimeReviewTime'] >= '2018-01-01') & (b_sentiment_df['datetimeReviewTime'] < '2019-01-01')]\n",
    "\n",
    "# df_14 = b_sentiment_df.loc['2014-01-01':'2015-01-01']\n",
    "# df_15 = b_sentiment_df.loc['2015-01-01':'2016-01-01']\n",
    "# df_16 = b_sentiment_df.loc['2016-01-01':'2017-01-01']\n",
    "# df_17 = b_sentiment_df.loc['2017-01-01':'2018-01-01']\n",
    "# df_18 = b_sentiment_df.loc['2018-01-01':'2019-01-01']\n",
    "\n",
    "\n",
    "sample_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "m2oI34EOy5Dw"
   },
   "outputs": [],
   "source": [
    "df_total = pd.concat([df_13.sample(sample_size, random_state=123), df_14.sample(sample_size, random_state=123), df_15.sample(sample_size, random_state=123), df_16.sample(sample_size, random_state=123), df_17.sample(sample_size, random_state=123), df_18.sample(sample_size, random_state=123)]).dropna()\n",
    "\n",
    "train_data, test_data, train_sentiment, test_sentiment = train_test_split(df_total.drop('sentiment', axis=1), df_total['sentiment'], test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 742
    },
    "id": "j3uhGdWRx4TX",
    "outputId": "57548b0f-15ca-48d7-a5af-77a79455262e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59978, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>997698</th>\n",
       "      <td>1375401600</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-02</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94687</th>\n",
       "      <td>1373587200</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297302</th>\n",
       "      <td>1368921600</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-19</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unixReviewTime  \\\n",
       "997698      1375401600   \n",
       "94687       1373587200   \n",
       "297302      1368921600   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  reviewText  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        overall  sentiment datetimeReviewTime  \\\n",
       "997698      3.0          0         2013-08-02   \n",
       "94687       5.0          1         2013-07-12   \n",
       "297302      4.0          1         2013-05-19   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Original_text  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        words  \n",
       "997698     24  \n",
       "94687      27  \n",
       "297302    512  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_total.shape)\n",
    "df_total.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quVDXZ-Inxoi",
    "outputId": "beca87ed-6c70-466a-a7b6-dec3c6c706b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5084613215045768\n",
      "0.5103589026393248\n",
      "0.5437046892526428\n",
      "0.4826891348434331\n",
      "0.4724139423180943\n",
      "0.47708296004300305\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(df_13['sentiment']))\n",
    "print(np.mean(df_14['sentiment']))\n",
    "print(np.mean(df_15['sentiment']))\n",
    "print(np.mean(df_16['sentiment']))\n",
    "print(np.mean(df_17['sentiment']))\n",
    "print(np.mean(df_18['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "Label_overall = df_total['overall']\n",
    "plt.ylabel(\"Distribution\")\n",
    "plt.xlabel(\"Overall Rating\")\n",
    "plt.hist(Label_overall, density=True, color='b', edgecolor='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79286     1\n",
       "483239    0\n",
       "234320    1\n",
       "252545    1\n",
       "652109    0\n",
       "         ..\n",
       "866755    0\n",
       "964939    0\n",
       "485244    1\n",
       "495110    1\n",
       "725828    0\n",
       "Name: sentiment, Length: 47982, dtype: int32"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(train_sentiment))\n",
    "train_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for various text combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59978, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>997698</th>\n",
       "      <td>1375401600</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-02</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94687</th>\n",
       "      <td>1373587200</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297302</th>\n",
       "      <td>1368921600</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-19</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unixReviewTime  \\\n",
       "997698      1375401600   \n",
       "94687       1373587200   \n",
       "297302      1368921600   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  reviewText  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        overall  sentiment datetimeReviewTime  \\\n",
       "997698      3.0          0         2013-08-02   \n",
       "94687       5.0          1         2013-07-12   \n",
       "297302      4.0          1         2013-05-19   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Original_text  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        words  \n",
       "997698     24  \n",
       "94687      27  \n",
       "297302    512  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_total.shape)\n",
    "df_total.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_total['reviewText_lower'] =df_total['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>997698</th>\n",
       "      <td>1375401600</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-02</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94687</th>\n",
       "      <td>1373587200</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297302</th>\n",
       "      <td>1368921600</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-19</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unixReviewTime  \\\n",
       "997698      1375401600   \n",
       "94687       1373587200   \n",
       "297302      1368921600   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  reviewText  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        overall  sentiment datetimeReviewTime  \\\n",
       "997698      3.0          0         2013-08-02   \n",
       "94687       5.0          1         2013-07-12   \n",
       "297302      4.0          1         2013-05-19   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Original_text  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        words  \n",
       "997698     24  \n",
       "94687      27  \n",
       "297302    512  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_total['reviewText_lower'] = df_total['reviewText_lower'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>997698</th>\n",
       "      <td>1375401600</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-02</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94687</th>\n",
       "      <td>1373587200</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297302</th>\n",
       "      <td>1368921600</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-19</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unixReviewTime  \\\n",
       "997698      1375401600   \n",
       "94687       1373587200   \n",
       "297302      1368921600   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  reviewText  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        overall  sentiment datetimeReviewTime  \\\n",
       "997698      3.0          0         2013-08-02   \n",
       "94687       5.0          1         2013-07-12   \n",
       "297302      4.0          1         2013-05-19   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Original_text  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        words  \n",
       "997698     24  \n",
       "94687      27  \n",
       "297302    512  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#df_total['reviewText_stop_Lower'] = df_total['reviewText_lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>997698</th>\n",
       "      <td>1375401600</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-02</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94687</th>\n",
       "      <td>1373587200</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297302</th>\n",
       "      <td>1368921600</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-19</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unixReviewTime  \\\n",
       "997698      1375401600   \n",
       "94687       1373587200   \n",
       "297302      1368921600   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  reviewText  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        overall  sentiment datetimeReviewTime  \\\n",
       "997698      3.0          0         2013-08-02   \n",
       "94687       5.0          1         2013-07-12   \n",
       "297302      4.0          1         2013-05-19   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Original_text  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        words  \n",
       "997698     24  \n",
       "94687      27  \n",
       "297302    512  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for various text combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in df_total.iterrows(): \n",
    "  training_string = str(x['datetimeReviewTime'].year) + ' ' +  str(x['reviewText'])\n",
    "  df_total.loc[i, 'text_with_year'] = training_string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59978, 8)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in df_total.iterrows():\n",
    "    year_string = str(x['datetimeReviewTime'].year)\n",
    "    df_total.loc[i, 'Year_string'] = year_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "      <th>words</th>\n",
       "      <th>text_with_year</th>\n",
       "      <th>Year_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>997698</th>\n",
       "      <td>1375401600</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-02</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>24</td>\n",
       "      <td>2013 I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94687</th>\n",
       "      <td>1373587200</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>27</td>\n",
       "      <td>2013 This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297302</th>\n",
       "      <td>1368921600</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-19</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>512</td>\n",
       "      <td>2013 I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unixReviewTime  \\\n",
       "997698      1375401600   \n",
       "94687       1373587200   \n",
       "297302      1368921600   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  reviewText  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        overall  sentiment datetimeReviewTime  \\\n",
       "997698      3.0          0         2013-08-02   \n",
       "94687       5.0          1         2013-07-12   \n",
       "297302      4.0          1         2013-05-19   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Original_text  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        words  \\\n",
       "997698     24   \n",
       "94687      27   \n",
       "297302    512   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text_with_year  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2013 I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  2013 This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  2013 I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "       Year_string  \n",
       "997698        2013  \n",
       "94687         2013  \n",
       "297302        2013  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['Year_Text_List'] = df_total.apply(lambda x: list([x['Year_string'],\n",
    "                                    x['reviewText']]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "      <th>words</th>\n",
       "      <th>text_with_year</th>\n",
       "      <th>Year_string</th>\n",
       "      <th>Year_Text_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>997698</th>\n",
       "      <td>1375401600</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-02</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>24</td>\n",
       "      <td>2013 I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>2013</td>\n",
       "      <td>[2013, I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94687</th>\n",
       "      <td>1373587200</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>27</td>\n",
       "      <td>2013 This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>2013</td>\n",
       "      <td>[2013, This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297302</th>\n",
       "      <td>1368921600</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-19</td>\n",
       "      <td>I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>512</td>\n",
       "      <td>2013 I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.</td>\n",
       "      <td>2013</td>\n",
       "      <td>[2013, I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unixReviewTime  \\\n",
       "997698      1375401600   \n",
       "94687       1373587200   \n",
       "297302      1368921600   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  reviewText  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        overall  sentiment datetimeReviewTime  \\\n",
       "997698      3.0          0         2013-08-02   \n",
       "94687       5.0          1         2013-07-12   \n",
       "297302      4.0          1         2013-05-19   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Original_text  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "        words  \\\n",
       "997698     24   \n",
       "94687      27   \n",
       "297302    512   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text_with_year  \\\n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2013 I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  2013 This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "297302  2013 I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.   \n",
       "\n",
       "       Year_string  \\\n",
       "997698        2013   \n",
       "94687         2013   \n",
       "297302        2013   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Year_Text_List  \n",
       "997698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [2013, I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.]  \n",
       "94687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [2013, This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!]  \n",
       "297302  [2013, I absolutely loovve my kindle fire HD. I waited over a week to do this review because I didn't want it to be compromised due to my overall excitement. I was tied between a kindle fire HD and an iPad mini. After doing some research and making my pro/con list, kindle won due to its affordable price, I even got the 32GB. I am a huge fan of amazon and already own a kindle keyboard. Not really a fan of apple, the only apple product I own is an iPod touch that I got over 4 years ago.\\n\\nThe display of the kindle fire HD is AH-MAZING. I was blown away at the crisp display. It makes everything look good, even the font! I find myself watching movies/shows on it rather than my 46 inch screen TV because I am just mesmerized by the display. There are plenty of apps. Since I don't own many apple products I don't have anything else to compare the apps too, but I am satisfied with what is given. Reading on the kindle fire is amazing as well, I am an avid reader so this is a huge plus. Upgrading from the kindle keyboard to the fire's back light did take some adjusting but after a day or two of reading I was comfortable with it. It has pretty cool features too such as seeing how much of the book is left displayed on the bottom of the screen and it also states up many more minutes till the next chapter! As well as some other new kinks. The speakers are nice and clear, although I wish they were a bit louder, I understand it's not suppose to be a stereo but slighter volume would of been a plus. The web experience is very nice, but it does lack flash. I heard there is a way around that, but it doesn't bother me (yet). Since I got the kindle (about 8 days ago) the web page crashed only once and that's because I had a lot of things going on at once (7+ pages open + pandora)...\\n\\nThey don't include a charger in the package which I think was a HUGE mistake on amazons part. The Amazon Kindle Power Fast adapter is a NECESSITY unless you want to wait 16+ hours to get your kindle charged. I used my other kindle adapter and only had it changed at 50% after 9 hours and drained to 15% after two hours of use. The very next day I went and got this charger, now it takes 6-7 hours for a full charge and the charge can last an entire day to two days depending on what you are using the kindle for.\\n\\nSince I got the kindle fire HD I also signed up for amazon prime to get the full experience. There are plenty of free shows and movies to watch! I just wish the kindle lending wasn't restricted to one book per month :(\\n\\nOverall I give this product a 4.5 rating and recommend it to everyone.]  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon’s site listed RB3Pro Keyboard... \n",
      "roBERTa-base: ['Amazon', 'âĢ', 'Ļ', 's', 'Ġsite', 'Ġlisted', 'ĠRB', '3', 'Pro', 'ĠKeyboard', '...', 'Ġ']\n",
      "Bert-base-cased: ['Amazon', '’', 's', 'site', 'listed', 'RB', '##3', '##P', '##ro', 'Key', '##board', '.', '.', '.']\n",
      "New NVIDIA RTX30 has 32GB of V RAM\n",
      "roBERTa-base: ['New', 'ĠNVIDIA', 'ĠRTX', '30', 'Ġhas', 'Ġ32', 'GB', 'Ġof', 'ĠV', 'ĠRAM']\n",
      "Bert-base-cased: ['New', 'N', '##VI', '##DI', '##A', 'R', '##T', '##X', '##30', 'has', '32', '##GB', 'of', 'V', 'RAM']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Amazon’s site listed RB3Pro Keyboard... \"\n",
    "print(sequence)\n",
    "print(\"roBERTa-base:\", tokenizer1.tokenize(sequence))\n",
    "print(\"Bert-base-cased:\",tokenizer2.tokenize(sequence))\n",
    "\n",
    "sequence = \"New NVIDIA RTX30 has 32GB of V RAM\"\n",
    "print(sequence)\n",
    "print(\"roBERTa-base:\", tokenizer1.tokenize(sequence))\n",
    "print(\"Bert-base-cased:\",tokenizer2.tokenize(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing the dataset as train & test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_Data_records = 10000\n",
    "test_record_count= int(Max_Data_records*0.2)\n",
    "train_record_count= int(Max_Data_records*0.8)\n",
    "Label_stratify = df_total['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bert Inputs\n",
    "max_tokens_input = 80\n",
    "\n",
    "learning_rate_input = 0.00005\n",
    "\n",
    "batch_size_input = 16\n",
    "\n",
    "epochs_input= 5\n",
    "\n",
    "steps_input = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (59978, 10)\n",
      "TRAIN Dataset: (47982, 6)\n",
      "TEST Dataset: (11996, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data_original, test_data_original = train_test_split(df_total,\n",
    "                                                           random_state=12345,\n",
    "                                                           stratify=Label_stratify, \n",
    "                                                           train_size = train_record_count, \n",
    "                                                           test_size = test_record_count\n",
    "                                                          )\n",
    "\n",
    "# information about dataset\n",
    "print(\"FULL Dataset: {}\".format(df_total.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirming the Training & Testing Data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "      <th>words</th>\n",
       "      <th>text_with_year</th>\n",
       "      <th>Year_string</th>\n",
       "      <th>Year_Text_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>628085</th>\n",
       "      <td>1436659200</td>\n",
       "      <td>great item</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-07-12</td>\n",
       "      <td>great item</td>\n",
       "      <td>2</td>\n",
       "      <td>2015 great item</td>\n",
       "      <td>2015</td>\n",
       "      <td>[2015, great item]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167501</th>\n",
       "      <td>1442620800</td>\n",
       "      <td>Great product and nice to keep sanitized.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-09-19</td>\n",
       "      <td>Great product and nice to keep sanitized.</td>\n",
       "      <td>7</td>\n",
       "      <td>2015 Great product and nice to keep sanitized.</td>\n",
       "      <td>2015</td>\n",
       "      <td>[2015, Great product and nice to keep sanitized.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210962</th>\n",
       "      <td>1526342400</td>\n",
       "      <td>Bought this because of the small size of the barrel thinking that it would make smaller curls. The first one arrived broken. Fast response sent out a new one and told not to sent the other back.\\n\\nThis product is made of plastic (the part that you use to curl) I find it unmanageable. Will throw it out.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-15</td>\n",
       "      <td>Bought this because of the small size of the barrel thinking that it would make smaller curls. The first one arrived broken. Fast response sent out a new one and told not to sent the other back.\\n\\nThis product is made of plastic (the part that you use to curl) I find it unmanageable. Will throw it out.</td>\n",
       "      <td>58</td>\n",
       "      <td>2018 Bought this because of the small size of the barrel thinking that it would make smaller curls. The first one arrived broken. Fast response sent out a new one and told not to sent the other back.\\n\\nThis product is made of plastic (the part that you use to curl) I find it unmanageable. Will throw it out.</td>\n",
       "      <td>2018</td>\n",
       "      <td>[2018, Bought this because of the small size of the barrel thinking that it would make smaller curls. The first one arrived broken. Fast response sent out a new one and told not to sent the other back.\\n\\nThis product is made of plastic (the part that you use to curl) I find it unmanageable. Will throw it out.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904879</th>\n",
       "      <td>1498348800</td>\n",
       "      <td>Would not recommend, it had a weak taste.  Would not reorder.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-25</td>\n",
       "      <td>Would not recommend, it had a weak taste.  Would not reorder.</td>\n",
       "      <td>11</td>\n",
       "      <td>2017 Would not recommend, it had a weak taste.  Would not reorder.</td>\n",
       "      <td>2017</td>\n",
       "      <td>[2017, Would not recommend, it had a weak taste.  Would not reorder.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129266</th>\n",
       "      <td>1431820800</td>\n",
       "      <td>Like the product was reccomended by my dentist. Flossing is still required. Another tool to use for fighting tooth decay.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-05-17</td>\n",
       "      <td>Like the product was reccomended by my dentist. Flossing is still required. Another tool to use for fighting tooth decay.</td>\n",
       "      <td>20</td>\n",
       "      <td>2015 Like the product was reccomended by my dentist. Flossing is still required. Another tool to use for fighting tooth decay.</td>\n",
       "      <td>2015</td>\n",
       "      <td>[2015, Like the product was reccomended by my dentist. Flossing is still required. Another tool to use for fighting tooth decay.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unixReviewTime  \\\n",
       "628085      1436659200   \n",
       "167501      1442620800   \n",
       "210962      1526342400   \n",
       "904879      1498348800   \n",
       "129266      1431820800   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                              reviewText  \\\n",
       "628085                                                                                                                                                                                                                                                                                                        great item   \n",
       "167501                                                                                                                                                                                                                                                                         Great product and nice to keep sanitized.   \n",
       "210962  Bought this because of the small size of the barrel thinking that it would make smaller curls. The first one arrived broken. Fast response sent out a new one and told not to sent the other back.\\n\\nThis product is made of plastic (the part that you use to curl) I find it unmanageable. Will throw it out.   \n",
       "904879                                                                                                                                                                                                                                                     Would not recommend, it had a weak taste.  Would not reorder.   \n",
       "129266                                                                                                                                                                                         Like the product was reccomended by my dentist. Flossing is still required. Another tool to use for fighting tooth decay.   \n",
       "\n",
       "        overall  sentiment datetimeReviewTime  \\\n",
       "628085      5.0          1         2015-07-12   \n",
       "167501      5.0          1         2015-09-19   \n",
       "210962      2.0          0         2018-05-15   \n",
       "904879      1.0          0         2017-06-25   \n",
       "129266      4.0          1         2015-05-17   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                           Original_text  \\\n",
       "628085                                                                                                                                                                                                                                                                                                        great item   \n",
       "167501                                                                                                                                                                                                                                                                         Great product and nice to keep sanitized.   \n",
       "210962  Bought this because of the small size of the barrel thinking that it would make smaller curls. The first one arrived broken. Fast response sent out a new one and told not to sent the other back.\\n\\nThis product is made of plastic (the part that you use to curl) I find it unmanageable. Will throw it out.   \n",
       "904879                                                                                                                                                                                                                                                     Would not recommend, it had a weak taste.  Would not reorder.   \n",
       "129266                                                                                                                                                                                         Like the product was reccomended by my dentist. Flossing is still required. Another tool to use for fighting tooth decay.   \n",
       "\n",
       "        words  \\\n",
       "628085      2   \n",
       "167501      7   \n",
       "210962     58   \n",
       "904879     11   \n",
       "129266     20   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                               text_with_year  \\\n",
       "628085                                                                                                                                                                                                                                                                                                        2015 great item   \n",
       "167501                                                                                                                                                                                                                                                                         2015 Great product and nice to keep sanitized.   \n",
       "210962  2018 Bought this because of the small size of the barrel thinking that it would make smaller curls. The first one arrived broken. Fast response sent out a new one and told not to sent the other back.\\n\\nThis product is made of plastic (the part that you use to curl) I find it unmanageable. Will throw it out.   \n",
       "904879                                                                                                                                                                                                                                                     2017 Would not recommend, it had a weak taste.  Would not reorder.   \n",
       "129266                                                                                                                                                                                         2015 Like the product was reccomended by my dentist. Flossing is still required. Another tool to use for fighting tooth decay.   \n",
       "\n",
       "       Year_string  \\\n",
       "628085        2015   \n",
       "167501        2015   \n",
       "210962        2018   \n",
       "904879        2017   \n",
       "129266        2015   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                  Year_Text_List  \n",
       "628085                                                                                                                                                                                                                                                                                                        [2015, great item]  \n",
       "167501                                                                                                                                                                                                                                                                         [2015, Great product and nice to keep sanitized.]  \n",
       "210962  [2018, Bought this because of the small size of the barrel thinking that it would make smaller curls. The first one arrived broken. Fast response sent out a new one and told not to sent the other back.\\n\\nThis product is made of plastic (the part that you use to curl) I find it unmanageable. Will throw it out.]  \n",
       "904879                                                                                                                                                                                                                                                     [2017, Would not recommend, it had a weak taste.  Would not reorder.]  \n",
       "129266                                                                                                                                                                                         [2015, Like the product was reccomended by my dentist. Flossing is still required. Another tool to use for fighting tooth decay.]  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name ='Training_Data.xlsx'\n",
    "#train_data_original.to_excel(file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name ='Test_Data.xlsx'\n",
    "#test_data_original.to_excel(file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Year_string</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>645</td>\n",
       "      <td>626</td>\n",
       "      <td>647</td>\n",
       "      <td>695</td>\n",
       "      <td>720</td>\n",
       "      <td>704</td>\n",
       "      <td>4037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>715</td>\n",
       "      <td>668</td>\n",
       "      <td>692</td>\n",
       "      <td>619</td>\n",
       "      <td>614</td>\n",
       "      <td>655</td>\n",
       "      <td>3963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>1360</td>\n",
       "      <td>1294</td>\n",
       "      <td>1339</td>\n",
       "      <td>1314</td>\n",
       "      <td>1334</td>\n",
       "      <td>1359</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Year_string  2013  2014  2015  2016  2017  2018  Total\n",
       "sentiment                                             \n",
       "0             645   626   647   695   720   704   4037\n",
       "1             715   668   692   619   614   655   3963\n",
       "Total        1360  1294  1339  1314  1334  1359   8000"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = train_data_original\n",
    "pd.crosstab(df_temp['sentiment'], df_temp['Year_string'], margins=True, margins_name = 'Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Year_string</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163</td>\n",
       "      <td>167</td>\n",
       "      <td>159</td>\n",
       "      <td>168</td>\n",
       "      <td>189</td>\n",
       "      <td>163</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174</td>\n",
       "      <td>167</td>\n",
       "      <td>183</td>\n",
       "      <td>158</td>\n",
       "      <td>143</td>\n",
       "      <td>166</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>337</td>\n",
       "      <td>334</td>\n",
       "      <td>342</td>\n",
       "      <td>326</td>\n",
       "      <td>332</td>\n",
       "      <td>329</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Year_string  2013  2014  2015  2016  2017  2018  Total\n",
       "sentiment                                             \n",
       "0             163   167   159   168   189   163   1009\n",
       "1             174   167   183   158   143   166    991\n",
       "Total         337   334   342   326   332   329   2000"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = test_data_original\n",
    "pd.crosstab(df_temp['sentiment'], df_temp['Year_string'], margins=True, margins_name = 'Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Base Clean Text (Model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = copy.deepcopy(train_data_original)\n",
    "test_data = copy.deepcopy(test_data_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_data['Original_text']\n",
    "train_sentiment = train_data['sentiment']\n",
    "train_text = tf.convert_to_tensor(train_text)\n",
    "train_sentiment  = tf.convert_to_tensor(train_sentiment)\n",
    "\n",
    "test_text = test_data['Original_text']\n",
    "test_sentiment = test_data['sentiment']\n",
    "test_text = tf.convert_to_tensor(test_text)\n",
    "test_sentiment = tf.convert_to_tensor(test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from tools import *\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/logs/20211202-000000'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "#rm -rf ./logs/\n",
    "import datetime\n",
    "today = datetime.date.today()\n",
    "today\n",
    "logdir=\"/logs/\" + today.strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras import backend as K\n",
    "tokenizer_bert_base = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer_bert_base([str(x.numpy())[2:] for x in train_text], \n",
    "              max_length=max_tokens_input,\n",
    "              truncation=True, padding='max_length', return_tensors='tf')\n",
    "y_train = train_sentiment\n",
    "\n",
    "x_test = tokenizer_bert_base([str(x.numpy())[2:] for x in test_text], \n",
    "              max_length=max_tokens_input,\n",
    "              truncation=True, padding='max_length', return_tensors='tf')\n",
    "y_test = test_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_model(hidden_size = 200, \n",
    "                                train_layers = -1, \n",
    "                                optimizer=tf.keras.optimizers.Adam(learning_rate_input)):\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_tokens_input,), dtype=tf.int32, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_tokens_input,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_tokens_input,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "    #restrict training to the train_layers outer transformer layers\n",
    "    if not train_layers == -1:\n",
    "\n",
    "            retrain_layers = []\n",
    "\n",
    "            for retrain_layer_number in range(train_layers):\n",
    "\n",
    "                layer_code = '_' + str(11 - retrain_layer_number)\n",
    "                retrain_layers.append(layer_code)\n",
    "\n",
    "            for w in bert_model.weights:\n",
    "                if not any([x in w.name for x in retrain_layers]):\n",
    "                    w._trainable = False\n",
    "\n",
    "\n",
    "    bert_out = bert_model(bert_inputs)\n",
    "\n",
    "\n",
    "    classification_token = tf.keras.layers.Lambda(lambda x: x[:,0,:], name='get_first_vector')(bert_out[0])\n",
    "\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, name='hidden_layer')(classification_token)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
    "\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], \n",
    "                                          outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=optimizer,\n",
    "                            loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                            metrics='accuracy')\n",
    "\n",
    "\n",
    "    return classification_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method TFBertModel.call of <transformers.models.bert.modeling_tf_bert.TFBertModel object at 0x000001C99342D250>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x000001C9934231F0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertEmbeddings object at 0x000001C993423790>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertEncoder.call of <transformers.models.bert.modeling_tf_bert.TFBertEncoder object at 0x000001C99317F9A0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertLayer object at 0x000001C99317FF10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertAttention object at 0x000001C99317F130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertSelfAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfAttention object at 0x000001C99317F310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertPooler.call of <transformers.models.bert.modeling_tf_bert.TFBertPooler object at 0x000001C99317F5B0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "classification_model = create_classification_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 1895s 4s/step - loss: 0.3520 - accuracy: 0.8474 - val_loss: 0.2638 - val_accuracy: 0.8950\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 1880s 4s/step - loss: 0.1770 - accuracy: 0.9308 - val_loss: 0.2789 - val_accuracy: 0.8930\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 1879s 4s/step - loss: 0.0948 - accuracy: 0.9649 - val_loss: 0.4059 - val_accuracy: 0.8795\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 1878s 4s/step - loss: 0.0738 - accuracy: 0.9737 - val_loss: 0.4511 - val_accuracy: 0.8795\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 1874s 4s/step - loss: 0.0428 - accuracy: 0.9846 - val_loss: 0.4843 - val_accuracy: 0.8870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a86fce5400>"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_test.input_ids, x_test.token_type_ids, x_test.attention_mask],\n",
    "                         y_test),\n",
    "                        epochs=epochs_input,\n",
    "                        batch_size=batch_size_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9984252e-01],\n",
       "       [9.9977911e-01],\n",
       "       [1.0462936e-05],\n",
       "       [5.3729300e-06],\n",
       "       [9.9995875e-01],\n",
       "       [2.1945012e-05],\n",
       "       [5.9219701e-06],\n",
       "       [9.9995315e-01],\n",
       "       [1.2642741e-03],\n",
       "       [8.1454900e-06],\n",
       "       [6.1906958e-06],\n",
       "       [9.9973512e-01],\n",
       "       [9.9540424e-01],\n",
       "       [2.9337468e-06],\n",
       "       [9.9993563e-01],\n",
       "       [2.3262946e-05],\n",
       "       [4.8184395e-04],\n",
       "       [9.9969006e-01],\n",
       "       [6.4325333e-04],\n",
       "       [9.9863422e-01],\n",
       "       [9.9367690e-01],\n",
       "       [2.2017901e-05],\n",
       "       [9.9841273e-01],\n",
       "       [9.9993145e-01],\n",
       "       [1.4969707e-04],\n",
       "       [2.4817276e-05],\n",
       "       [5.1477668e-06],\n",
       "       [9.9987370e-01],\n",
       "       [2.0837542e-05],\n",
       "       [8.3347986e-06],\n",
       "       [9.9971616e-01],\n",
       "       [9.9996448e-01]], dtype=float32)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask], \n",
    "                             batch_size=batch_size_input, \n",
    "                             steps=steps_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Model FrameWork Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1837s 4s/step - loss: 0.1786 - accuracy: 0.9311 - val_loss: 0.3569 - val_accuracy: 0.8775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ca3288e670>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_test.input_ids, x_test.token_type_ids, x_test.attention_mask],\n",
    "                         y_test),\n",
    "                        epochs=1,\n",
    "                        batch_size=batch_size_input,callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logdir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-04c5efbb5782>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tensorboard'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tensorboard'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'--logdir logs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logdir' is not defined"
     ]
    }
   ],
   "source": [
    "print(logdir)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.save('saved_model/bert_base_Original_Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_model_0 = classification_model.predict([x_test.input_ids, x_test.token_type_ids, x_test.attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions_model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_values = []\n",
    "Actual_probabilities =[]\n",
    "for p in predictions:\n",
    "    value = p[0]\n",
    "    Actual_probabilities.append(value)\n",
    "    if value >= 0.5:\n",
    "        predictions_values.append(1)\n",
    "    else:\n",
    "        predictions_values.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_values = []\n",
    "Actual_probabilities =[]\n",
    "for p in predictions:\n",
    "    value = p[0]\n",
    "    Actual_probabilities.append(value)\n",
    "    if value >= 0.5:\n",
    "        predictions_values.append(1)\n",
    "    else:\n",
    "        predictions_values.append(0)\n",
    "        \n",
    "test_sentiment = test_data['sentiment']\n",
    "sentiment_list = test_sentiment.to_list()\n",
    "data_year = test_data['Year_string'].to_list()\n",
    "data_text = test_data['Original_text'].to_list()\n",
    "rating = test_data['overall'].to_list()\n",
    "\n",
    "\n",
    "false_positive = {'Year': [],'Text': [], 'Overall Rating': [],'Sentiment':[],'Prediction_value': [], 'Predicted_prob': []}\n",
    "false_negative = {'Year': [],'Text': [], 'Overall Rating': [],'Sentiment':[],'Prediction_value': [], 'Predicted_prob': []}\n",
    "Correct_values = {'Year': [],'Text': [], 'Overall Rating': [],'Sentiment':[],'Prediction_value': [], 'Predicted_prob': []}\n",
    "false_positive = pd.DataFrame(false_positive)\n",
    "false_negative = pd.DataFrame(false_negative)\n",
    "Correct_values = pd.DataFrame(Correct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_year_concat = {'2013': 0, '2014': 0, '2015': 0, '2016': 0, '2017': 0, '2018': 0}\n",
    "\n",
    "for i in range(len(sentiment_list)):\n",
    "    sent = sentiment_list[i]\n",
    "    year = data_year[i].split()[0]\n",
    "    text = data_text[i]\n",
    "    text = str(text)\n",
    "    if len(text)>500:\n",
    "        text =text[0:500]\n",
    "    else :\n",
    "        text= text \n",
    "    if predictions_values[i] == sentiment_list[i]:\n",
    "        new_row = {'Year': data_year[i],'Text': text, 'Overall Rating': rating[i],'Sentiment':sentiment_list[i],'Prediction_value': predictions_values[i],'Predicted_prob': Actual_probabilities[i]}\n",
    "        Correct_values = Correct_values.append(new_row, ignore_index=True)\n",
    "    else:\n",
    "        if predictions_values[i] == 1:\n",
    "            new_row = {'Year': data_year[i],'Text': text, 'Overall Rating': rating[i],'Sentiment': sentiment_list[i],'Prediction_value': predictions_values[i],'Predicted_prob': Actual_probabilities[i]}\n",
    "            false_positive = false_positive.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            new_row = {'Year': data_year[i],'Text': text, 'Overall Rating': rating[i],'Sentiment':sentiment_list[i],'Prediction_value': predictions_values[i],'Predicted_prob': Actual_probabilities[i]}\n",
    "            false_negative = false_negative.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples = 10\n",
    "import random\n",
    "false_positive_samples = false_positive\n",
    "\n",
    "print(\"False Positive \\n\")\n",
    "print(\"*\" * 50)\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "false_positive_samples = shuffle(false_positive_samples)\n",
    "\n",
    "false_positive_samples[0:Examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative_samples = false_negative\n",
    "\n",
    "print(\"False Negative \\n\",)\n",
    "\n",
    "print(\"*\" * 50)\n",
    "print('\\n')\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "false_negative_samples = shuffle(false_negative_samples)\n",
    "\n",
    "false_negative_samples[0:Examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive.insert(0, 'Name', 'false_positive')\n",
    "\n",
    "false_negative.insert(0, 'Name', 'false_negative')\n",
    "\n",
    "Correct_values.insert(0, 'Name', 'Correct_values')\n",
    "\n",
    "frames_1 = [false_positive, false_negative,Correct_values]\n",
    "\n",
    "frames_2 =[false_positive, false_negative]\n",
    "\n",
    "result_incorrect = pd.concat(frames_2)\n",
    "\n",
    "result_all = pd.concat(frames_1)\n",
    "result_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_all = result_all\n",
    "\n",
    "df_result_incorrect = result_incorrect\n",
    "pd.crosstab(df_result_incorrect['Overall Rating'], df_result_incorrect['Name'], margins=True, margins_name = 'Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_result_all['Overall Rating'], df_result_all['Name']).plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT YEar + Text Concat (Model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = copy.deepcopy(train_data_original)\n",
    "test_data = copy.deepcopy(test_data_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "      <th>words</th>\n",
       "      <th>text_with_year</th>\n",
       "      <th>Year_string</th>\n",
       "      <th>Year_Text_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>628085</th>\n",
       "      <td>1436659200</td>\n",
       "      <td>great item</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-07-12</td>\n",
       "      <td>great item</td>\n",
       "      <td>2</td>\n",
       "      <td>2015 great item</td>\n",
       "      <td>2015</td>\n",
       "      <td>[2015, great item]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167501</th>\n",
       "      <td>1442620800</td>\n",
       "      <td>Great product and nice to keep sanitized.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-09-19</td>\n",
       "      <td>Great product and nice to keep sanitized.</td>\n",
       "      <td>7</td>\n",
       "      <td>2015 Great product and nice to keep sanitized.</td>\n",
       "      <td>2015</td>\n",
       "      <td>[2015, Great product and nice to keep sanitized.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unixReviewTime                                 reviewText  overall  \\\n",
       "628085      1436659200                                 great item      5.0   \n",
       "167501      1442620800  Great product and nice to keep sanitized.      5.0   \n",
       "\n",
       "        sentiment datetimeReviewTime  \\\n",
       "628085          1         2015-07-12   \n",
       "167501          1         2015-09-19   \n",
       "\n",
       "                                    Original_text  words  \\\n",
       "628085                                 great item      2   \n",
       "167501  Great product and nice to keep sanitized.      7   \n",
       "\n",
       "                                        text_with_year Year_string  \\\n",
       "628085                                 2015 great item        2015   \n",
       "167501  2015 Great product and nice to keep sanitized.        2015   \n",
       "\n",
       "                                           Year_Text_List  \n",
       "628085                                 [2015, great item]  \n",
       "167501  [2015, Great product and nice to keep sanitized.]  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_data['text_with_year']\n",
    "train_sentiment = train_data['sentiment']\n",
    "train_text = tf.convert_to_tensor(train_text)\n",
    "train_sentiment  = tf.convert_to_tensor(train_sentiment)\n",
    "\n",
    "test_text = test_data['text_with_year']\n",
    "test_sentiment = test_data['sentiment']\n",
    "test_text = tf.convert_to_tensor(test_text)\n",
    "test_sentiment = tf.convert_to_tensor(test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'2015 great item' b'2015 Great product and nice to keep sanitized.'\n",
      " b'2018 Bought this because of the small size of the barrel thinking that it would make smaller curls. The first one arrived broken. Fast response sent out a new one and told not to sent the other back.\\n\\nThis product is made of plastic (the part that you use to curl) I find it unmanageable. Will throw it out.'\n",
      " ... b'2017 great fit'\n",
      " b'2014 This is a liquid, so when sugar is the first ingredient, ahead of water, it is terrible. You might ass well inject sugar into your bloodstream.'\n",
      " b\"2018 First after the first month unless you pay a monthly fee there is no way to view video recording which would make it useless as security record. You can't store any of the images on your computer or phone, you have to pay them to do it. Second the app always takes too long to open and see who is at the door to be really useful. This is true for all the Ring camaras. So get ready for $100 yearly fee\"], shape=(8000,), dtype=string)\n",
      "tf.Tensor([1 1 0 ... 1 0 0], shape=(8000,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[b'2015 great item' b'2015 Great product and nice to keep sanitized.'\n",
      " b'2018 Bought this because of the small size of the barrel thinking that it would make smaller curls. The first one arrived broken. Fast response sent out a new one and told not to sent the other back.\\n\\nThis product is made of plastic (the part that you use to curl) I find it unmanageable. Will throw it out.'\n",
      " ... b'2017 great fit'\n",
      " b'2014 This is a liquid, so when sugar is the first ingredient, ahead of water, it is terrible. You might ass well inject sugar into your bloodstream.'\n",
      " b\"2018 First after the first month unless you pay a monthly fee there is no way to view video recording which would make it useless as security record. You can't store any of the images on your computer or phone, you have to pay them to do it. Second the app always takes too long to open and see who is at the door to be really useful. This is true for all the Ring camaras. So get ready for $100 yearly fee\"], shape=(8000,), dtype=string)\n",
      "tf.Tensor([1 1 0 ... 1 0 0], shape=(8000,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(train_text)\n",
    "print(train_sentiment)\n",
    "print(train_text)\n",
    "print(train_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer_bert_base([str(x.numpy())[2:] for x in train_text], \n",
    "              max_length=max_tokens_input,\n",
    "              truncation=True, padding='max_length', return_tensors='tf')\n",
    "y_train = train_sentiment\n",
    "\n",
    "x_test = tokenizer_bert_base([str(x.numpy())[2:] for x in test_text], \n",
    "              max_length=max_tokens_input,\n",
    "              truncation=True, padding='max_length', return_tensors='tf')\n",
    "y_test = test_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_model(hidden_size = 200, \n",
    "                                train_layers = -1, \n",
    "                                optimizer=tf.keras.optimizers.Adam(learning_rate_input)):\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_tokens_input,), dtype=tf.int32, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_tokens_input,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_tokens_input,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "    #restrict training to the train_layers outer transformer layers\n",
    "    if not train_layers == -1:\n",
    "\n",
    "            retrain_layers = []\n",
    "\n",
    "            for retrain_layer_number in range(train_layers):\n",
    "\n",
    "                layer_code = '_' + str(11 - retrain_layer_number)\n",
    "                retrain_layers.append(layer_code)\n",
    "\n",
    "            for w in bert_model.weights:\n",
    "                if not any([x in w.name for x in retrain_layers]):\n",
    "                    w._trainable = False\n",
    "\n",
    "\n",
    "    bert_out = bert_model(bert_inputs)\n",
    "\n",
    "\n",
    "    classification_token = tf.keras.layers.Lambda(lambda x: x[:,0,:], name='get_first_vector')(bert_out[0])\n",
    "\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, name='hidden_layer')(classification_token)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
    "\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], \n",
    "                                          outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=optimizer,\n",
    "                            loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                            metrics='accuracy')\n",
    "\n",
    "\n",
    "    return classification_model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method TFBertModel.call of <transformers.models.bert.modeling_tf_bert.TFBertModel object at 0x0000022F13D2D160>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x0000022F13D2D580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertEmbeddings.call of <transformers.models.bert.modeling_tf_bert.TFBertEmbeddings object at 0x0000022F13D3E610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertEncoder.call of <transformers.models.bert.modeling_tf_bert.TFBertEncoder object at 0x0000022F11BE3CD0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertLayer.call of <transformers.models.bert.modeling_tf_bert.TFBertLayer object at 0x0000022F11BC2730>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertAttention object at 0x0000022F11BC2160>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertSelfAttention.call of <transformers.models.bert.modeling_tf_bert.TFBertSelfAttention object at 0x0000022F11BC2580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFBertPooler.call of <transformers.models.bert.modeling_tf_bert.TFBertPooler object at 0x0000022F11BE3310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "classification_model = create_classification_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "attention_mask_layer (InputLaye [(None, 80)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_layer (InputLayer)    [(None, 80)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids_layer (InputLaye [(None, 80)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 108310272   attention_mask_layer[0][0]       \n",
      "                                                                 input_ids_layer[0][0]            \n",
      "                                                                 token_type_ids_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "get_first_vector (Lambda)       (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer (Dense)            (None, 200)          153800      get_first_vector[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "classification_layer (Dense)    (None, 1)            201         hidden_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 108,464,273\n",
      "Trainable params: 108,464,273\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 1828s 4s/step - loss: 0.3577 - accuracy: 0.8446 - val_loss: 0.2532 - val_accuracy: 0.8990\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 1822s 4s/step - loss: 0.1749 - accuracy: 0.9324 - val_loss: 0.3259 - val_accuracy: 0.8835\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 1856s 4s/step - loss: 0.1047 - accuracy: 0.9614 - val_loss: 0.3403 - val_accuracy: 0.8855\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 1867s 4s/step - loss: 0.0735 - accuracy: 0.9732 - val_loss: 0.4098 - val_accuracy: 0.8940\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 1846s 4s/step - loss: 0.0573 - accuracy: 0.9806 - val_loss: 0.4203 - val_accuracy: 0.8805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22f234e8190>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_test.input_ids, x_test.token_type_ids, x_test.attention_mask],\n",
    "                         y_test),\n",
    "                        epochs=epochs_input,\n",
    "                        batch_size=batch_size_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99995708e-01],\n",
       "       [9.99989510e-01],\n",
       "       [1.18999385e-04],\n",
       "       [5.37304040e-05],\n",
       "       [9.99408185e-01],\n",
       "       [1.82569027e-04],\n",
       "       [1.60038471e-04],\n",
       "       [9.99977469e-01],\n",
       "       [8.20904970e-04],\n",
       "       [1.07452273e-03],\n",
       "       [3.21894884e-04],\n",
       "       [9.99604940e-01],\n",
       "       [9.87433553e-01],\n",
       "       [7.14596827e-05],\n",
       "       [9.99943018e-01],\n",
       "       [1.30176544e-04],\n",
       "       [3.88470653e-05],\n",
       "       [9.99859929e-01],\n",
       "       [8.85495538e-05],\n",
       "       [9.99139071e-01],\n",
       "       [9.99967754e-01],\n",
       "       [1.47312880e-04],\n",
       "       [9.99213696e-01],\n",
       "       [9.84140813e-01],\n",
       "       [3.43861580e-02],\n",
       "       [8.29580426e-03],\n",
       "       [9.34343407e-05],\n",
       "       [9.99413788e-01],\n",
       "       [2.19523907e-04],\n",
       "       [6.95228577e-03],\n",
       "       [9.99837518e-01],\n",
       "       [9.99992073e-01]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask], \n",
    "                             batch_size=batch_size_input, \n",
    "                             steps=steps_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.save('saved_model/Bert_YearTextConcat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_model_2 = classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_model_2\n",
    "\n",
    "predictions = predictions_model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalution & Errror Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_values = []\n",
    "Actual_probabilities =[]\n",
    "for p in predictions:\n",
    "    value = p[0]\n",
    "    Actual_probabilities.append(value)\n",
    "    if value >= 0.5:\n",
    "        predictions_values.append(1)\n",
    "    else:\n",
    "        predictions_values.append(0)\n",
    "        \n",
    "predictions_values = []\n",
    "Actual_probabilities =[]\n",
    "for p in predictions:\n",
    "    value = p[0]\n",
    "    Actual_probabilities.append(value)\n",
    "    if value >= 0.5:\n",
    "        predictions_values.append(1)\n",
    "    else:\n",
    "        predictions_values.append(0)\n",
    "        \n",
    "test_sentiment = test_data['sentiment']\n",
    "sentiment_list = test_sentiment.to_list()\n",
    "data_year = test_data['Year_string'].to_list()\n",
    "data_text = test_data['Original_text'].to_list()\n",
    "rating = test_data['overall'].to_list()\n",
    "\n",
    "\n",
    "false_positive = {'Year': [],'Text': [], 'Overall Rating': [],'Sentiment':[],'Prediction_value': [], 'Predicted_prob': []}\n",
    "false_negative = {'Year': [],'Text': [], 'Overall Rating': [],'Sentiment':[],'Prediction_value': [], 'Predicted_prob': []}\n",
    "Correct_values = {'Year': [],'Text': [], 'Overall Rating': [],'Sentiment':[],'Prediction_value': [], 'Predicted_prob': []}\n",
    "false_positive = pd.DataFrame(false_positive)\n",
    "false_negative = pd.DataFrame(false_negative)\n",
    "Correct_values = pd.DataFrame(Correct_values)\n",
    "\n",
    "for i in range(len(sentiment_list)):\n",
    "    sent = sentiment_list[i]\n",
    "    year = data_year[i].split()[0]\n",
    "    text = data_text[i]\n",
    "    text = str(text)\n",
    "    if len(text)>500:\n",
    "        text =text[0:500]\n",
    "    else :\n",
    "        text= text \n",
    "    if predictions_values[i] == sentiment_list[i]:\n",
    "        new_row = {'Year': data_year[i],'Text': text, 'Overall Rating': rating[i],'Sentiment':sentiment_list[i],'Prediction_value': predictions_values[i],'Predicted_prob': Actual_probabilities[i]}\n",
    "        Correct_values = Correct_values.append(new_row, ignore_index=True)\n",
    "    else:\n",
    "        if predictions_values[i] == 1:\n",
    "            new_row = {'Year': data_year[i],'Text': text, 'Overall Rating': rating[i],'Sentiment': sentiment_list[i],'Prediction_value': predictions_values[i],'Predicted_prob': Actual_probabilities[i]}\n",
    "            false_positive = false_positive.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            new_row = {'Year': data_year[i],'Text': text, 'Overall Rating': rating[i],'Sentiment':sentiment_list[i],'Prediction_value': predictions_values[i],'Predicted_prob': Actual_probabilities[i]}\n",
    "            false_negative = false_negative.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive \n",
      "\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Text</th>\n",
       "      <th>Overall Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Prediction_value</th>\n",
       "      <th>Predicted_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>2016</td>\n",
       "      <td>Good thing their cheap because they don't last.  First two pairs of these that I had the cord shorted out within a month.  The third pair has lasted a little longer however this pair the right headphone has gone out.  Not sure if it the cord again or due to sweat but in either case, these headphones are just not very durable.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>2014</td>\n",
       "      <td>This magazine is OK, although as a teaching aide there are much, much better resources out there, particularly online.  I've used it mainly to satisfy the urge to look at/read about beautiful guitars and the luthiers who make them.\\n\\nMy complaint is mostly with their customer service.  I received from them, in the mail, an offer for a renewed subscription at $19.95 for the year, along with a free subscription to their Acoustic Guitar University online lessons.  It gave you the option of mailing i</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>2013</td>\n",
       "      <td>Hi, I would not know how good the guitar was since all I got was a box with remains of a guitar, it was trashed!! Imagine my horror when I opened the package! But I can return it, however impossible to exchange it for another for some unknown reason. Well, lets hope that he next time is the charm. Giving a good grade for the return policy.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>2017</td>\n",
       "      <td>Started falling apart after about 4 months. My daughter did take it to school everyday, but I thought that was the norm nowadays.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2018</td>\n",
       "      <td>Was working great until 1/26/2018. Please fix your skills.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>2018</td>\n",
       "      <td>This device is described as a power supply, but I would describe it more like a noise factory.\\n\\nThe good: It supplies 9 VDC, center negative, so it will power your pedals. It's super light.\\n\\nThe bad: OMG, I haven't tried to take this power supply apart, but I'll have to assume that someone forgot to install the filter caps or that they're blown, because any pedal I've used this to power instantly has an insane amount of noise and hiss (and a bit of a low frequency growl).  The same pedals using</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2017</td>\n",
       "      <td>Great sounds good</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.935688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>2015</td>\n",
       "      <td>I am having a tough time getting the sound I want just right from this head. I am still experimenting with what pedals give me what I am after. That being said, it is after all a Marshall so I know I will get what I am seeking. The option of 40 or 100 watts is very handy as well</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>2016</td>\n",
       "      <td>Pencil doesn't work very well on this paper.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>2013</td>\n",
       "      <td>Using a Perris premium leather strap....\\n\\nThe guitar bolt's cylinderical core doesn't have enough space to fit both strap and a strap block..  Waste of money on first guitar tested.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2018</td>\n",
       "      <td>Didnt like the flavor. Tried 3-4 times. Tasted watery</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>2016</td>\n",
       "      <td>Mushy, I didn't like it</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.831228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>2015</td>\n",
       "      <td>Apparently there is a problem with \"buffering.\"  My Wi-Fi router is very fast. The Fire TV stuff installed easily on my TV.  So...I don't know which device is \"slow\" according to the on-screen message on the TV.  I guess it is the TV itself.  Big disappointment . Don't know how to fix this. Haven't time to research it right now.  If it worked, I'd be enjoying a \"Prime\" movie or other show later this evening. But it doesn't work.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2016</td>\n",
       "      <td>Old and almost stale</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.917801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2016</td>\n",
       "      <td>a waist,  but exactly as promissed</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  \\\n",
       "405  2016   \n",
       "268  2014   \n",
       "486  2013   \n",
       "289  2017   \n",
       "475  2018   \n",
       "192  2018   \n",
       "84   2017   \n",
       "290  2015   \n",
       "261  2016   \n",
       "241  2013   \n",
       "122  2018   \n",
       "455  2016   \n",
       "417  2015   \n",
       "215  2016   \n",
       "34   2016   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Text  \\\n",
       "405                                                                                                                                                                                   Good thing their cheap because they don't last.  First two pairs of these that I had the cord shorted out within a month.  The third pair has lasted a little longer however this pair the right headphone has gone out.  Not sure if it the cord again or due to sweat but in either case, these headphones are just not very durable.   \n",
       "268    This magazine is OK, although as a teaching aide there are much, much better resources out there, particularly online.  I've used it mainly to satisfy the urge to look at/read about beautiful guitars and the luthiers who make them.\\n\\nMy complaint is mostly with their customer service.  I received from them, in the mail, an offer for a renewed subscription at $19.95 for the year, along with a free subscription to their Acoustic Guitar University online lessons.  It gave you the option of mailing i   \n",
       "486                                                                                                                                                                     Hi, I would not know how good the guitar was since all I got was a box with remains of a guitar, it was trashed!! Imagine my horror when I opened the package! But I can return it, however impossible to exchange it for another for some unknown reason. Well, lets hope that he next time is the charm. Giving a good grade for the return policy.   \n",
       "289                                                                                                                                                                                                                                                                                                                                                                                         Started falling apart after about 4 months. My daughter did take it to school everyday, but I thought that was the norm nowadays.   \n",
       "475                                                                                                                                                                                                                                                                                                                                                                                                                                                                Was working great until 1/26/2018. Please fix your skills.   \n",
       "192  This device is described as a power supply, but I would describe it more like a noise factory.\\n\\nThe good: It supplies 9 VDC, center negative, so it will power your pedals. It's super light.\\n\\nThe bad: OMG, I haven't tried to take this power supply apart, but I'll have to assume that someone forgot to install the filter caps or that they're blown, because any pedal I've used this to power instantly has an insane amount of noise and hiss (and a bit of a low frequency growl).  The same pedals using    \n",
       "84                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Great sounds good   \n",
       "290                                                                                                                                                                                                                                   I am having a tough time getting the sound I want just right from this head. I am still experimenting with what pedals give me what I am after. That being said, it is after all a Marshall so I know I will get what I am seeking. The option of 40 or 100 watts is very handy as well   \n",
       "261                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Pencil doesn't work very well on this paper.   \n",
       "241                                                                                                                                                                                                                                                                                                                                   Using a Perris premium leather strap....\\n\\nThe guitar bolt's cylinderical core doesn't have enough space to fit both strap and a strap block..  Waste of money on first guitar tested.   \n",
       "122                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Didnt like the flavor. Tried 3-4 times. Tasted watery   \n",
       "455                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Mushy, I didn't like it   \n",
       "417                                                                          Apparently there is a problem with \"buffering.\"  My Wi-Fi router is very fast. The Fire TV stuff installed easily on my TV.  So...I don't know which device is \"slow\" according to the on-screen message on the TV.  I guess it is the TV itself.  Big disappointment . Don't know how to fix this. Haven't time to research it right now.  If it worked, I'd be enjoying a \"Prime\" movie or other show later this evening. But it doesn't work.   \n",
       "215                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Old and almost stale   \n",
       "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         a waist,  but exactly as promissed   \n",
       "\n",
       "     Overall Rating  Sentiment  Prediction_value  Predicted_prob  \n",
       "405             2.0        0.0               1.0        0.991886  \n",
       "268             1.0        0.0               1.0        0.999454  \n",
       "486             3.0        0.0               1.0        0.999990  \n",
       "289             3.0        0.0               1.0        0.999982  \n",
       "475             1.0        0.0               1.0        0.999995  \n",
       "192             2.0        0.0               1.0        0.999972  \n",
       "84              3.0        0.0               1.0        0.935688  \n",
       "290             3.0        0.0               1.0        0.995115  \n",
       "261             3.0        0.0               1.0        0.999988  \n",
       "241             1.0        0.0               1.0        0.999998  \n",
       "122             1.0        0.0               1.0        0.997669  \n",
       "455             1.0        0.0               1.0        0.831228  \n",
       "417             3.0        0.0               1.0        0.999717  \n",
       "215             3.0        0.0               1.0        0.917801  \n",
       "34              3.0        0.0               1.0        0.993352  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Examples = 15\n",
    "import random\n",
    "false_positive_samples = false_positive\n",
    "\n",
    "print(\"False Positive \\n\")\n",
    "print(\"*\" * 50)\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "false_positive_samples = shuffle(false_positive_samples)\n",
    "\n",
    "false_positive_samples[0:Examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative \n",
      "\n",
      "**************************************************\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Text</th>\n",
       "      <th>Overall Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Prediction_value</th>\n",
       "      <th>Predicted_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>2016</td>\n",
       "      <td>Great color combo, maybe a little on the thicker side but overall happy!</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018</td>\n",
       "      <td>I am very happy with this purchase. It was easy to set up and does all I need it to. The picture is pretty good depending on the conditions, if the light is very bright, its not as clear but you can see everything and at night a little bit off but still ok. You can hear everything going on very clearly and when you speak, the sound is good. overall a good buy.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>2013</td>\n",
       "      <td>Seriously, don't bother even looking at other apps for viewing webcams! This is THE BEST one, trust me and the other reviewers. I have 2 Loftek CXS 2200 webcams, and as I had expected, this app works flawlessly with them. It works so well that I got the paid versions for Android AND for iOS (to use with my iPad). So, get the free version, try it out to make sure that it works with your camera (I would be surprised if it doesn't! And would blame your camera... he he), and then get the paid versio</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>2018</td>\n",
       "      <td>just what I needed for utilitarian primping and beauty processes.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2015</td>\n",
       "      <td>Fast Shipping and worked as described. fit perfectly in dell diminsion 8500</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>2016</td>\n",
       "      <td>WONDERFULL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2013</td>\n",
       "      <td>The memory upgrade which I ordered to improve my dell computer arrived in great time, was exactly what I ordered, packaged well, also protected against any wet weather and it works perfectly.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2016</td>\n",
       "      <td>A good case.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2016</td>\n",
       "      <td>Um, I'm so hesitant to review this product. What your basically saying is I tested this on my nasty bum and tadah! It is no longer nasty :)\\n\\nIt's true though! I've never in my 33 years even heard of a product like this. After having a baby, 3 months ago I did stock up on feminine washes out of necessity due to sensitivity.\\n\\nLook, all I can say without totally sharing my business is that this product works. I will keep this in my travel/overnight bag. You know the stress of traveling actually agg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2013</td>\n",
       "      <td>This Kindle replaces one I had for almost 2 years.  I went over a week without my kindle and did not think I would make it until my new one came.  The good part is I had taken out the warranty offered by Amazon and am so glad I did.  I just called and explained my problem and zap!  a new one was on it's way.  Thank you Amazon</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2017</td>\n",
       "      <td>good.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>2015</td>\n",
       "      <td>This is really nice looking. I get compliments on it wherever I go. It also provides a little protection as well. They have improved on older versions by making an actual pocket that your iPad slips into with slots for taking photos and such.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>2015</td>\n",
       "      <td>I wasn't sure what to expect but this was terrific. The casting was great (loved Freema Agyeman) and the story was very nice- plenty of twists and suspense. I wish there was a sequel or prequel.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>2014</td>\n",
       "      <td>Perfect.\\nArrived a day late.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  \\\n",
       "331  2016   \n",
       "26   2018   \n",
       "240  2013   \n",
       "486  2018   \n",
       "126  2015   \n",
       "406  2016   \n",
       "459  2013   \n",
       "447  2016   \n",
       "163  2016   \n",
       "363  2013   \n",
       "77   2017   \n",
       "444  2015   \n",
       "405  2015   \n",
       "158  2018   \n",
       "303  2014   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Text  \\\n",
       "331                                                                                                                                                                                                                                                                                                                                                                                                                                                  Great color combo, maybe a little on the thicker side but overall happy!   \n",
       "26                                                                                                                                                 I am very happy with this purchase. It was easy to set up and does all I need it to. The picture is pretty good depending on the conditions, if the light is very bright, its not as clear but you can see everything and at night a little bit off but still ok. You can hear everything going on very clearly and when you speak, the sound is good. overall a good buy.   \n",
       "240      Seriously, don't bother even looking at other apps for viewing webcams! This is THE BEST one, trust me and the other reviewers. I have 2 Loftek CXS 2200 webcams, and as I had expected, this app works flawlessly with them. It works so well that I got the paid versions for Android AND for iOS (to use with my iPad). So, get the free version, try it out to make sure that it works with your camera (I would be surprised if it doesn't! And would blame your camera... he he), and then get the paid versio   \n",
       "486                                                                                                                                                                                                                                                                                                                                                                                                                                                         just what I needed for utilitarian primping and beauty processes.   \n",
       "126                                                                                                                                                                                                                                                                                                                                                                                                                                               Fast Shipping and worked as described. fit perfectly in dell diminsion 8500   \n",
       "406                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                WONDERFULL   \n",
       "459                                                                                                                                                                                                                                                                                                                           The memory upgrade which I ordered to improve my dell computer arrived in great time, was exactly what I ordered, packaged well, also protected against any wet weather and it works perfectly.   \n",
       "447                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              A good case.   \n",
       "163  Um, I'm so hesitant to review this product. What your basically saying is I tested this on my nasty bum and tadah! It is no longer nasty :)\\n\\nIt's true though! I've never in my 33 years even heard of a product like this. After having a baby, 3 months ago I did stock up on feminine washes out of necessity due to sensitivity.\\n\\nLook, all I can say without totally sharing my business is that this product works. I will keep this in my travel/overnight bag. You know the stress of traveling actually agg   \n",
       "363                                                                                                                                                                                   This Kindle replaces one I had for almost 2 years.  I went over a week without my kindle and did not think I would make it until my new one came.  The good part is I had taken out the warranty offered by Amazon and am so glad I did.  I just called and explained my problem and zap!  a new one was on it's way.  Thank you Amazon   \n",
       "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      good.   \n",
       "444                                                                                                                                                                                                                                                                        This is really nice looking. I get compliments on it wherever I go. It also provides a little protection as well. They have improved on older versions by making an actual pocket that your iPad slips into with slots for taking photos and such.   \n",
       "405                                                                                                                                                                                                                                                                                                                        I wasn't sure what to expect but this was terrific. The casting was great (loved Freema Agyeman) and the story was very nice- plenty of twists and suspense. I wish there was a sequel or prequel.   \n",
       "158                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1   \n",
       "303                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Perfect.\\nArrived a day late.   \n",
       "\n",
       "     Overall Rating  Sentiment  Prediction_value  Predicted_prob  \n",
       "331             4.0        1.0               0.0        0.000416  \n",
       "26              5.0        1.0               0.0        0.000055  \n",
       "240             5.0        1.0               0.0        0.004418  \n",
       "486             5.0        1.0               0.0        0.000139  \n",
       "126             5.0        1.0               0.0        0.230743  \n",
       "406             5.0        1.0               0.0        0.002081  \n",
       "459             5.0        1.0               0.0        0.012205  \n",
       "447             5.0        1.0               0.0        0.000219  \n",
       "163             4.0        1.0               0.0        0.019555  \n",
       "363             5.0        1.0               0.0        0.000318  \n",
       "77              4.0        1.0               0.0        0.000118  \n",
       "444             5.0        1.0               0.0        0.000270  \n",
       "405             5.0        1.0               0.0        0.017978  \n",
       "158             4.0        1.0               0.0        0.000785  \n",
       "303             5.0        1.0               0.0        0.001127  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negative_samples = false_negative\n",
    "\n",
    "print(\"False Negative \\n\",)\n",
    "\n",
    "print(\"*\" * 50)\n",
    "print('\\n')\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "false_negative_samples = shuffle(false_negative_samples)\n",
    "\n",
    "false_negative_samples[0:Examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Name</th>\n",
       "      <th>false_negative</th>\n",
       "      <th>false_positive</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall Rating</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>385</td>\n",
       "      <td>0</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>489</td>\n",
       "      <td>503</td>\n",
       "      <td>992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Name            false_negative  false_positive  Total\n",
       "Overall Rating                                       \n",
       "1.0                          0             185    185\n",
       "2.0                          0             106    106\n",
       "3.0                          0             212    212\n",
       "4.0                        104               0    104\n",
       "5.0                        385               0    385\n",
       "Total                      489             503    992"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive.insert(0, 'Name', 'false_positive')\n",
    "\n",
    "false_negative.insert(0, 'Name', 'false_negative')\n",
    "frames_2 =[false_positive, false_negative]\n",
    "result_incorrect = pd.concat(frames_2)\n",
    "df_result_incorrect = result_incorrect\n",
    "pd.crosstab(df_result_incorrect['Overall Rating'], df_result_incorrect['Name'], margins=True, margins_name = 'Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Model Year + Text Seperated (Model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2013', 'This is great']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text=[['2013', 'This is great']]\n",
    "sample_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 100), dtype=int32, numpy=\n",
       "array([[ 101, 1381,  102, 1188, 1110, 1632,  102,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]])>, 'token_type_ids': <tf.Tensor: shape=(1, 100), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 100), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= tokenizer_bert_base(sample_text,max_length=100,\n",
    "          truncation=True, \n",
    "          padding='max_length', \n",
    "          return_tensors='tf')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df =df_total[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "      <th>words</th>\n",
       "      <th>text_with_year</th>\n",
       "      <th>Year_string</th>\n",
       "      <th>Year_Text_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>997698</th>\n",
       "      <td>1375401600</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-02</td>\n",
       "      <td>I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>24</td>\n",
       "      <td>2013 I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.</td>\n",
       "      <td>2013</td>\n",
       "      <td>[2013, I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94687</th>\n",
       "      <td>1373587200</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>27</td>\n",
       "      <td>2013 This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!</td>\n",
       "      <td>2013</td>\n",
       "      <td>[2013, This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unixReviewTime  \\\n",
       "997698      1375401600   \n",
       "94687       1373587200   \n",
       "\n",
       "                                                                                                                                                                   reviewText  \\\n",
       "997698                                    I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687   This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "\n",
       "        overall  sentiment datetimeReviewTime  \\\n",
       "997698      3.0          0         2013-08-02   \n",
       "94687       5.0          1         2013-07-12   \n",
       "\n",
       "                                                                                                                                                                Original_text  \\\n",
       "997698                                    I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687   This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "\n",
       "        words  \\\n",
       "997698     24   \n",
       "94687      27   \n",
       "\n",
       "                                                                                                                                                                    text_with_year  \\\n",
       "997698                                    2013 I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.   \n",
       "94687   2013 This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!   \n",
       "\n",
       "       Year_string  \\\n",
       "997698        2013   \n",
       "94687         2013   \n",
       "\n",
       "                                                                                                                                                                       Year_Text_List  \n",
       "997698                                    [2013, I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.]  \n",
       "94687   [2013, This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!]  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_df = temp_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997698    2013\n",
       "94687     2013\n",
       "Name: Year_string, dtype: object"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Year_token =temp_df['Year_string']\n",
    "Year_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997698                                      I don't really dislike it but it is difficult to work with because it is hard you should give more information about your products.\n",
       "94687     This stuff really works. Worked in the heat with ridiculous humidity, parties with vigorous dancing and even some light workouts. Get it... your balls will be siked!\n",
       "Name: reviewText, dtype: object"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_token= temp_df['reviewText']\n",
    "text_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer_bert_base([str(x) for x in Year_token], [str(x) for x in text_token],\n",
    "              max_length=max_tokens_input,\n",
    "              truncation=True, padding='max_length', return_tensors='tf')\n",
    "\n",
    "x_test  = tokenizer_bert_base([str(x) for x in Year_token], [str(x) for x in text_token],\n",
    "              max_length=max_tokens_input,\n",
    "              truncation=True, padding='max_length', return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 80), dtype=int32, numpy=\n",
       "array([[  101,  1381,   102,   146,  1274,   112,   189,  1541, 20662,\n",
       "         1122,  1133,  1122,  1110,  2846,  1106,  1250,  1114,  1272,\n",
       "         1122,  1110,  1662,  1128,  1431,  1660,  1167,  1869,  1164,\n",
       "         1240,  2982,   119,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [  101,  1381,   102,  1188,  4333,  1541,  1759,   119,  6955,\n",
       "         1174,  1107,  1103,  3208,  1114,  9944, 20641,   117,  3512,\n",
       "         1114, 24739,  5923,  1105,  1256,  1199,  1609,  1250, 10469,\n",
       "          119,  3949,  1122,   119,   119,   119,  1240,  7318,  1209,\n",
       "         1129, 27466,  7831,   106,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0]])>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = copy.deepcopy(train_data_original)\n",
    "test_data = copy.deepcopy(test_data_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_year = train_data['Year_string']\n",
    "train_data_text = train_data['reviewText']\n",
    "test_data_year = test_data['Year_string']\n",
    "test_data_text = test_data['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment = train_data['sentiment']\n",
    "train_sentiment  = tf.convert_to_tensor(train_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment = test_data['sentiment']\n",
    "test_sentiment = tf.convert_to_tensor(test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "x_train = tokenizer_bert_base([str(x) for x in train_data_year],\n",
    "                              [str(x) for x in train_data_text],\n",
    "                              max_length=max_tokens_input,\n",
    "                              truncation=True, \n",
    "                              padding='max_length', \n",
    "                              return_tensors='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "x_test = tokenizer_bert_base([str(x) for x in test_data_year],\n",
    "                              [str(x) for x in test_data_text],\n",
    "                              max_length=max_tokens_input,\n",
    "                              truncation=True, \n",
    "                              padding='max_length', \n",
    "                              return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_model(hidden_size = 200, \n",
    "                                train_layers = -1, \n",
    "                                optimizer=tf.keras.optimizers.Adam(learning_rate_input)):\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_tokens_input,), dtype=tf.int32, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_tokens_input,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_tokens_input,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "    #restrict training to the train_layers outer transformer layers\n",
    "    if not train_layers == -1:\n",
    "\n",
    "            retrain_layers = []\n",
    "\n",
    "            for retrain_layer_number in range(train_layers):\n",
    "\n",
    "                layer_code = '_' + str(11 - retrain_layer_number)\n",
    "                retrain_layers.append(layer_code)\n",
    "\n",
    "            for w in bert_model.weights:\n",
    "                if not any([x in w.name for x in retrain_layers]):\n",
    "                    w._trainable = False\n",
    "\n",
    "\n",
    "    bert_out = bert_model(bert_inputs)\n",
    "\n",
    "\n",
    "    classification_token = tf.keras.layers.Lambda(lambda x: x[:,0,:], name='get_first_vector')(bert_out[0])\n",
    "\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, name='hidden_layer')(classification_token)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
    "\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], \n",
    "                                          outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=optimizer,\n",
    "                            loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                            metrics='accuracy')\n",
    "\n",
    "\n",
    "    return classification_model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model_3 = create_classification_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/fit/20211203-054601\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "print(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 1885s 4s/step - loss: 0.0626 - accuracy: 0.9762 - val_loss: 0.3598 - val_accuracy: 0.8815\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 1848s 4s/step - loss: 0.0395 - accuracy: 0.9854 - val_loss: 0.5836 - val_accuracy: 0.8825\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 1834s 4s/step - loss: 0.0414 - accuracy: 0.9862 - val_loss: 0.5472 - val_accuracy: 0.8985\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 1838s 4s/step - loss: 0.0436 - accuracy: 0.9860 - val_loss: 0.3754 - val_accuracy: 0.8875\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 1835s 4s/step - loss: 0.0311 - accuracy: 0.9880 - val_loss: 0.5033 - val_accuracy: 0.8890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22f200b5550>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model_3.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                           y_train,\n",
    "                           validation_data=([x_test.input_ids, x_test.token_type_ids, x_test.attention_mask],y_test),\n",
    "                           epochs=epochs_input,\n",
    "                           batch_size=batch_size_input,\n",
    "                           callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/fit/20211203-054601\n"
     ]
    }
   ],
   "source": [
    "print(logdir)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.save('saved_model/Bert_Year_Text_Seperated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_model_3 = classification_model_3.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9998975e-01],\n",
       "       [9.9997765e-01],\n",
       "       [1.2987852e-04],\n",
       "       ...,\n",
       "       [9.9590194e-01],\n",
       "       [2.3209432e-05],\n",
       "       [4.2279648e-06]], dtype=float32)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions_model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_values = []\n",
    "Actual_probabilities =[]\n",
    "for p in predictions:\n",
    "    value = p[0]\n",
    "    Actual_probabilities.append(value)\n",
    "    if value >= 0.5:\n",
    "        predictions_values.append(1)\n",
    "    else:\n",
    "        predictions_values.append(0)\n",
    "        \n",
    "predictions_values = []\n",
    "Actual_probabilities =[]\n",
    "for p in predictions:\n",
    "    value = p[0]\n",
    "    Actual_probabilities.append(value)\n",
    "    if value >= 0.5:\n",
    "        predictions_values.append(1)\n",
    "    else:\n",
    "        predictions_values.append(0)\n",
    "        \n",
    "test_sentiment = test_data['sentiment']\n",
    "sentiment_list = test_sentiment.to_list()\n",
    "data_year = test_data['Year_string'].to_list()\n",
    "data_text = test_data['Original_text'].to_list()\n",
    "rating = test_data['overall'].to_list()\n",
    "\n",
    "\n",
    "false_positive = {'Year': [],'Text': [], 'Overall Rating': [],'Sentiment':[],'Prediction_value': [], 'Predicted_prob': []}\n",
    "false_negative = {'Year': [],'Text': [], 'Overall Rating': [],'Sentiment':[],'Prediction_value': [], 'Predicted_prob': []}\n",
    "Correct_values = {'Year': [],'Text': [], 'Overall Rating': [],'Sentiment':[],'Prediction_value': [], 'Predicted_prob': []}\n",
    "false_positive = pd.DataFrame(false_positive)\n",
    "false_negative = pd.DataFrame(false_negative)\n",
    "Correct_values = pd.DataFrame(Correct_values)\n",
    "\n",
    "for i in range(len(sentiment_list)):\n",
    "    sent = sentiment_list[i]\n",
    "    year = data_year[i].split()[0]\n",
    "    text = data_text[i]\n",
    "    text = str(text)\n",
    "    if len(text)>500:\n",
    "        text =text[0:500]\n",
    "    else :\n",
    "        text= text \n",
    "    if predictions_values[i] == sentiment_list[i]:\n",
    "        new_row = {'Year': data_year[i],'Text': text, 'Overall Rating': rating[i],'Sentiment':sentiment_list[i],'Prediction_value': predictions_values[i],'Predicted_prob': Actual_probabilities[i]}\n",
    "        Correct_values = Correct_values.append(new_row, ignore_index=True)\n",
    "    else:\n",
    "        if predictions_values[i] == 1:\n",
    "            new_row = {'Year': data_year[i],'Text': text, 'Overall Rating': rating[i],'Sentiment': sentiment_list[i],'Prediction_value': predictions_values[i],'Predicted_prob': Actual_probabilities[i]}\n",
    "            false_positive = false_positive.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            new_row = {'Year': data_year[i],'Text': text, 'Overall Rating': rating[i],'Sentiment':sentiment_list[i],'Prediction_value': predictions_values[i],'Predicted_prob': Actual_probabilities[i]}\n",
    "            false_negative = false_negative.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive \n",
      "\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Text</th>\n",
       "      <th>Overall Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Prediction_value</th>\n",
       "      <th>Predicted_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2016</td>\n",
       "      <td>If you ask to unsubscribe from their useless newsletter, they delete your account.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2017</td>\n",
       "      <td>still have the noise...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2013</td>\n",
       "      <td>I do not actually own this and here's why. I see that this kit is being sold for $575! I own actual Fender STRATOcasters and have been playing for the last 18 years. The advertisement states that this is a Strat and but it's not. It's a STARcaster. Yes it has the Fender name on it and it's in the design and style of the Strat but it is cheaply manufactured by another company. It is the extreme bottom of the barrel and not a true representation of Fender's actual quality. It may come with an amp</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2017</td>\n",
       "      <td>I love the Irig microphone but the field just doesn't cut it. The sound quality is decent as long as you don't move your phone. But the whole point of recording with your phone is there's probably going to be movement. Whenever you move this you hear the device move.  Maybe if you wrap an elastic around it it wouldn't be so bad. But still this is a design flaw they should have considered before releasing it.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2018</td>\n",
       "      <td>This product caused a major allergic reaction with swelling and puffing under both eyes to the point I could not keep my eyes open comfortably nor sleep comfortably. It also caused bumps all over my face and neck to occur. It took a week of not using the product before my face finally started to calm down. Don't know what chemicals they have, but it was not good and caused much anxiety during the episode.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2018</td>\n",
       "      <td>These were eaten but the texture on them is not crisp. More like stale waffles with gooey honey center. The taste was great but I just wasn't into the texture.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.682299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2015</td>\n",
       "      <td>My cat will not eat these.  I'll be taking them to the animal shelter.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2015</td>\n",
       "      <td>I am having a tough time getting the sound I want just right from this head. I am still experimenting with what pedals give me what I am after. That being said, it is after all a Marshall so I know I will get what I am seeking. The option of 40 or 100 watts is very handy as well</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2015</td>\n",
       "      <td>i'm don't think good?</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2018</td>\n",
       "      <td>It is OK but missing tone generator function for cable tracing.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2018</td>\n",
       "      <td>did not work</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2014</td>\n",
       "      <td>Texture on these is fine cold but when you cook them they end up having the texture of firm mushrooms.  Our household does not particularly care for mushrooms so this is not a win.  A little goes a long way with these.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2018</td>\n",
       "      <td>I ordered two of these carts. One came without one of the front legs rendering it useless. I tried contacting their support but have had no response.  The other cart is great. Sturdy build, strong hold on my display, I really like it. But until I get the missing part, I cant really give this a great review.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2016</td>\n",
       "      <td>The wires are closer to 20 AWG (0.89 mm diameter, stranded). 18 AWG should be about 1.25 mm.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>false_positive</td>\n",
       "      <td>2017</td>\n",
       "      <td>Had a few bags leak. Not happy when I waste my babies meal.  Will not be buying this brand anymore. Also sucks to dump milk into bottles from the bag.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name  Year  \\\n",
       "254  false_positive  2016   \n",
       "78   false_positive  2017   \n",
       "481  false_positive  2013   \n",
       "106  false_positive  2017   \n",
       "59   false_positive  2018   \n",
       "369  false_positive  2018   \n",
       "347  false_positive  2015   \n",
       "290  false_positive  2015   \n",
       "127  false_positive  2015   \n",
       "11   false_positive  2018   \n",
       "459  false_positive  2018   \n",
       "52   false_positive  2014   \n",
       "133  false_positive  2018   \n",
       "260  false_positive  2016   \n",
       "2    false_positive  2017   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Text  \\\n",
       "254                                                                                                                                                                                                                                                                                                                                                                                                                                    If you ask to unsubscribe from their useless newsletter, they delete your account.   \n",
       "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                still have the noise...   \n",
       "481  I do not actually own this and here's why. I see that this kit is being sold for $575! I own actual Fender STRATOcasters and have been playing for the last 18 years. The advertisement states that this is a Strat and but it's not. It's a STARcaster. Yes it has the Fender name on it and it's in the design and style of the Strat but it is cheaply manufactured by another company. It is the extreme bottom of the barrel and not a true representation of Fender's actual quality. It may come with an amp    \n",
       "106                                                                                           I love the Irig microphone but the field just doesn't cut it. The sound quality is decent as long as you don't move your phone. But the whole point of recording with your phone is there's probably going to be movement. Whenever you move this you hear the device move.  Maybe if you wrap an elastic around it it wouldn't be so bad. But still this is a design flaw they should have considered before releasing it.   \n",
       "59                                                                                               This product caused a major allergic reaction with swelling and puffing under both eyes to the point I could not keep my eyes open comfortably nor sleep comfortably. It also caused bumps all over my face and neck to occur. It took a week of not using the product before my face finally started to calm down. Don't know what chemicals they have, but it was not good and caused much anxiety during the episode.   \n",
       "369                                                                                                                                                                                                                                                                                                                                                       These were eaten but the texture on them is not crisp. More like stale waffles with gooey honey center. The taste was great but I just wasn't into the texture.   \n",
       "347                                                                                                                                                                                                                                                                                                                                                                                                                                                My cat will not eat these.  I'll be taking them to the animal shelter.   \n",
       "290                                                                                                                                                                                                                               I am having a tough time getting the sound I want just right from this head. I am still experimenting with what pedals give me what I am after. That being said, it is after all a Marshall so I know I will get what I am seeking. The option of 40 or 100 watts is very handy as well   \n",
       "127                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 i'm don't think good?   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                        It is OK but missing tone generator function for cable tracing.   \n",
       "459                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          did not work   \n",
       "52                                                                                                                                                                                                                                                                                             Texture on these is fine cold but when you cook them they end up having the texture of firm mushrooms.  Our household does not particularly care for mushrooms so this is not a win.  A little goes a long way with these.   \n",
       "133                                                                                                                                                                                                  I ordered two of these carts. One came without one of the front legs rendering it useless. I tried contacting their support but have had no response.  The other cart is great. Sturdy build, strong hold on my display, I really like it. But until I get the missing part, I cant really give this a great review.   \n",
       "260                                                                                                                                                                                                                                                                                                                                                                                                                          The wires are closer to 20 AWG (0.89 mm diameter, stranded). 18 AWG should be about 1.25 mm.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                  Had a few bags leak. Not happy when I waste my babies meal.  Will not be buying this brand anymore. Also sucks to dump milk into bottles from the bag.   \n",
       "\n",
       "     Overall Rating  Sentiment  Prediction_value  Predicted_prob  \n",
       "254             1.0        0.0               1.0        0.997584  \n",
       "78              1.0        0.0               1.0        0.987339  \n",
       "481             1.0        0.0               1.0        0.998478  \n",
       "106             2.0        0.0               1.0        0.999971  \n",
       "59              1.0        0.0               1.0        0.996135  \n",
       "369             3.0        0.0               1.0        0.682299  \n",
       "347             2.0        0.0               1.0        0.999610  \n",
       "290             3.0        0.0               1.0        0.999532  \n",
       "127             2.0        0.0               1.0        0.999857  \n",
       "11              3.0        0.0               1.0        0.999996  \n",
       "459             1.0        0.0               1.0        0.999793  \n",
       "52              3.0        0.0               1.0        0.999709  \n",
       "133             3.0        0.0               1.0        0.999792  \n",
       "260             3.0        0.0               1.0        0.998570  \n",
       "2               1.0        0.0               1.0        0.999556  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Examples = 15\n",
    "import random\n",
    "false_positive_samples = false_positive\n",
    "\n",
    "print(\"False Positive \\n\")\n",
    "print(\"*\" * 50)\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "false_positive_samples = shuffle(false_positive_samples)\n",
    "\n",
    "false_positive_samples[0:Examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative \n",
      "\n",
      "**************************************************\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Text</th>\n",
       "      <th>Overall Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Prediction_value</th>\n",
       "      <th>Predicted_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2016</td>\n",
       "      <td>Great product, as decribed.\\n\\nFast shipper</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2014</td>\n",
       "      <td>I ordered this to replace a cord that had a short in it. It's identical to the factory Oster cord, and the very easy and simple installation took only a couple of minutes from start to finish. The price was right, the shipping fast, and the product excellent. What more could you ask for? If you're a professional who uses multiple Oster clippers daily, it would be a good idea to order an extra to keep on hand for the future, especially considering the great deal you get here.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2013</td>\n",
       "      <td>I bought this Thermal Compound along with the Thermal Pads (http://www.amazon.com/gp/product/B007PODQTQ/ref=cm_cr_ryp_prd_ttl_hst_13) when I planned to replace the thermal pads of my old Dell XPS 1530 laptop. As all other products from Amazon and the sellers on Amazon, this one didn't disappoint me. The quantities are sufficient for more than 3 dozen applicatioons and cleanings.\\n\\nOne has to keep in mind to fist use Arctic Clean 1 first to remove bulk of residual grease or melted thermal pad and</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2013</td>\n",
       "      <td>I decided to purchase this because it is a Panasonic device labeled as Everpower. So far iIt is working perfectly. I knew it didn't come with software and I am still looking for a Bluray player software to see the movies in the PC if I want to.\\nRecommended!!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>2016</td>\n",
       "      <td>Works well</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>2016</td>\n",
       "      <td>It worked very well i disliked nothing</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018</td>\n",
       "      <td>Gets my airbrush clean and keeps it clog free. Great price compared to other brands.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>2013</td>\n",
       "      <td>My mouth feels great after a water flossing.  I put mouthwash into the fill bucket so that my teeth and gums get a thorough cleaning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>2017</td>\n",
       "      <td>Second time buying, cute, multicolored, recommend.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2015</td>\n",
       "      <td>Product is as advertised and works well.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>2016</td>\n",
       "      <td>I was really surprised at the 10.1 inch display when I actually received the unit and had it in my hand.  Being an owner of the original Fire HD, I can not say enough about the innovative approaches Amazon has put into these tablets.  I have watched a couple of movies and the clarity is superb.  I also like the \"desk top\" with the various icons displayed versus the carousel in the Fire HD. I am waiting for the arrival of the cover case, which I believe will enhance the handling characteristics .</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2014</td>\n",
       "      <td>Love these flip flops. Fit perfectly!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>2013</td>\n",
       "      <td>My kids bought the Kindle Fire HD for me for Mother's Day. I love it! I was surprised how much heavier it is than my regular Kindle, will take some getting used to.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2015</td>\n",
       "      <td>Great product. Great brand... Will be buying more.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>2016</td>\n",
       "      <td>I have been buying these Apple Certified iXCC cables for awhile -- these are my second or third purchase in various lengths for a couple of iPhones. The cables are excellent -- well-made and quite substantial, so they have held up well under the normal wear and tear that I subject them to -- the charging and recharging, as well as frequent connections to the car and elsewhere. The wires have not split, and the plugs remain solidly attached to the wires. There are no extraneous sounds that indica</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  \\\n",
       "89   2016   \n",
       "46   2014   \n",
       "43   2013   \n",
       "223  2013   \n",
       "339  2016   \n",
       "396  2016   \n",
       "11   2018   \n",
       "437  2013   \n",
       "219  2017   \n",
       "20   2015   \n",
       "279  2016   \n",
       "80   2014   \n",
       "408  2013   \n",
       "197  2015   \n",
       "402  2016   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Text  \\\n",
       "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Great product, as decribed.\\n\\nFast shipper   \n",
       "46                          I ordered this to replace a cord that had a short in it. It's identical to the factory Oster cord, and the very easy and simple installation took only a couple of minutes from start to finish. The price was right, the shipping fast, and the product excellent. What more could you ask for? If you're a professional who uses multiple Oster clippers daily, it would be a good idea to order an extra to keep on hand for the future, especially considering the great deal you get here.   \n",
       "43   I bought this Thermal Compound along with the Thermal Pads (http://www.amazon.com/gp/product/B007PODQTQ/ref=cm_cr_ryp_prd_ttl_hst_13) when I planned to replace the thermal pads of my old Dell XPS 1530 laptop. As all other products from Amazon and the sellers on Amazon, this one didn't disappoint me. The quantities are sufficient for more than 3 dozen applicatioons and cleanings.\\n\\nOne has to keep in mind to fist use Arctic Clean 1 first to remove bulk of residual grease or melted thermal pad and    \n",
       "223                                                                                                                                                                                                                                                     I decided to purchase this because it is a Panasonic device labeled as Everpower. So far iIt is working perfectly. I knew it didn't come with software and I am still looking for a Bluray player software to see the movies in the PC if I want to.\\nRecommended!!   \n",
       "339                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Works well   \n",
       "396                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  It worked very well i disliked nothing   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                     Gets my airbrush clean and keeps it clog free. Great price compared to other brands.   \n",
       "437                                                                                                                                                                                                                                                                                                                                                                                    My mouth feels great after a water flossing.  I put mouthwash into the fill bucket so that my teeth and gums get a thorough cleaning   \n",
       "219                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Second time buying, cute, multicolored, recommend.   \n",
       "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Product is as advertised and works well.   \n",
       "279    I was really surprised at the 10.1 inch display when I actually received the unit and had it in my hand.  Being an owner of the original Fire HD, I can not say enough about the innovative approaches Amazon has put into these tablets.  I have watched a couple of movies and the clarity is superb.  I also like the \"desk top\" with the various icons displayed versus the carousel in the Fire HD. I am waiting for the arrival of the cover case, which I believe will enhance the handling characteristics .   \n",
       "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Love these flip flops. Fit perfectly!   \n",
       "408                                                                                                                                                                                                                                                                                                                                                    My kids bought the Kindle Fire HD for me for Mother's Day. I love it! I was surprised how much heavier it is than my regular Kindle, will take some getting used to.   \n",
       "197                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Great product. Great brand... Will be buying more.   \n",
       "402    I have been buying these Apple Certified iXCC cables for awhile -- these are my second or third purchase in various lengths for a couple of iPhones. The cables are excellent -- well-made and quite substantial, so they have held up well under the normal wear and tear that I subject them to -- the charging and recharging, as well as frequent connections to the car and elsewhere. The wires have not split, and the plugs remain solidly attached to the wires. There are no extraneous sounds that indica   \n",
       "\n",
       "     Overall Rating  Sentiment  Prediction_value  Predicted_prob  \n",
       "89              5.0        1.0               0.0        0.000030  \n",
       "46              5.0        1.0               0.0        0.000009  \n",
       "43              5.0        1.0               0.0        0.000192  \n",
       "223             5.0        1.0               0.0        0.001962  \n",
       "339             5.0        1.0               0.0        0.000005  \n",
       "396             4.0        1.0               0.0        0.000081  \n",
       "11              5.0        1.0               0.0        0.000009  \n",
       "437             5.0        1.0               0.0        0.000004  \n",
       "219             5.0        1.0               0.0        0.001023  \n",
       "20              5.0        1.0               0.0        0.000636  \n",
       "279             5.0        1.0               0.0        0.000002  \n",
       "80              5.0        1.0               0.0        0.000003  \n",
       "408             5.0        1.0               0.0        0.000098  \n",
       "197             5.0        1.0               0.0        0.000005  \n",
       "402             5.0        1.0               0.0        0.000254  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negative_samples = false_negative\n",
    "\n",
    "print(\"False Negative \\n\",)\n",
    "\n",
    "print(\"*\" * 50)\n",
    "print('\\n')\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "false_negative_samples = shuffle(false_negative_samples)\n",
    "\n",
    "false_negative_samples[0:Examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Name</th>\n",
       "      <th>false_negative</th>\n",
       "      <th>false_positive</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall Rating</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>387</td>\n",
       "      <td>0</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>493</td>\n",
       "      <td>506</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Name            false_negative  false_positive  Total\n",
       "Overall Rating                                       \n",
       "1.0                          0             185    185\n",
       "2.0                          0             108    108\n",
       "3.0                          0             213    213\n",
       "4.0                        106               0    106\n",
       "5.0                        387               0    387\n",
       "Total                      493             506    999"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive.insert(0, 'Name', 'false_positive')\n",
    "\n",
    "false_negative.insert(0, 'Name', 'false_negative')\n",
    "frames_2 =[false_positive, false_negative]\n",
    "result_incorrect = pd.concat(frames_2)\n",
    "df_result_incorrect = result_incorrect\n",
    "pd.crosstab(df_result_incorrect['Overall Rating'], df_result_incorrect['Name'], margins=True, margins_name = 'Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta Model Review Text (Model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = copy.deepcopy(train_data_original)\n",
    "test_data = copy.deepcopy(test_data_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "      <th>words</th>\n",
       "      <th>text_with_year</th>\n",
       "      <th>Year_string</th>\n",
       "      <th>Year_Text_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>628085</th>\n",
       "      <td>1436659200</td>\n",
       "      <td>great item</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-07-12</td>\n",
       "      <td>great item</td>\n",
       "      <td>2</td>\n",
       "      <td>2015 great item</td>\n",
       "      <td>2015</td>\n",
       "      <td>[2015, great item]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167501</th>\n",
       "      <td>1442620800</td>\n",
       "      <td>Great product and nice to keep sanitized.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-09-19</td>\n",
       "      <td>Great product and nice to keep sanitized.</td>\n",
       "      <td>7</td>\n",
       "      <td>2015 Great product and nice to keep sanitized.</td>\n",
       "      <td>2015</td>\n",
       "      <td>[2015, Great product and nice to keep sanitized.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unixReviewTime                                 reviewText  overall  \\\n",
       "628085      1436659200                                 great item      5.0   \n",
       "167501      1442620800  Great product and nice to keep sanitized.      5.0   \n",
       "\n",
       "        sentiment datetimeReviewTime  \\\n",
       "628085          1         2015-07-12   \n",
       "167501          1         2015-09-19   \n",
       "\n",
       "                                    Original_text  words  \\\n",
       "628085                                 great item      2   \n",
       "167501  Great product and nice to keep sanitized.      7   \n",
       "\n",
       "                                        text_with_year Year_string  \\\n",
       "628085                                 2015 great item        2015   \n",
       "167501  2015 Great product and nice to keep sanitized.        2015   \n",
       "\n",
       "                                           Year_Text_List  \n",
       "628085                                 [2015, great item]  \n",
       "167501  [2015, Great product and nice to keep sanitized.]  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =train_data[['sentiment', 'reviewText']]\n",
    "test_data =test_data[['sentiment', 'reviewText']]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>great item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Great product and nice to keep sanitized.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                 reviewText\n",
       "0          1                                 great item\n",
       "1          1  Great product and nice to keep sanitized."
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The only problem was the shipping. It was tossed into a much bigger box without sufficient padding, so a few of the colors shattered. It was a Christmas gift, so that was disappointing. The pallet itself had usable colors and applied nicely. My daughter was overall happy with it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>small size, but works well</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  \\\n",
       "0          1   \n",
       "1          1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                 reviewText  \n",
       "0  The only problem was the shipping. It was tossed into a much bigger box without sufficient padding, so a few of the colors shattered. It was a Christmas gift, so that was disappointing. The pallet itself had usable colors and applied nicely. My daughter was overall happy with it.  \n",
       "1                                                                                                                                                                                                                                                                small size, but works well  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClass(\n",
       "  (l1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base', \n",
    "                                             truncation=True, \n",
    "                                             do_lower_case=True)\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "from transformers import RobertaModel\n",
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, \n",
    "                           attention_mask=attention_mask, \n",
    "                           token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "    \n",
    "model = RobertaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_data = [tokenizer_roberta.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_tokens_input,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        for text in train_data['reviewText']]\n",
    "test_tokenized_data = [tokenizer_roberta.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_tokens_input,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        for text in test_data['reviewText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=learning_rate_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SentimentData(Dataset):\n",
    "    def __init__(self, data, inputs_tokenized):\n",
    "        self.inputs = inputs_tokenized\n",
    "        self.text = data['reviewText']\n",
    "        self.targets = data['sentiment']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        input = self.inputs[index]\n",
    "        ids = input['input_ids']\n",
    "        mask = input['attention_mask']\n",
    "        token_type_ids = input['token_type_ids']\n",
    "\n",
    "        return {\n",
    "            'sentence': text,\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "train_dataset = SentimentData(train_data, train_tokenized_data)\n",
    "test_dataset = SentimentData(test_data, test_tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': batch_size_input,\n",
    "                'shuffle': True\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': batch_size_input,\n",
    "                'shuffle': True\n",
    "                }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, **train_params)\n",
    "test_loader = DataLoader(test_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "test_answers = [[[],[]], [[],[]]]\n",
    "\n",
    "def train_loop(epochs):\n",
    "  for epoch in range(epochs):\n",
    "    for phase in ['Train', 'Test']:\n",
    "      if(phase == 'Train'):\n",
    "        model.train()\n",
    "        loader = train_loader\n",
    "      else:\n",
    "        model.eval()\n",
    "        loader = test_loader  \n",
    "      epoch_loss = 0\n",
    "      epoch_acc = 0\n",
    "      for steps, data in tqdm(enumerate(loader, 0)):\n",
    "        sentence = data['sentence']\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model.forward(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_function(outputs, targets)        \n",
    "        \n",
    "        epoch_loss += loss.detach()\n",
    "        _, max_indices = torch.max(outputs.data, dim=1)\n",
    "        bath_acc = (max_indices==targets).sum().item()/targets.size(0)\n",
    "        epoch_acc += bath_acc\n",
    "\n",
    "        if (phase == 'Train'):\n",
    "          train_loss.append(loss.detach()) \n",
    "          train_accuracy.append(bath_acc)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        else:\n",
    "          test_loss.append(loss.detach()) \n",
    "          test_accuracy.append(bath_acc)\n",
    "          if epoch == epochs-1:\n",
    "            for i in range(len(targets)):\n",
    "              test_answers[targets[i].item()][max_indices[i].item()].append([sentence[i], \n",
    "                                                                 targets[i].item(), \n",
    "                                                                 max_indices[i].item()])\n",
    "\n",
    "      print(f\"{phase} Loss: {epoch_loss/steps}\")\n",
    "      print(f\"{phase} Accuracy: {epoch_acc/steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [37:52,  4.54s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3257475197315216\n",
      "Train Accuracy: 0.8654809619238477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [02:58,  1.43s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.27137261629104614\n",
      "Test Accuracy: 0.9122983870967742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [37:49,  4.54s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.22314849495887756\n",
      "Train Accuracy: 0.9159569138276553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [02:59,  1.43s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.267788827419281\n",
      "Test Accuracy: 0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [36:53,  4.43s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1605372577905655\n",
      "Train Accuracy: 0.9427605210420842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [02:15,  1.09s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2557421326637268\n",
      "Test Accuracy: 0.9153225806451613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [30:47,  3.70s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.11484195291996002\n",
      "Train Accuracy: 0.9572895791583166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [02:17,  1.10s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2672377824783325\n",
      "Test Accuracy: 0.9077620967741935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [30:13,  3.63s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0969042256474495\n",
      "Train Accuracy: 0.967560120240481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [02:12,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.24354314804077148\n",
      "Test Accuracy: 0.9148185483870968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop(epochs_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative:\n",
      " ['I received on time in fact earlier than expected so it was a perfect birthday gift. When the seller said accessories that was not true. I received the kindle with what normally comes with it, no accessories.', 1, 0] \n",
      " ['It ended up being smaller than I expected and not really suitable for my long, curly wig.', 1, 0] \n",
      " ['a little goopy for my tastes. Was looking for a texturizing product for my short hair, but this made it fall flat. Maybe for someone with really thick hair this would work better.', 1, 0] \n",
      " \n",
      "False Positive:\n",
      " ['Smells and feels super nice but surprisingly it made me break out. I have very resilient skin and have never had more than two or three blemishes but after using this for a week, I had at least 8. It was very alarming and took some time (about two weeks) to get my skin back to normal.', 0, 1] \n",
      " ['This skill is fine. No integration with your fidelity account and for some reason it reads the entire timestamp to you when it tells you a stock price.', 0, 1] \n",
      " ['This is my 1st fire tablet. I waited to write a review so I could use it & give an honest opinion. It takes FOREVER to charge, and when the WiFi is on the tablet drains quickly. That being said, I do like reading on it. The screen size is nice & can be enlarged if needed.', 0, 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('False Negative:\\n', test_answers[1][0][0], \"\\n\", test_answers[1][0][1], \"\\n\", test_answers[1][0][2], \"\\n\",\n",
    "      '\\nFalse Positive:\\n', test_answers[0][1][0], '\\n', test_answers[0][1][1], '\\n', test_answers[0][1][2], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta Model Text year Concat (Model_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = copy.deepcopy(train_data_original)\n",
    "test_data = copy.deepcopy(test_data_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>datetimeReviewTime</th>\n",
       "      <th>Original_text</th>\n",
       "      <th>words</th>\n",
       "      <th>text_with_year</th>\n",
       "      <th>Year_string</th>\n",
       "      <th>Year_Text_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>628085</th>\n",
       "      <td>1436659200</td>\n",
       "      <td>great item</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-07-12</td>\n",
       "      <td>great item</td>\n",
       "      <td>2</td>\n",
       "      <td>2015 great item</td>\n",
       "      <td>2015</td>\n",
       "      <td>[2015, great item]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167501</th>\n",
       "      <td>1442620800</td>\n",
       "      <td>Great product and nice to keep sanitized.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-09-19</td>\n",
       "      <td>Great product and nice to keep sanitized.</td>\n",
       "      <td>7</td>\n",
       "      <td>2015 Great product and nice to keep sanitized.</td>\n",
       "      <td>2015</td>\n",
       "      <td>[2015, Great product and nice to keep sanitized.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        unixReviewTime                                 reviewText  overall  \\\n",
       "628085      1436659200                                 great item      5.0   \n",
       "167501      1442620800  Great product and nice to keep sanitized.      5.0   \n",
       "\n",
       "        sentiment datetimeReviewTime  \\\n",
       "628085          1         2015-07-12   \n",
       "167501          1         2015-09-19   \n",
       "\n",
       "                                    Original_text  words  \\\n",
       "628085                                 great item      2   \n",
       "167501  Great product and nice to keep sanitized.      7   \n",
       "\n",
       "                                        text_with_year Year_string  \\\n",
       "628085                                 2015 great item        2015   \n",
       "167501  2015 Great product and nice to keep sanitized.        2015   \n",
       "\n",
       "                                           Year_Text_List  \n",
       "628085                                 [2015, great item]  \n",
       "167501  [2015, Great product and nice to keep sanitized.]  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =train_data[['sentiment', 'text_with_year']]\n",
    "test_data =test_data[['sentiment', 'text_with_year']]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_data = [tokenizer_roberta.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_tokens_input,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        for text in train_data['text_with_year']]\n",
    "test_tokenized_data = [tokenizer_roberta.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_tokens_input,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        for text in test_data['text_with_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SentimentData(Dataset):\n",
    "    def __init__(self, data, inputs_tokenized):\n",
    "        self.inputs = inputs_tokenized\n",
    "        self.text = data['text_with_year']\n",
    "        self.targets = data['sentiment']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        input = self.inputs[index]\n",
    "        ids = input['input_ids']\n",
    "        mask = input['attention_mask']\n",
    "        token_type_ids = input['token_type_ids']\n",
    "\n",
    "        return {\n",
    "            'sentence': text,\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "train_dataset = SentimentData(train_data, train_tokenized_data)\n",
    "test_dataset = SentimentData(test_data, test_tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': batch_size_input,\n",
    "                'shuffle': True\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': batch_size_input,\n",
    "                'shuffle': True\n",
    "                }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, **train_params)\n",
    "test_loader = DataLoader(test_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "test_answers = [[[],[]], [[],[]]]\n",
    "\n",
    "def train_loop(epochs):\n",
    "  for epoch in range(epochs):\n",
    "    for phase in ['Train', 'Test']:\n",
    "      if(phase == 'Train'):\n",
    "        model.train()\n",
    "        loader = train_loader\n",
    "      else:\n",
    "        model.eval()\n",
    "        loader = test_loader  \n",
    "      epoch_loss = 0\n",
    "      epoch_acc = 0\n",
    "      for steps, data in tqdm(enumerate(loader, 0)):\n",
    "        sentence = data['sentence']\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model.forward(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_function(outputs, targets)        \n",
    "        \n",
    "        epoch_loss += loss.detach()\n",
    "        _, max_indices = torch.max(outputs.data, dim=1)\n",
    "        bath_acc = (max_indices==targets).sum().item()/targets.size(0)\n",
    "        epoch_acc += bath_acc\n",
    "\n",
    "        if (phase == 'Train'):\n",
    "          train_loss.append(loss.detach()) \n",
    "          train_accuracy.append(bath_acc)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        else:\n",
    "          test_loss.append(loss.detach()) \n",
    "          test_accuracy.append(bath_acc)\n",
    "          if epoch == epochs-1:\n",
    "            for i in range(len(targets)):\n",
    "              test_answers[targets[i].item()][max_indices[i].item()].append([sentence[i], \n",
    "                                                                 targets[i].item(), \n",
    "                                                                 max_indices[i].item()])\n",
    "\n",
    "      print(f\"{phase} Loss: {epoch_loss/steps}\")\n",
    "      print(f\"{phase} Accuracy: {epoch_acc/steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [30:15,  3.63s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.09755197167396545\n",
      "Train Accuracy: 0.967059118236473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [02:13,  1.06s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3852009177207947\n",
      "Test Accuracy: 0.8830645161290323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [29:57,  3.59s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.07367594540119171\n",
      "Train Accuracy: 0.9775801603206413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [02:12,  1.06s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3054005205631256\n",
      "Test Accuracy: 0.9198588709677419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [29:56,  3.59s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.06699316203594208\n",
      "Train Accuracy: 0.9774549098196392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [02:15,  1.08s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3821345269680023\n",
      "Test Accuracy: 0.9178427419354839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [29:48,  3.58s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0771292895078659\n",
      "Train Accuracy: 0.9746993987975952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [02:14,  1.07s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.45205551385879517\n",
      "Test Accuracy: 0.9163306451612904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [29:58,  3.60s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.052948206663131714\n",
      "Train Accuracy: 0.9828406813627254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [02:13,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3850998282432556\n",
      "Test Accuracy: 0.9173387096774194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop(epochs_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative:\n",
      " [\"2013 I've had one of these before and I remember the older one being much louder. I wish the cord were a little longer.\", 1, 0] \n",
      " [\"2015 Great for a seasoned tech. I wouldn't recommend this drill for a nonprofessional.\", 1, 0] \n",
      " ['2015 Excellent for the price', 1, 0] \n",
      " \n",
      "False Positive:\n",
      " ['2013 the quality of the protector is good, it is well packaged and easy to apply, as well as quite thick... the only problem is, its TOO thick. after application i immediately began experiencing problems with responsiveness to touch input... i actually thought it was some other problem, and tried for quite a while to fix it, up to and including resetting the tablet, before realizing that the problems coincided with installing the screen protector. removed the protector, and the problems resolved themselves. to assure myself it was in fact the protector, i then applied the second one in the pack... and again, experienced problems with touch input responsiveness. so, unfortunately, while it is a nice protector with good fit and finish, i cant recommend it. your mileage may vary, but my personal tablet doesnt like having the protector on it.', 0, 1] \n",
      " [\"2016 Pretty good, considering the low price. On first use the cover is a bit stiff and doesn't easily fold into the triangle-stand, but I expect that will soften over time and become more flexible. The angle it holds screen at is a bit to vertical for typing, wish it held it just a tad more angled. All minor gripes. For an under $10 case I am pleased enough.\", 0, 1] \n",
      " ['2014 Good for a 4 year old or a senior citizen.', 0, 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('False Negative:\\n', test_answers[1][0][0], \"\\n\", test_answers[1][0][1], \"\\n\", test_answers[1][0][2], \"\\n\",\n",
    "      '\\nFalse Positive:\\n', test_answers[0][1][0], '\\n', test_answers[0][1][1], '\\n', test_answers[0][1][2], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bert_Implementation_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
