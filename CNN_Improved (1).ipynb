{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Improved.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pnp0ErblmrCm",
        "outputId": "764ee84c-7930-4013-8826-a5e11ac36805"
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "import nltk\n",
        "import string\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.constraints import unit_norm\n",
        "from datetime import datetime\n",
        "drive.mount('/content/gdrive')\n",
        "nltk.download('stopwords')\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TQWEou3_IYj"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQwI3ThSmiIr",
        "outputId": "bb4899ae-4a5c-4c47-bf69-38b831954894"
      },
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/All_Amazon_Review_5.json.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-01 07:35:24--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/All_Amazon_Review_5.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25270692239 (24G) [application/octet-stream]\n",
            "Saving to: ‘All_Amazon_Review_5.json.gz’\n",
            "\n",
            "All_Amazon_Review_5 100%[===================>]  23.54G  53.7MB/s    in 8m 17s  \n",
            "\n",
            "2021-12-01 07:43:41 (48.5 MB/s) - ‘All_Amazon_Review_5.json.gz’ saved [25270692239/25270692239]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxm_7Hp-mzHn"
      },
      "source": [
        "balanced_data = []\n",
        "with gzip.open('All_Amazon_Review_5.json.gz') as f:\n",
        "    pos_limit = 500000\n",
        "    neg_limit = 500000\n",
        "    for l in f:\n",
        "        #Using first 1M records. All records used up all the RAM. \n",
        "        if pos_limit == 0 and neg_limit == 0:\n",
        "          break\n",
        "        json_data = json.loads(l.strip())\n",
        "        overall = json_data['overall']\n",
        "        if 'reviewText' not in json_data:\n",
        "          continue\n",
        "        if overall < 4.0 and neg_limit > 0: \n",
        "          balanced_data.append(json_data)\n",
        "          neg_limit -= 1\n",
        "        elif overall >= 4.0 and pos_limit > 0:\n",
        "          balanced_data.append(json_data)\n",
        "          pos_limit -= 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVjya-p4m2j6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9a0dea-9978-4f86-b38f-1a73eb1aecd1"
      },
      "source": [
        "balanced_df = pd.DataFrame.from_dict(balanced_data)\n",
        "b_sentiment_df = pd.concat([balanced_df['unixReviewTime'], balanced_df['reviewText'], balanced_df['overall']], axis=1)\n",
        "b_sentiment_df['sentiment'] = np.where(b_sentiment_df['overall'] < 4.0, 0, 1)\n",
        "b_sentiment_df['datetimeReviewTime'] = pd.to_datetime(b_sentiment_df['unixReviewTime'], unit='s')\n",
        "b_sentiment_df['reviewText'] = np.where(pd.isnull(b_sentiment_df['reviewText']), b_sentiment_df['reviewText'], b_sentiment_df['reviewText'].astype(str))\n",
        "print(len(b_sentiment_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yayxW65LnCoE"
      },
      "source": [
        "\n",
        "df_13 = b_sentiment_df[(b_sentiment_df['datetimeReviewTime'] >= '2013-01-01') & (b_sentiment_df['datetimeReviewTime'] < '2014-01-01')]\n",
        "df_14 = b_sentiment_df[(b_sentiment_df['datetimeReviewTime'] >= '2014-01-01') & (b_sentiment_df['datetimeReviewTime'] < '2015-01-01')]\n",
        "df_15 = b_sentiment_df[(b_sentiment_df['datetimeReviewTime'] >= '2015-01-01') & (b_sentiment_df['datetimeReviewTime'] < '2016-01-01')]\n",
        "df_16 = b_sentiment_df[(b_sentiment_df['datetimeReviewTime'] >= '2016-01-01') & (b_sentiment_df['datetimeReviewTime'] < '2017-01-01')]\n",
        "df_17 = b_sentiment_df[(b_sentiment_df['datetimeReviewTime'] >= '2017-01-01') & (b_sentiment_df['datetimeReviewTime'] < '2018-01-01')]\n",
        "df_18 = b_sentiment_df[(b_sentiment_df['datetimeReviewTime'] >= '2018-01-01') & (b_sentiment_df['datetimeReviewTime'] < '2019-01-01')]\n",
        "\n",
        "\n",
        "\n",
        "sample_size = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUFQ41wPnGvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc53b43-0469-46a4-eae3-2a2a2a4e96f8"
      },
      "source": [
        "df_total = pd.concat([df_13.sample(sample_size, random_state=123), df_14.sample(sample_size, random_state=123), df_15.sample(sample_size, random_state=123), df_16.sample(sample_size, random_state=123), df_17.sample(sample_size, random_state=123), df_18.sample(sample_size, random_state=123)]).dropna()\n",
        "print(len(df_total))\n",
        "train_data, test_data, train_sentiment, test_sentiment = train_test_split(df_total.drop('sentiment', axis=1), df_total['sentiment'], test_size=0.2, random_state=12345)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C49EacrQnRWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf78f31c-c502-4a54-eb56-6026ef9396a7"
      },
      "source": [
        "# print(np.mean(df_13['sentiment']))\n",
        "# print(np.mean(df_14['sentiment']))\n",
        "# print(np.mean(df_15['sentiment']))\n",
        "# print(np.mean(df_16['sentiment']))\n",
        "# print(np.mean(df_17['sentiment']))\n",
        "# print(np.mean(df_18['sentiment']))\n",
        "\n",
        "print(np.mean(df_total['sentiment']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD2d0HIprTU_"
      },
      "source": [
        "Sentiment looks roughly balanced in each year. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2yLpJLrqs1I",
        "outputId": "5324e6af-5b75-471a-e649-760956787b27"
      },
      "source": [
        "all_train_text = train_data['reviewText']\n",
        "\n",
        "vocab = Counter()\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "clean_text = []\n",
        "for x in all_train_text:\n",
        "  #unigrams\n",
        "  x = x.lower()\n",
        "  tokens = x.split()\n",
        "\n",
        "  #remove punctuation\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "\n",
        "  #remove stopwords\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  vocab.update(tokens)\n",
        "  clean_text.append(tokens)\n",
        "  \n",
        "print(len(vocab))\n",
        "print(vocab.most_common(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45229\n",
            "[('one', 10622), ('like', 10081), ('great', 10058), ('use', 9371), ('', 9220), ('good', 8358), ('would', 8312), ('product', 7446), ('get', 7348), ('kindle', 6670)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaUJrs_-rawe",
        "outputId": "eda1e6de-937c-4693-ff9c-31f3a06c2e06"
      },
      "source": [
        "min_occurance=2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
        "print(len(tokens))\n",
        "print(tokens[:10])\n",
        "vocab = set(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21412\n",
            "['tooooo', 'leathery', 'could', 'barely', 'chew', 'amazing', 'love', 'like', 'many', 'probably']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTUiLEq8r_kR"
      },
      "source": [
        "filtered_text = []\n",
        "for text in clean_text:\n",
        "  text = [w for w in text if w in vocab]\n",
        "  filtered_text.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUuKqzFIsI2y"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(filtered_text)\n",
        "encoded_text = tokenizer.texts_to_sequences(filtered_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Hv91LltZK-"
      },
      "source": [
        "# # print(filtered_text[:3])\n",
        "# # print(encoded_text[:3])\n",
        "\n",
        "# for i in range(len(filtered_text)): \n",
        "#   text = filtered_text[i]\n",
        "#   for j in range(len(text)):\n",
        "#     word = text[j]\n",
        "#     if word == 'works':\n",
        "#       print(\"WORKS\")\n",
        "#       print(encoded_text[i][j])\n",
        "#     # if word == 'nice':\n",
        "#     #   print(\"NICE\")\n",
        "#     #   print(encoded_text[i][j]) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c22R-SjGWRrg"
      },
      "source": [
        "max_len = 80"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnYyi3i0tqB6",
        "outputId": "1d8d7ec9-58c4-4cff-be80-67e6e7423072"
      },
      "source": [
        "train_text = pad_sequences(encoded_text, maxlen=max_len, padding='post')\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXpy_FMXv0mD"
      },
      "source": [
        "all_test_text = test_data['reviewText']\n",
        "filtered_text_test = []\n",
        "for x in all_test_text:\n",
        "  #unigrams\n",
        "  x = x.lower()\n",
        "  tokens = x.split()\n",
        "\n",
        "  #remove punctuation\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "\n",
        "  #remove stopwords\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "  #filter\n",
        "  filtered_tokens = [w for w in tokens if w in vocab]\n",
        "  \n",
        "  filtered_text_test.append(filtered_tokens)\n",
        "\n",
        "\n",
        "encoded_text_test = tokenizer.texts_to_sequences(filtered_text_test)\n",
        "test_text = pad_sequences(encoded_text_test, maxlen=max_len, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBoO_LA05s2C"
      },
      "source": [
        "# input = tf.keras.layers.Input(shape=(max_len,))\n",
        "# embedding = Embedding(vocab_size, 100, input_length=max_len, name='embedding_layer')(input)\n",
        "# conv_size_8 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_8', padding='same')(embedding)\n",
        "# conv_size_4 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_4', padding='same')(embedding)\n",
        "# conv_size_2 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_2', padding='same')(embedding)\n",
        "# pooling_8 = MaxPooling1D(pool_size=max_len, name='pooling_8')(conv_size_8)\n",
        "# pooling_4 = MaxPooling1D(pool_size=max_len, name='pooling_4')(conv_size_4)\n",
        "# pooling_2 = MaxPooling1D(pool_size=max_len, name='pooling_2')(conv_size_2)\n",
        "# concat = tf.keras.layers.concatenate([pooling_8, pooling_4, pooling_2], axis=2, name='concatenation')\n",
        "# flatten = Flatten(name='flattening')(concat)\n",
        "# dense = Dense(10, activation='relu', name='hidden_layer')(flatten)\n",
        "# output = Dense(1, activation='sigmoid', name='output_layer')(dense)\n",
        "\n",
        "# model_a = tf.keras.Model(inputs=input, outputs=output, name='Baseline_Model')\n",
        "# print(model_a.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I84NL6v_Sxg"
      },
      "source": [
        "logdir=\"/content/gdrive/My Drive/large_CNN_imp_a/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUIl3hl-6Cb-"
      },
      "source": [
        "# model_a.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model_a.fit(train_text, train_sentiment, epochs=10, callbacks=[tensorboard_callback])\n",
        "# model_a.save(\"/content/gdrive/My Drive/large_CNN_imp_a/model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfpnycR8AkOI"
      },
      "source": [
        "# %tensorboard --logdir \"/content/gdrive/My Drive/large_CNN_imp_a/logs/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMVqtSLK6VCR",
        "outputId": "a4d52337-3913-44ab-dd41-5e85ccf68316"
      },
      "source": [
        "model_loaded_a = tf.keras.models.load_model('/content/gdrive/My Drive/large_CNN_imp_a/model')\n",
        "print(model_loaded_a.summary())\n",
        "loss, accuracy = model_loaded_a.evaluate(test_text, test_sentiment)\n",
        "print(\"Accuracy: \" + str(accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Baseline_Model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 80)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_layer (Embedding)    (None, 80, 100)      2141300     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " convolutional_layer_8 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer[0][0]']        \n",
            "                                                                                                  \n",
            " convolutional_layer_4 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer[0][0]']        \n",
            "                                                                                                  \n",
            " convolutional_layer_2 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer[0][0]']        \n",
            "                                                                                                  \n",
            " pooling_8 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_8[0][0]']  \n",
            "                                                                                                  \n",
            " pooling_4 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_4[0][0]']  \n",
            "                                                                                                  \n",
            " pooling_2 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_2[0][0]']  \n",
            "                                                                                                  \n",
            " concatenation (Concatenate)    (None, 1, 96)        0           ['pooling_8[0][0]',              \n",
            "                                                                  'pooling_4[0][0]',              \n",
            "                                                                  'pooling_2[0][0]']              \n",
            "                                                                                                  \n",
            " flattening (Flatten)           (None, 96)           0           ['concatenation[0][0]']          \n",
            "                                                                                                  \n",
            " hidden_layer (Dense)           (None, 10)           970         ['flattening[0][0]']             \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 1)            11          ['hidden_layer[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,219,177\n",
            "Trainable params: 2,219,177\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 1.1166 - accuracy: 0.8366\n",
            "Accuracy: 83.65833163261414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRS31tx205zw"
      },
      "source": [
        "### Add year to data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CsyWS-K0y2t"
      },
      "source": [
        "for i, x in train_data.iterrows(): \n",
        "  training_string = str(x['datetimeReviewTime'].year) + ' ' +  x['reviewText']\n",
        "  train_data.loc[i, 'text_with_year'] = training_string \n",
        "\n",
        "for i, x in test_data.iterrows(): \n",
        "  training_string = str(x['datetimeReviewTime'].year) + ' ' +  x['reviewText']\n",
        "  test_data.loc[i, 'text_with_year'] = training_string \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgZhdShg4T85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1498f97-b4eb-4218-bcd3-82e14ad7badb"
      },
      "source": [
        "print(train_data.head()[['reviewText', 'text_with_year']])\n",
        "print(test_data.head()[['reviewText', 'text_with_year']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          reviewText                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text_with_year\n",
            "926385                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Tooooo leathery! Could barely chew!!!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         2018 Tooooo leathery! Could barely chew!!!\n",
            "331118                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Amazing!  We love it!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         2015 Amazing!  We love it!\n",
            "729949                                                                                                                                                                                                                                                                          I was like many of you probably looking through the reviews to see if this was a good cheaper option. I saw that the majority of people said that the cable easily snapped and I was so excited to get a pedal that I ignored their warning and bought it anyway. I figured that I if I didn't play hard, I would't break it....wrong.... the wire came completely unspooled and that was it. there is no fixing this thing. luckily mine was still under warranty and I'm sending it back. so don't waste your money on this thing.                                                                                                                                                                                                                                                                          2014 I was like many of you probably looking through the reviews to see if this was a good cheaper option. I saw that the majority of people said that the cable easily snapped and I was so excited to get a pedal that I ignored their warning and bought it anyway. I figured that I if I didn't play hard, I would't break it....wrong.... the wire came completely unspooled and that was it. there is no fixing this thing. luckily mine was still under warranty and I'm sending it back. so don't waste your money on this thing.\n",
            "273234  My husband gifted me my kindle as a Christmas gift, I was unsure being devoted to paper......What can I say I very quickly became devoted to my kindle, due to having all my books with me at all times {I do re-read when the writing is worth it} and the lightness compared to some of the heavier tomes I sometime read.\\nPerfect....until I dropped it and it refused to switch on....I contacted amazon.com to see if they could help me and Oh My the service I received was fantastic. Within 3 days they sent me a this new kindle that I have been enjoying ever since. Nowadays customer service is generally grudging and mandatory at best so to get pleasant, friendly, patient Human advice and assistance is a refreshing change and to be aspired to by other services. Many thanks Amazon.  2013 My husband gifted me my kindle as a Christmas gift, I was unsure being devoted to paper......What can I say I very quickly became devoted to my kindle, due to having all my books with me at all times {I do re-read when the writing is worth it} and the lightness compared to some of the heavier tomes I sometime read.\\nPerfect....until I dropped it and it refused to switch on....I contacted amazon.com to see if they could help me and Oh My the service I received was fantastic. Within 3 days they sent me a this new kindle that I have been enjoying ever since. Nowadays customer service is generally grudging and mandatory at best so to get pleasant, friendly, patient Human advice and assistance is a refreshing change and to be aspired to by other services. Many thanks Amazon.\n",
            "630708                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Didn't work for me, wasn't what I thought it was used for. So I ordered the right adapter for my purposes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    2014 Didn't work for me, wasn't what I thought it was used for. So I ordered the right adapter for my purposes.\n",
            "                                                                                                                                                                                                                                                                                                         reviewText                                                                                                                                                                                                                                                                                                    text_with_year\n",
            "39283                                                                                                                    I must be doing something wrong with this aweful pink wax!  It doesn't harden at all to stick hair to strip.  I pull the strip off and am left with a gunky mess on my face!  Please HELP!                                                                                                                   2013 I must be doing something wrong with this aweful pink wax!  It doesn't harden at all to stick hair to strip.  I pull the strip off and am left with a gunky mess on my face!  Please HELP!\n",
            "418936                                                                                                                               Fantastic connectivity and a wide breadth of content. UI is a little stilted but responsive.  Voice activation is responsive, as well.  It is very easy to set up and operate.                                                                                                                               2017 Fantastic connectivity and a wide breadth of content. UI is a little stilted but responsive.  Voice activation is responsive, as well.  It is very easy to set up and operate.\n",
            "693153                                                                                                                                                                                                                          Bought 2. One is fine, the other has a funky xlr socket. Otherwise, they work fine.                                                                                                                                                                                                                          2015 Bought 2. One is fine, the other has a funky xlr socket. Otherwise, they work fine.\n",
            "35526                                                                                                                   Have been using a different cream and thought I would give this a chance because of price per ounce.  Seems a little heavy and not as rich as I had hoped.  Will go back to my other cream.                                                                                                                  2013 Have been using a different cream and thought I would give this a chance because of price per ounce.  Seems a little heavy and not as rich as I had hoped.  Will go back to my other cream.\n",
            "437089  So glad I decided to finally order one of these. We have smart TVs but they don't let you do anywhere near what the Firestick does. We haven't had satellite in 2 years so everyone in the house is excited to be able to watch our shows and movies again without having to wait for Netflix to post them.  2018 So glad I decided to finally order one of these. We have smart TVs but they don't let you do anywhere near what the Firestick does. We haven't had satellite in 2 years so everyone in the house is excited to be able to watch our shows and movies again without having to wait for Netflix to post them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44hYkrjA1A9A",
        "outputId": "7ab9ad5b-a86a-42d5-9ea5-b066690ee022"
      },
      "source": [
        "all_train_text_year = train_data['text_with_year']\n",
        "\n",
        "vocab_year = Counter()\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "clean_text_year = []\n",
        "for x in all_train_text_year:\n",
        "  #unigrams\n",
        "  x = x.lower()\n",
        "  tokens = x.split()\n",
        "\n",
        "  #remove punctuation\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "\n",
        "  #remove stopwords\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  vocab_year.update(tokens)\n",
        "  clean_text_year.append(tokens)\n",
        "  \n",
        "print(len(vocab_year))\n",
        "print(vocab_year.most_common(10))\n",
        "\n",
        "min_occurance=2\n",
        "tokens_year = [k for k,c in vocab_year.items() if c >= min_occurance]\n",
        "print(len(tokens_year))\n",
        "print(tokens_year[:10])\n",
        "vocab_year = set(tokens_year)\n",
        "\n",
        "\n",
        "\n",
        "filtered_text_year = []\n",
        "for text in clean_text_year:\n",
        "  text = [w for w in text if w in vocab_year]\n",
        "  filtered_text_year.append(text)\n",
        "\n",
        "\n",
        "\n",
        "tokenizer_year = Tokenizer()\n",
        "tokenizer_year.fit_on_texts(filtered_text_year)\n",
        "encoded_text_year = tokenizer_year.texts_to_sequences(filtered_text_year)\n",
        "\n",
        "train_text_year = pad_sequences(encoded_text_year, maxlen=max_len, padding='post')\n",
        "\n",
        "vocab_size_year = len(tokenizer_year.word_index) + 1\n",
        "print(vocab_size_year)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45229\n",
            "[('one', 10622), ('like', 10081), ('great', 10058), ('use', 9371), ('', 9220), ('good', 8358), ('would', 8312), ('2014', 8157), ('2017', 8104), ('2015', 8057)]\n",
            "21412\n",
            "['2018', 'tooooo', 'leathery', 'could', 'barely', 'chew', '2015', 'amazing', 'love', '2014']\n",
            "21413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP85v0YS5mcp"
      },
      "source": [
        "all_test_text_year = test_data['text_with_year']\n",
        "filtered_text_test_year = []\n",
        "for x in all_test_text_year:\n",
        "  #unigrams\n",
        "  x = x.lower()\n",
        "  tokens = x.split()\n",
        "\n",
        "  #remove punctuation\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "\n",
        "  #remove stopwords\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "  #filter\n",
        "  filtered_tokens = [w for w in tokens if w in vocab]\n",
        "  \n",
        "  filtered_text_test_year.append(filtered_tokens)\n",
        "\n",
        "\n",
        "encoded_text_test_year = tokenizer_year.texts_to_sequences(filtered_text_test_year)\n",
        "test_text_year = pad_sequences(encoded_text_test_year, maxlen=max_len, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBW35ziEhcES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6918e01e-601b-451b-828a-ed2edfc19cd3"
      },
      "source": [
        "# input = tf.keras.layers.Input(shape=(max_len,))\n",
        "# embedding = Embedding(vocab_size_year, 100, input_length=max_len, name='embedding_layer')(input)\n",
        "# conv_size_8 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_8', padding='same')(embedding)\n",
        "# conv_size_4 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_4', padding='same')(embedding)\n",
        "# conv_size_2 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_2', padding='same')(embedding)\n",
        "# pooling_8 = MaxPooling1D(pool_size=max_len, name='pooling_8')(conv_size_8)\n",
        "# pooling_4 = MaxPooling1D(pool_size=max_len, name='pooling_4')(conv_size_4)\n",
        "# pooling_2 = MaxPooling1D(pool_size=max_len, name='pooling_2')(conv_size_2)\n",
        "# concat = tf.keras.layers.concatenate([pooling_8, pooling_4, pooling_2], axis=2, name='concatenation')\n",
        "# flatten = Flatten(name='flattening')(concat)\n",
        "# dense = Dense(10, activation='relu', name='hidden_layer')(flatten)\n",
        "# output = Dense(1, activation='sigmoid', name='output_layer')(dense)\n",
        "\n",
        "# model_year_a = tf.keras.Model(inputs=input, outputs=output)\n",
        "# print(model_year_a.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 80)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_layer (Embedding)    (None, 80, 100)      2141300     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " convolutional_layer_8 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer[0][0]']        \n",
            "                                                                                                  \n",
            " convolutional_layer_4 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer[0][0]']        \n",
            "                                                                                                  \n",
            " convolutional_layer_2 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer[0][0]']        \n",
            "                                                                                                  \n",
            " pooling_8 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_8[0][0]']  \n",
            "                                                                                                  \n",
            " pooling_4 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_4[0][0]']  \n",
            "                                                                                                  \n",
            " pooling_2 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_2[0][0]']  \n",
            "                                                                                                  \n",
            " concatenation (Concatenate)    (None, 1, 96)        0           ['pooling_8[0][0]',              \n",
            "                                                                  'pooling_4[0][0]',              \n",
            "                                                                  'pooling_2[0][0]']              \n",
            "                                                                                                  \n",
            " flattening (Flatten)           (None, 96)           0           ['concatenation[0][0]']          \n",
            "                                                                                                  \n",
            " hidden_layer (Dense)           (None, 10)           970         ['flattening[0][0]']             \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 1)            11          ['hidden_layer[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,219,177\n",
            "Trainable params: 2,219,177\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZLcL10fDHFO"
      },
      "source": [
        "logdir=\"/content/gdrive/My Drive/large_CNN_imp_year_a/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFcasKO9tw3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc003928-1529-47c1-c629-ba7e4e618cf8"
      },
      "source": [
        "# model_year_a.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model_year_a.fit(train_text_year, train_sentiment, epochs=10, callbacks=[tensorboard_callback])\n",
        "# model_year_a.save(\"/content/gdrive/My Drive/large_CNN_imp_year_a/model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 118s 78ms/step - loss: 0.3569 - accuracy: 0.8415\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 119s 79ms/step - loss: 0.2216 - accuracy: 0.9131\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 117s 78ms/step - loss: 0.1125 - accuracy: 0.9606\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 119s 79ms/step - loss: 0.0596 - accuracy: 0.9791\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 121s 80ms/step - loss: 0.0415 - accuracy: 0.9847\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 120s 80ms/step - loss: 0.0361 - accuracy: 0.9863\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 118s 79ms/step - loss: 0.0340 - accuracy: 0.9869\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 121s 81ms/step - loss: 0.0324 - accuracy: 0.9875\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 122s 82ms/step - loss: 0.0305 - accuracy: 0.9879\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 123s 82ms/step - loss: 0.0269 - accuracy: 0.9893\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/large_CNN_imp_year_a/model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNvkGggWDREl"
      },
      "source": [
        "# %tensorboard --logdir \"/content/gdrive/My Drive/large_CNN_imp_year_a/logs/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3Nk2J1rtzZ_",
        "outputId": "16cff5de-d23a-48cd-9308-d5f3ac0e71f9"
      },
      "source": [
        "model_loaded_year_a = tf.keras.models.load_model('/content/gdrive/My Drive/large_CNN_imp_year_a/model')\n",
        "print(model_loaded_year_a.summary())\n",
        "loss, accuracy = model_loaded_year_a.evaluate(test_text_year, test_sentiment)\n",
        "print(\"Accuracy: \" + str(accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 80)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_layer (Embedding)    (None, 80, 100)      2141300     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " convolutional_layer_8 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer[0][0]']        \n",
            "                                                                                                  \n",
            " convolutional_layer_4 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer[0][0]']        \n",
            "                                                                                                  \n",
            " convolutional_layer_2 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer[0][0]']        \n",
            "                                                                                                  \n",
            " pooling_8 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_8[0][0]']  \n",
            "                                                                                                  \n",
            " pooling_4 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_4[0][0]']  \n",
            "                                                                                                  \n",
            " pooling_2 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_2[0][0]']  \n",
            "                                                                                                  \n",
            " concatenation (Concatenate)    (None, 1, 96)        0           ['pooling_8[0][0]',              \n",
            "                                                                  'pooling_4[0][0]',              \n",
            "                                                                  'pooling_2[0][0]']              \n",
            "                                                                                                  \n",
            " flattening (Flatten)           (None, 96)           0           ['concatenation[0][0]']          \n",
            "                                                                                                  \n",
            " hidden_layer (Dense)           (None, 10)           970         ['flattening[0][0]']             \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 1)            11          ['hidden_layer[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,219,177\n",
            "Trainable params: 2,219,177\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 1.0755 - accuracy: 0.8378\n",
            "Accuracy: 83.77500176429749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D-KB3CsPa8p"
      },
      "source": [
        "year_dict = {'2013': 0, '2014': 1, '2015': 2, '2016': 3, '2017': 4, '2018': 5}\n",
        "\n",
        "\n",
        "for i, x in train_data.iterrows():\n",
        "  train_data.loc[i, 'year'] = str(x['datetimeReviewTime'].year)\n",
        "\n",
        "\n",
        "\n",
        "for i, x in test_data.iterrows():\n",
        "  test_data['year'] = str(x['datetimeReviewTime'].year)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVLIptQCmWCu"
      },
      "source": [
        "year_tokenizer = Tokenizer()\n",
        "year_tokenizer.fit_on_texts(['2013', '2014', '2015', '2016', '2017', '2018'])\n",
        "\n",
        "encoded_year = year_tokenizer.texts_to_sequences(train_data['year'])\n",
        "year_tokens = pad_sequences(encoded_year, maxlen=1)\n",
        "\n",
        "\n",
        "test_encoded_year = year_tokenizer.texts_to_sequences(test_data['year'])\n",
        "test_year_tokens = pad_sequences(test_encoded_year, maxlen=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RYTbvzHB1Rr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f675162-efe7-4ddf-e5e0-4b1e4ecf1458"
      },
      "source": [
        "# input_text = tf.keras.layers.Input(shape=(max_len,))\n",
        "# input_year = tf.keras.layers.Input(shape=(1,))\n",
        "# embedding_text = Embedding(vocab_size, 100, input_length=max_len, name='embedding_layer_text')(input_text)\n",
        "# embedding_year = Embedding(7, 96, name='embedding_layer_year')(input_year)\n",
        "# conv_size_8 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_8', padding='same')(embedding_text)\n",
        "# conv_size_4 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_4', padding='same')(embedding_text)\n",
        "# conv_size_2 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_2', padding='same')(embedding_text)\n",
        "# pooling_8 = MaxPooling1D(pool_size=max_len, name='pooling_8')(conv_size_8)\n",
        "# pooling_4 = MaxPooling1D(pool_size=max_len, name='pooling_4')(conv_size_4)\n",
        "# pooling_2 = MaxPooling1D(pool_size=max_len, name='pooling_2')(conv_size_2)\n",
        "# concat = tf.keras.layers.concatenate([pooling_8, pooling_4, pooling_2], axis=2, name='concatenation')\n",
        "# flatten_text = Flatten(name='flattening_text')(concat)\n",
        "# flatten_year = Flatten(name='flattening_year')(embedding_year)\n",
        "# concat_year = tf.keras.layers.concatenate([flatten_text, flatten_year], axis=1, name='join_year')\n",
        "# dense = Dense(10, activation='relu', name='hidden_layer')(concat_year)\n",
        "# output = Dense(1, activation='sigmoid', name='output_layer')(dense)\n",
        "\n",
        "# model_year_concat = tf.keras.Model(inputs=[input_text, input_year], outputs=output)\n",
        "# print(model_year_concat.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 80)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_layer_text (Embeddin  (None, 80, 100)     2141300     ['input_3[0][0]']                \n",
            " g)                                                                                               \n",
            "                                                                                                  \n",
            " convolutional_layer_8 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer_text[0][0]']   \n",
            "                                                                                                  \n",
            " convolutional_layer_4 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer_text[0][0]']   \n",
            "                                                                                                  \n",
            " convolutional_layer_2 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer_text[0][0]']   \n",
            "                                                                                                  \n",
            " pooling_8 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_8[0][0]']  \n",
            "                                                                                                  \n",
            " pooling_4 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_4[0][0]']  \n",
            "                                                                                                  \n",
            " pooling_2 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_2[0][0]']  \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenation (Concatenate)    (None, 1, 96)        0           ['pooling_8[0][0]',              \n",
            "                                                                  'pooling_4[0][0]',              \n",
            "                                                                  'pooling_2[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_layer_year (Embeddin  (None, 1, 96)       672         ['input_4[0][0]']                \n",
            " g)                                                                                               \n",
            "                                                                                                  \n",
            " flattening_text (Flatten)      (None, 96)           0           ['concatenation[0][0]']          \n",
            "                                                                                                  \n",
            " flattening_year (Flatten)      (None, 96)           0           ['embedding_layer_year[0][0]']   \n",
            "                                                                                                  \n",
            " join_year (Concatenate)        (None, 192)          0           ['flattening_text[0][0]',        \n",
            "                                                                  'flattening_year[0][0]']        \n",
            "                                                                                                  \n",
            " hidden_layer (Dense)           (None, 10)           1930        ['join_year[0][0]']              \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 1)            11          ['hidden_layer[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,220,809\n",
            "Trainable params: 2,220,809\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRyFcHFHEfOu"
      },
      "source": [
        "logdir=\"/content/gdrive/My Drive/large_CNN_imp_year_concat/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrImjnA-Qo8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37aee81c-020b-4392-d2a7-54d5b6988d48"
      },
      "source": [
        "# model_year_concat.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model_year_concat.fit([train_text, year_tokens], train_sentiment, epochs=10, callbacks=[tensorboard_callback])\n",
        "# model_year_concat.save(\"/content/gdrive/My Drive/large_CNN_imp_year_concat/model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 134s 89ms/step - loss: 0.3643 - accuracy: 0.8355\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 123s 82ms/step - loss: 0.2255 - accuracy: 0.9108\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 124s 83ms/step - loss: 0.1161 - accuracy: 0.9596\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 0.0599 - accuracy: 0.9790\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 125s 83ms/step - loss: 0.0423 - accuracy: 0.9846\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 120s 80ms/step - loss: 0.0352 - accuracy: 0.9869\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 119s 79ms/step - loss: 0.0335 - accuracy: 0.9868\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 118s 79ms/step - loss: 0.0330 - accuracy: 0.9869\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 119s 79ms/step - loss: 0.0291 - accuracy: 0.9891\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 118s 79ms/step - loss: 0.0270 - accuracy: 0.9893\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/large_CNN_imp_year_concat/model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba58WOEvElmx"
      },
      "source": [
        "# %tensorboard --logdir \"/content/gdrive/My Drive/large_CNN_imp_year_concat/logs/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbleEKFvqU5B",
        "outputId": "31bfe29a-63fa-4015-b62e-070c38cf40b9"
      },
      "source": [
        "model_loaded_year_concat = tf.keras.models.load_model('/content/gdrive/My Drive/large_CNN_imp_year_concat/model')\n",
        "loss, accuracy = model_loaded_year_concat.evaluate([test_text, test_year_tokens] , test_sentiment)\n",
        "print(\"Accuracy: \" + str(accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375/375 [==============================] - 5s 14ms/step - loss: 1.1180 - accuracy: 0.8335\n",
            "Accuracy: 83.35000276565552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_JN9W1sz2uD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2be7cdb-5c17-4f66-d91f-4583aeffb536"
      },
      "source": [
        "# input_text = tf.keras.layers.Input(shape=(max_len,))\n",
        "# input_year = tf.keras.layers.Input(shape=(1,))\n",
        "# embedding_text = Embedding(vocab_size, 100, input_length=max_len, name='embedding_layer_text')(input_text)\n",
        "# embedding_year = Embedding(7, 96, name='embedding_layer_year')(input_year)\n",
        "# conv_size_8 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_8', padding='same')(embedding_text)\n",
        "# conv_size_4 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_4', padding='same')(embedding_text)\n",
        "# conv_size_2 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_2', padding='same')(embedding_text)\n",
        "# pooling_8 = MaxPooling1D(pool_size=max_len, name='pooling_8')(conv_size_8)\n",
        "# pooling_4 = MaxPooling1D(pool_size=max_len, name='pooling_4')(conv_size_4)\n",
        "# pooling_2 = MaxPooling1D(pool_size=max_len, name='pooling_2')(conv_size_2)\n",
        "# concat = tf.keras.layers.concatenate([pooling_8, pooling_4, pooling_2], axis=2, name='concatenation')\n",
        "# flatten_text = Flatten(name='flattening_text')(concat)\n",
        "# flatten_year = Flatten(name='flattening_year')(embedding_year)\n",
        "# sum_year = tf.keras.layers.Add(name=\"summation_layer\")([flatten_text, flatten_year])\n",
        "# dense = Dense(10, activation='relu', name='hidden_layer')(sum_year)\n",
        "# output = Dense(1, activation='sigmoid', name='output_layer')(dense)\n",
        "\n",
        "# model_year_sum = tf.keras.Model(inputs=[input_text, input_year], outputs=output)\n",
        "# print(model_year_sum.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 80)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_layer_text (Embeddin  (None, 80, 100)     2141300     ['input_5[0][0]']                \n",
            " g)                                                                                               \n",
            "                                                                                                  \n",
            " convolutional_layer_8 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer_text[0][0]']   \n",
            "                                                                                                  \n",
            " convolutional_layer_4 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer_text[0][0]']   \n",
            "                                                                                                  \n",
            " convolutional_layer_2 (Conv1D)  (None, 80, 32)      25632       ['embedding_layer_text[0][0]']   \n",
            "                                                                                                  \n",
            " pooling_8 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_8[0][0]']  \n",
            "                                                                                                  \n",
            " pooling_4 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_4[0][0]']  \n",
            "                                                                                                  \n",
            " pooling_2 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_2[0][0]']  \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenation (Concatenate)    (None, 1, 96)        0           ['pooling_8[0][0]',              \n",
            "                                                                  'pooling_4[0][0]',              \n",
            "                                                                  'pooling_2[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_layer_year (Embeddin  (None, 1, 96)       672         ['input_6[0][0]']                \n",
            " g)                                                                                               \n",
            "                                                                                                  \n",
            " flattening_text (Flatten)      (None, 96)           0           ['concatenation[0][0]']          \n",
            "                                                                                                  \n",
            " flattening_year (Flatten)      (None, 96)           0           ['embedding_layer_year[0][0]']   \n",
            "                                                                                                  \n",
            " summation_layer (Add)          (None, 96)           0           ['flattening_text[0][0]',        \n",
            "                                                                  'flattening_year[0][0]']        \n",
            "                                                                                                  \n",
            " hidden_layer (Dense)           (None, 10)           970         ['summation_layer[0][0]']        \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 1)            11          ['hidden_layer[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,219,849\n",
            "Trainable params: 2,219,849\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3s38W7jGOoX"
      },
      "source": [
        "logdir=\"/content/gdrive/My Drive/large_CNN_imp_year_sum/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12GhO9_k07yO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd4f432-8b2d-4fa0-da57-0cad3f9eb4a6"
      },
      "source": [
        "# model_year_sum.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model_year_sum.fit([train_text, year_tokens], train_sentiment, epochs=10, callbacks=[tensorboard_callback])\n",
        "# model_year_sum.save(\"/content/gdrive/My Drive/large_CNN_imp_year_sum/model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 121s 80ms/step - loss: 0.3646 - accuracy: 0.8385\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 123s 82ms/step - loss: 0.2286 - accuracy: 0.9099\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 123s 82ms/step - loss: 0.1186 - accuracy: 0.9585\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 121s 81ms/step - loss: 0.0637 - accuracy: 0.9776\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 122s 81ms/step - loss: 0.0450 - accuracy: 0.9837\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 124s 83ms/step - loss: 0.0366 - accuracy: 0.9863\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 120s 80ms/step - loss: 0.0339 - accuracy: 0.9869\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 121s 81ms/step - loss: 0.0315 - accuracy: 0.9877\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 122s 82ms/step - loss: 0.0326 - accuracy: 0.9874\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 124s 83ms/step - loss: 0.0297 - accuracy: 0.9883\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/large_CNN_imp_year_sum/model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWMPmukxGVld"
      },
      "source": [
        "# %tensorboard --logdir \"/content/gdrive/My Drive/large_CNN_imp_year_sum/logs/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfnSIKr31kgx",
        "outputId": "199918a4-c36a-4668-9ef6-0dcef6023222"
      },
      "source": [
        "model_loaded_year_sum = tf.keras.models.load_model('/content/gdrive/My Drive/large_CNN_imp_year_sum/model')\n",
        "loss, accuracy = model_loaded_year_sum.evaluate([test_text, test_year_tokens] , test_sentiment)\n",
        "print(\"Accuracy: \" + str(accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375/375 [==============================] - 5s 13ms/step - loss: 1.0345 - accuracy: 0.8392\n",
            "Accuracy: 83.91666412353516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqUzKbWk9TYD"
      },
      "source": [
        "probabilities = model_loaded_a.predict(test_text)\n",
        "predictions = []\n",
        "for p in probabilities:\n",
        "  value = p[0]\n",
        "  if value >= 0.5:\n",
        "    predictions.append(1)\n",
        "  else:\n",
        "    predictions.append(0)\n",
        "\n",
        "probabilities_year = model_loaded_year_a.predict(test_text_year)\n",
        "predictions_year = []\n",
        "for p in probabilities_year:\n",
        "  value = p[0]\n",
        "  if value >= 0.5:\n",
        "    predictions_year.append(1)\n",
        "  else:\n",
        "    predictions_year.append(0)\n",
        "\n",
        "probabilities_year_concat = model_loaded_year_concat.predict([test_text, test_year_tokens])\n",
        "predictions_year_concat = []\n",
        "for p in probabilities_year_concat:\n",
        "  value = p[0]\n",
        "  if value >= 0.5:\n",
        "    predictions_year_concat.append(1)\n",
        "  else:\n",
        "    predictions_year_concat.append(0)\n",
        "\n",
        "probabilities_year_sum = model_loaded_year_concat.predict([test_text, test_year_tokens])\n",
        "predictions_year_sum = []\n",
        "for p in probabilities_year_sum:\n",
        "  value = p[0]\n",
        "  if value >= 0.5:\n",
        "    predictions_year_sum.append(1)\n",
        "  else:\n",
        "    predictions_year_sum.append(0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxgFfRs4GbUq"
      },
      "source": [
        "sentiment_list = test_sentiment.to_list()\n",
        "data_year = test_data['text_with_year'].to_list()\n",
        "rating = test_data['overall'].to_list()\n",
        "\n",
        "\n",
        "correct = {'2013': 0, '2014': 0, '2015': 0, '2016': 0, '2017': 0, '2018': 0}\n",
        "total = {'2013': 0, '2014': 0, '2015': 0, '2016': 0, '2017': 0, '2018': 0}\n",
        "\n",
        "correct_year = {'2013': 0, '2014': 0, '2015': 0, '2016': 0, '2017': 0, '2018': 0}\n",
        "\n",
        "correct_year_concat = {'2013': 0, '2014': 0, '2015': 0, '2016': 0, '2017': 0, '2018': 0}\n",
        "\n",
        "correct_year_sum = {'2013': 0, '2014': 0, '2015': 0, '2016': 0, '2017': 0, '2018': 0}\n",
        "\n",
        "false_positive = []\n",
        "false_positive_year = []\n",
        "false_positive_concat = []\n",
        "false_positive_sum = []\n",
        "\n",
        "false_negative = []\n",
        "false_negative_year = []\n",
        "false_negative_concat = []\n",
        "false_negative_sum = []\n",
        "\n",
        "for i in range(len(sentiment_list)):\n",
        "  sent = sentiment_list[i]\n",
        "  year = data_year[i].split()[0]\n",
        "\n",
        "  if predictions[i] == sent: \n",
        "    correct[year] = correct[year] + 1\n",
        "  else:\n",
        "    if predictions[i] == 1:\n",
        "      false_positive.append((data_year[i], rating[i]))\n",
        "    else:\n",
        "      false_negative.append((data_year[i], rating[i]))\n",
        "  \n",
        "  if predictions_year[i] == sent: \n",
        "    correct_year[year] = correct_year[year] + 1\n",
        "  else:\n",
        "    if predictions_year[i] == 1:\n",
        "      false_positive_year.append((data_year[i], rating[i]))\n",
        "    else:\n",
        "      false_negative_year.append((data_year[i], rating[i]))\n",
        "  \n",
        "  if predictions_year_concat[i] == sent: \n",
        "    correct_year_concat[year] = correct_year_concat[year] + 1\n",
        "  else:\n",
        "    if predictions_year_concat[i] == 1:\n",
        "      false_positive_concat.append((data_year[i], rating[i]))\n",
        "    else:\n",
        "      false_negative_concat.append((data_year[i], rating[i]))\n",
        "\n",
        "  if predictions_year_sum[i] == sent: \n",
        "    correct_year_sum[year] = correct_year_sum[year] + 1\n",
        "  else:\n",
        "    if predictions_year_sum[i] == 1:\n",
        "      false_positive_sum.append((data_year[i], rating[i]))\n",
        "    else:\n",
        "      false_negative_sum.append((data_year[i], rating[i]))\n",
        "  \n",
        "  total[year] = total[year] + 1\n",
        "\n",
        "# print(correct)\n",
        "# print(correct_year)\n",
        "# print(correct_year_concat)\n",
        "# print(correct_year_sum)\n",
        "# print(total)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHQGkYJswOAN",
        "outputId": "f621ad94-fa64-467b-ece2-e613e922da84"
      },
      "source": [
        "prng = np.random.RandomState(123)\n",
        "\n",
        "print(\"False Positive\")\n",
        "\n",
        "for i in range(10):\n",
        "  index = prng.randint(0, len(false_positive))\n",
        "  print(false_positive[index][0] + \"|\" + str(false_positive[index][1]))\n",
        "\n",
        "print(\"False Negative\")\n",
        "\n",
        "for i in range(10):\n",
        "  index = prng.randint(0, len(false_negative))\n",
        "  print(false_negative[index][0] + \"|\" + str(false_negative[index][1]))\n",
        "\n",
        "\n",
        "\n",
        "print(\"--------------\")\n",
        "\n",
        "print(\"False Positive\")\n",
        "\n",
        "for i in range(10):\n",
        "  index = prng.randint(0, len(false_positive_year))\n",
        "  print(false_positive_year[index][0] + \"|\" + str(false_positive_year[index][1]))\n",
        "\n",
        "print(\"False Negative\")\n",
        "\n",
        "for i in range(10):\n",
        "  index = prng.randint(0, len(false_negative_year))\n",
        "  print(false_negative_year[index][0] + \"|\" + str(false_negative_year[index][1]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"--------------\")\n",
        "\n",
        "print(\"False Positive\")\n",
        "\n",
        "for i in range(10):\n",
        "  index = prng.randint(0, len(false_positive_concat))\n",
        "  print(false_positive_concat[index][0] + \"|\" + str(false_positive_concat[index][1]))\n",
        "\n",
        "print(\"False Negative\")\n",
        "\n",
        "for i in range(10):\n",
        "  index = prng.randint(0, len(false_negative_concat))\n",
        "  print(false_negative_concat[index][0] + \"|\" + str(false_negative_concat[index][1]))\n",
        "\n",
        "print(\"--------------\")\n",
        "\n",
        "print(\"False Positive\")\n",
        "\n",
        "for i in range(10):\n",
        "  index = prng.randint(0, len(false_positive_sum))\n",
        "  print(false_positive_sum[index][0] + \"|\" + str(false_positive_sum[index][1]))\n",
        "\n",
        "print(\"False Negative\")\n",
        "\n",
        "for i in range(10):\n",
        "  index = prng.randint(0, len(false_negative_sum))\n",
        "  print(false_negative_sum[index][0] + \"|\" + str(false_negative_sum[index][1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False Positive\n",
            "2018 Gave away as a gift. No thank you from our friends who we gave it to.|2.0\n",
            "2016 Much too little for a small child. My mistake for not checking the measurements.  The sound is lovely and is works very well, just nit for a toddler.|3.0\n",
            "2016 I've been using this daily, but so far have seen no difference in the appearance of my dark spot. If this changes by the time I finish the product, I will update my review.. hoping this is a matter of time and patience.|1.0\n",
            "2017 Dimensions of buy fraction of a millimeter|1.0\n",
            "2013 i bought this for a gift and the person was really happy. no complaints. good way to save some money too|3.0\n",
            "2017 Works great but needs to  be a foot longer|3.0\n",
            "2018 Not at all what I expected from the Braun name.  Causes Ingrown hairs and is not as efficient or easy to use|2.0\n",
            "2017 It is sturdy and ruggedly built but it does not have a height adjustment for the microphone, which is a huge drawback. In hindsight, I would have done some more careful research and purchased a more flexible stand.|3.0\n",
            "2015 I want to love it, but don't yet...|3.0\n",
            "2017 Nice color selection. but it's kind of thick which reduce the voice thru MIC....|3.0\n",
            "False Negative\n",
            "2013 Made a great gift. Even for it being used, its a perfect e-reader. Not a bad price and shipping was easy.|4.0\n",
            "2014 I think this product is working and I like it.|5.0\n",
            "2018 The quality seems to be well. Its as advertised. But there is to much slack on top. The top of my head is not as large as some people i suppose. So i have to pull it down and foldthe end up twice. But it works very well.|4.0\n",
            "2013 I really like this program. I sell cars/trucks for a living, and went to a meeting. They suggested this app. It works GREAT.|5.0\n",
            "2017 base on the price it matchesnot good not bad|4.0\n",
            "2017 This came with easy to follow directions.  I use this Amazon Fire TV way more than the basic channels I have that comes with the best Internet Service package I could purchase.  The only problem is that it is a little confusing.  For example, you can download so many apps, but you don't really have access to video/tv channels, etc. That is an entirely separate step that you have to take over and over again for each app and it more than likely requires additional membership and subscriptions.  I guess I was just hoping there would be a more interactive type of access to help with adding apps that actually make streaming video easier.  Have you ever thought about adding video links posted from Amazon Video that expedites this process?  Seriously, you could make so much more money that way.  People want things to be easier.  Yes, I can go to youtube and watch countless videos but are these people reputable?  what if these are pirated sites?  what if there is some sort of problem after downloading from these youtube videos?  For someone like me, who knows almost nothing about URLs and gateways and reputable streaming alternatives, obviously,  I would want some sort of assurance from my trusted Amazon company, right?  I know it would go a long way for me.  I for one would be more at peace with knowing that a tried and true company like Amazon had my back.|4.0\n",
            "2017 I love the fact that, because of the fit, it becomes part of the echo. Can't comment pro or con on it's charge life because I haven't had it that long but, I'm not unhappy with this purchase.|5.0\n",
            "2018 Great product for the price. Refurbished item was like new. Sound quality is somewhat limited compared to Echo, that is why I give 4 stars.|4.0\n",
            "2017 I tried the \"lick test\" and failed miserably (lick your wrist, let it dry 10 seconds, sniff it... if it stinks your breath stinks). This stuff solved the problem and works great! This used twice a day combined with Therabreath mouthwash has made it so I have fresh breath all day and I now pass the \"lick test\" even at the end of the day|5.0\n",
            "2014 I expected this computer top be fast, and it is.  I am not too wild about windows 8 graphical interface vs XP and Vista.  I was able to load Office 2007 small business.  I had to download a new printer driver for my OKI 5800 Ldn.|5.0\n",
            "--------------\n",
            "False Positive\n",
            "2013 Obviously, I love reading books on the Kindle Fire way better than reading them on my iPhone.  Battery life is okay, not great.  Picture is clear when watching streaming video.  I absolutely hate that there is no external volume control.  Super incovenient to have to go through the menu when I want to adjust the volume on certain apps.|3.0\n",
            "2015 None of the 5 star reviews can be believed because they all girvtgevhead fur a discount. Aka their reviews were bought.|1.0\n",
            "2013 I returned these rollers because they did not fit into the legs of my bed frames.  I would have thought these would be a universal fit.  I was wrong.|1.0\n",
            "2017 Not really what I expected|2.0\n",
            "2013 love the features.  works very well with my I pod.  good volume control. great for walks but too lose for heavy workouts.  micro chip mp3 player is great  but no shuffle.|3.0\n",
            "2017 looks good but the straps is not good at all. I have only been using this for a week and it already broke. I wish that I had read the comments before I bought the bag.|1.0\n",
            "2018 Can't seem to dial it in to my comfort zone|3.0\n",
            "2017 Good on medium spots|3.0\n",
            "2014 Seems like what I wanted to write music for several medieval instruments and different vocal parts.|3.0\n",
            "2013 I used this mic to record the vocals on my album \"My Girlfriend's Dead\".  It was the first album I ever recorded and produced on my own. I thought the nasal sound of my voice was simply the sound of my voice at the time. It wasn't. It was the mic. If you listen to people even just speaking over this mic, they sound nasal. The Shure SM7B does everything the RE20 does but sounds far better on vocals. Don't be fooled by the price difference into thinking that the RE20 is better. It most definitely is not.  I kept the RE20 for years and used it as a kick drum mic.|3.0\n",
            "False Negative\n",
            "2014 I replaced the busted stock head unit in my 05 F150 with this and the difference is night and day. The sound quality from this deck is superb, and the control you have with the Pro Eq is fantastic. I was able to dial it in to the point where you would swear I have a small sub in there somewhere, but it's just stock speakers in the short doors and some aftermarket Pioneers in the front, plus this head unit.\n",
            "\n",
            "I had lines run for an amp and sub when I had this installed and it sounds so good as is that I'm considering just leaving it alone. We shall see.\n",
            "\n",
            "The bluetooth works really well and I haven't had any real issues with it. It was easy to sync my phone up and it automatically links up every time I get in my truck. I put the mic on the top edge of the door trim just behind the hinge for the sun visor and it picks up my voice just fine. Calls coming through are clear enough and I haven't had any complaints from clients or friends about the call quality. The only annoying thing is that when there's road noise, the voice command feature can get a little iffy. There have been multiple times when I've ended up cursing at the top of my lungs at the phone because it comes up with some unintelligible jargon from me trying to find somebody's name. Half of that is probably my phone (GS3) though, cause it can't differentiate certain names when there's no background noise, so ymmv.|5.0\n",
            "2015 i like it because it stains and has another shade of pink u can't really see the designs on it|4.0\n",
            "2017 These work really well. They are not big and bulky, they fit tight but comfortable as they should and do not feel cheap or that they will easily fray and disintegrate.|4.0\n",
            "2015 Valley of Fear proves that Conan Doyle has the talent to write both short stories and longer pieces. Of all the Sherlock Holmes stories, this is my favorite!|5.0\n",
            "2016 Good but it is a little awkward in my hand.|4.0\n",
            "2013 I've had my first smart phone since July, the Motorola Droid Razr Maxx.  After deleting all the NFL apps and other useless things, I set out looking for a simple metronome, tuning fork, flashlight, and other functional, simple, small, free, and non-net-necessary apps.\n",
            "\n",
            "  The Note Calculator & Tuning Fork is exactly that.  The note is clear and rings continuously, allowing me to tune my guitar, mandolin, or ukulele, or anything else if I don't have my physical tuning fork (I keep one in every instrument case, but this app helps if I'm playing someone else's instrument).  It has also helped me to figure out what key a song or piece of music is in (for instance, while listening to the radio in the car).\n",
            "\n",
            "  One major criticism is the way in which the pitch and frequency is selected.  The slide bar works, but it is an incredible frustration to land the indicator exactly on the spot you want (especially the frequency).  The formula being shown is neat, but I don't even know how high or low this thing goes because I don't dare take it off of A 440.  Advice for an update:  add + and - buttons for each slider to fine-tune it.  \"Simple Metronome\" is another good app that does exactly that.|4.0\n",
            "2015 O don't know how I would act with out kindlr.|5.0\n",
            "2014 What you see is what you get. nothing special but this is a great price and fits perfectly on my motherboard.|5.0\n",
            "2015 If you do your nails at home these are great for anyone trying to keep remover on with the tin foil caps.  They just don't work if you have to use your hands.  These keep the cotton ball in place and you can leave them on and do what you need to without the tin foil falling off.  Great idea.|5.0\n",
            "2016 excellent multipurpose bag|5.0\n",
            "--------------\n",
            "False Positive\n",
            "2013 test all Monorprice cables with a pin to pin tester. I have had issues with wiring problems causing phasing issues.\n",
            "\n",
            "Monoprice knows about the problem for a couple years now but they have no current solution to fix it.|1.0\n",
            "2013 While I love Sriracha the packaging job was horrible and all 3 bottles exploded. Take warning when ordering. Oh and did I mention it covered everything else in my box... It was just lovely.|2.0\n",
            "2017 Works great but needs to  be a foot longer|3.0\n",
            "2017 I love this Chip,  tasty as always.  Ordered two, but one Chip is smashed and opened.|3.0\n",
            "2015 They arrived earlier than I expected, they are nice and very light easy to put on. The flowers are not as defined as the image but still are nice for casual wear. I like them, overall good buy|3.0\n",
            "2013 My new Kindle Fire works great and is a great product.  However, it is not a \"stand alone\" product.  You have to buy books.  I believe the books are, for the most part, over priced.  For example, \"Gone Girl: A Novel\" sells on Kindle for $12.99.  That compares well with list price of $25.00.  BUT, Amazon sells hard cover (free two day shipping with Prime memberships, which I have) for $13.75.  That is less than $1.00 difference.  Honestly, come on.  It is an electronic version that flies through the air, no printing costs and no shipping costs.  I realize there is some effort in preparing books for Kindle but new books should be less than $10, max, most should be $7.00 or $8.00.  Older books should be really cheap.  I was going to reread Gregory McDonald's \"Fletch\" series, but they are $9.99 each.  Really, the first Fletch book is about 35 years old.  Sure, they are great books but they should be at bargin bin prices.  There are exceptions.  Laurence Shames Key West series is available at $2.99 a book.  These are great books (you should read them if you haven't, fans of Carl Hiaasen will love them) at a fair price.  More books should follow suit.  I do have Amazon Prime which allows you a once a month borrow from the \"lending library.\"  Note, the books available to borrow are very limited.|3.0\n",
            "2017 tips with semi heavy mics, besides that its fantastic|3.0\n",
            "2016 I was very disappointed with the lack of actual comics. All I got were trivia questions and little phrases.|3.0\n",
            "2018 Gave away as a gift. No thank you from our friends who we gave it to.|2.0\n",
            "2016 I've been using a USB version of Logitech's headset for the past few years and it has worked perfectly. I use it for both Skype and Dragon Naturally Speaking. It can be tough to get a good microphone for Dragon. Sound quality is very good, and more importantly they are comfortable on your head. When the plastic snapped on the old set, I thought I would try this version, but this is not a USB version. I could not get the headset to connect to my system properly. I was able to get sound, but could not get the microphone working. The USB version was pretty much plug-and-play. So, I'm going back to my tried-and-true USB version of this headset, and suggest others do the same.|1.0\n",
            "False Negative\n",
            "2015 I love this as a base color.\n",
            "I thought I'd use it alone but it's very dark which is OK, it's supposed to be. But I'm not daring enough to use it that dark.\n",
            "Nonetheless, I love it. I LOVE IT.|4.0\n",
            "2015 Love the size.  Have not been able to use it enough to really get comfortable with it, but what I have been able to use, love it....  Something I forgot about though was the camera.  You can't see what you are taking pictures of unless you hold it to the side or you are taking a picture of yourself...|4.0\n",
            "2018 The quality seems to be well. Its as advertised. But there is to much slack on top. The top of my head is not as large as some people i suppose. So i have to pull it down and foldthe end up twice. But it works very well.|4.0\n",
            "2014 Its really good charger cable for your portable mp3 player.|5.0\n",
            "2013 The Crest Pro-Health line has several different flavors of mint. Fresh mint, smooth mint, clean mint and invigorating mint. How much mint do you actually need? It's refreshing to have a non-mint option. I like the wintergreen flavor. It's refreshing and a nice change from the usual mint.|5.0\n",
            "2013 The really nice part of this instrument is that, because the music card can be slid under the strings, you can play a song that is recognizable without trouble.  Just a little bit of timing and practice, you feel like a success.  One of my daughters got a keyboard, but that just seems like we are making noise. Rhythmical noise, but still just noise.  Turns out that the eight year old, was the best of playing the lap harp.  I would even bet that you can download additional music from the internet and print on your home computer for even more variety.|5.0\n",
            "2016 Not bad at all haven't really done much with it since my set up isn't all to great so hoping it turn out great|5.0\n",
            "2014 Good for Kids.|5.0\n",
            "2015 I've dabbled with Android (both stock and custom ROMs), WebOs (HP TouchPad), and Apple (parents and in laws) tablets, but I never used any Fire OS tablets.  Having read reviews and comparisons, as well as some videos of the updated OS, I took the plunge.\n",
            "\n",
            "The compact yet lengthy box opens to reveal a remarkably thin and incredibly light tablet.  Any case weighs almost as much as the tablet, and I cannot imagine anything lighter.  The build quality is good, though not exceptional, but it never feels cheep.  In use it's responsive, keeps going all day, and never feels warm.  The app selection is great (pretty much any non-Google Android app), and the OS makes finding your apps and media a breeze.\n",
            "\n",
            "As an Android user, I don't find myself missing much (no Google now is a bit of a bummer).  I love the size and aspect ratio, as a 10\" 16x9 device really isn't too practical.  I never really liked iOS, but I could see my parents loving this.|5.0\n",
            "2017 Pretty good player. There is a random delay from sleep sometimes which is rather annoying|4.0\n",
            "--------------\n",
            "False Positive\n",
            "2013 test all Monorprice cables with a pin to pin tester. I have had issues with wiring problems causing phasing issues.\n",
            "\n",
            "Monoprice knows about the problem for a couple years now but they have no current solution to fix it.|1.0\n",
            "2016 I wanted a temp/humidity monitor that had a high/low reading. Pleased with that. Works fine. My issue is with the humidity reading. I lined up two other hygrometers next to it and the AcuRite was 10% below the other two. It fluctuated along with the rest, but was constantly 10% below the proper reading. The temp reading are spot on though. Guess you get what you pay for.|3.0\n",
            "2017 Dimensions of buy fraction of a millimeter|1.0\n",
            "2015 It worked.|3.0\n",
            "2015 So it kinda prevents pops... But not as good as I had hoped. I live in Ohio and shipping took 8 days to get here with free shipping... Easy set up, wiggles a little.|3.0\n",
            "2016 I wish iI had more battery power that would last more than 8 hours when I have the wii on.\n",
            "asi is I only get about 3 hours when I have it on!!!!|3.0\n",
            "2014 I have had Maschine Studio for almost a year.. and while the controller is very nice, the software is severely lacking and cannot be recommended at all.\n",
            "\n",
            "The image NI is selling is that Maschine is easier to use than a DAW.. well, its just not.\n",
            "\n",
            "If you have hardware gear,\n",
            "A. Forget about Maschine if you have Windows.. only one midi out port can be used.. as in, only one of the midi out ports in the options can be enabled.. if you attempt to say use usb midi and one of the (3) DIN ports, everything hangs up after 10-15mins... this is a bug that has been in Maschine for over two years.\n",
            "B. You cannot use the \"midi learn\", \"freely assignable\" 8 macro knobs to do anything at all on external gear...want to adjust cutoff on that cool beat on your rack synth?.. forget it.\n",
            "\n",
            "If you have ITB setup..\n",
            "A. Forget about multi-timbral VST.. as soon as you view your second channel your VST disappears!.. there is a workaround with routing that you can hear a second sound while playing with the VST, but you cannot adjust anything while you record as you lose the macro knobs on pads 2-16.. so, recording or playing knob twiddles live is a non-starter.\n",
            "B. Be warned that maschine pretty much plays well with NI VST only... some other lose their presets, lose their setups, lose functionality.\n",
            "\n",
            "Maschine also.\n",
            "A. has zero internal midi routing, so you cant insert a midi VST into a channel.. , and cant use midi out of a VST to drive another one.\n",
            "B. Does not accept tempo changes in a song.\n",
            "C. Cannot record a performance of solo's mute's pattern changes etc.\n",
            "D. Cannot arm individual pad/tracks for recording\n",
            "E. Cannot turn off midi in on any pad/channel (\"OFF\" is not OFF, its \"OMNI\")..so recording can be a crap shoot of unwanted data spewing around across groups.\n",
            "F. If you don't have Komplete,\n",
            "\n",
            "So, buy this if..\n",
            "1. you are gonna be sample based, (hiphop type stuff).. its probably going to be ok,\n",
            "2. you are really just looking for a drum pattern sequencer.. for this its great.\n",
            "\n",
            "Don't buy if..\n",
            "1. You are looking for a capable sequencer.\n",
            "2. You want to integrate this with hardware synths\n",
            "3. You want a DAW and think this is easier... its not... buy Live, Ableton, Sonar or Logic.. They all can do far far more for less $$$\n",
            "\n",
            "but hey!.. it looks pretty and the glib videos are awesome.|1.0\n",
            "2014 This is a mid grade guitar string. If you are looking for an inexpensive string that works, then you found it. It is not the best just to make it clear. I experience vibration in the 2nd and 5th string. Comparing this to Martin Acoustic SP Light, Martin is better. I hope this review help others looking for an inexpensive guitar string.|3.0\n",
            "2018 Got it on sale.  Easy setup but just ok on video quality.|3.0\n",
            "2016 Worked fine for about a year, then the voice related functions stopped working.|3.0\n",
            "False Negative\n",
            "2015 funny accent to my teen daughter's room. She has painted the nails and put her rings on it.|5.0\n",
            "2013 It work good and I was having issues with the dimming of my lights and my alternator and battery was taking a beaten  I went threw 2 alternators and one battery but ever since I put the capacitor on it was dimming a lot less but as it charged more and more the dimming stopped and my bass was hitting harder and crispier so I say it's a good deal especially for the price|4.0\n",
            "2018 Dot works well but sound quality is not up to full echo 2nd edition|4.0\n",
            "2018 Works great but it broke in half after a few uses|4.0\n",
            "2015 Worked with my Galaxy Tab 10|4.0\n",
            "2013 Had a bar like this for several years. Didn't know where I got it, so ...looked around, found this...which looked similar. It was the same soap! This is a hard-edged TOUGH bar of soap that gets rid of EVERYTHING! Cuts through grease like magic, has a nice, strong scent, and the grit scrubs off the most stubborn dirt/grime from the garden. Careful...it'll dry out your skin if you over use it, but it's perfect if you're looking to get out soil and/or oils from outdoor work. Doesn't take much to get it going, either. A rub or two gets you enough soap to lather up AND grit off whatever you're trying to manage. HIGHLY recommend this.|5.0\n",
            "2014 I received a free product for the purpose of my review. Opinions are 100% my own.\n",
            "\n",
            "Crest Sensi-Stop Strips are fast acting and completely amazing! I used them in addition to my sensitive tooth & gums toothpaste.\n",
            "\n",
            "I followed the directions and left the strip on the affected area for 10 minutes and I was very very pleased with the results! I was able to go about my day and not have the temperature of my beverages or food be bothersome. Once I find these in local stores, I'm most assuredly making a purchase! I don't have anything negative to say about Crest Sensi-Stop Strips!|5.0\n",
            "2013 The only thing I don't like is the coiled tube as it doesn't seem to want to straighten out easily for use and the lid (which houses the tips) is always falling into the reservoir when I try to put it back on after filling it.  Otherwise, it's a great product!|4.0\n",
            "2018 it's super heavy and basically straight-up vaseline/petroleum jelly but it does absorb better than that. I put it on before I sleep when I don't have to touch anything because it does take a while to absorb.|5.0\n",
            "2016 This really worked after my lashes broke.|5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "1eY338VFyga2",
        "outputId": "a207adb0-ec52-4806-f051-38c17610be45"
      },
      "source": [
        "fig, axes = plt.subplots(4,2)\n",
        "fig.suptitle(\"Actual Ratings\")\n",
        "fig.subplots_adjust(hspace=0.7)\n",
        "\n",
        "false_positive_values = []\n",
        "for x in false_positive:\n",
        "  false_positive_values.append(x[1])\n",
        "\n",
        "\n",
        "axes[0, 0].hist(false_positive_values, bins=[0.5,1.5,2.5,3.5], rwidth=0.5)\n",
        "axes[0,0].set_title(\"Baseline False Positive\")\n",
        "\n",
        "\n",
        "false_negative_values = []\n",
        "for x in false_negative:\n",
        "  false_negative_values.append(x[1])\n",
        "\n",
        "axes[0, 1].hist(false_negative_values, bins=[3.5,4.5,5.5], rwidth=0.5)\n",
        "axes[0,1].set_title(\"Baseline False Negative\")\n",
        "\n",
        "false_positive_values_year = []\n",
        "for x in false_positive_year:\n",
        "  false_positive_values_year.append(x[1])\n",
        "\n",
        "axes[1, 0].hist(false_positive_values_year, bins=[0.5,1.5,2.5,3.5], rwidth=0.5)\n",
        "axes[1,0].set_title(\"W/ Year False Positive\")\n",
        "\n",
        "false_negative_values_year = []\n",
        "for x in false_negative_year:\n",
        "  false_negative_values_year.append(x[1])\n",
        "\n",
        "axes[1, 1].hist(false_negative_values_year, bins=[3.5,4.5,5.5], rwidth=0.5)\n",
        "axes[1,1].set_title(\"W/ Year False Negative\")\n",
        "\n",
        "\n",
        "false_positive_values_concat = []\n",
        "for x in false_positive_concat:\n",
        "  false_positive_values_concat.append(x[1])\n",
        "\n",
        "axes[2, 0].hist(false_positive_values_concat, bins=[0.5,1.5,2.5,3.5], rwidth=0.5)\n",
        "axes[2,0].set_title(\"Concatenation False Positive\")\n",
        "\n",
        "false_negative_values_concat = []\n",
        "for x in false_negative_concat:\n",
        "  false_negative_values_concat.append(x[1])\n",
        "\n",
        "axes[2, 1].hist(false_negative_values_concat, bins=[3.5,4.5,5.5], rwidth=0.5)\n",
        "axes[2,1].set_title(\"Concatenation False Negative\")\n",
        "\n",
        "false_positive_values_sum = []\n",
        "for x in false_positive_sum:\n",
        "  false_positive_values_sum.append(x[1])\n",
        "\n",
        "axes[3, 0].hist(false_positive_values_sum, bins=[0.5,1.5,2.5,3.5], rwidth=0.5)\n",
        "axes[3,0].set_title(\"Summation False Positive\")\n",
        "\n",
        "false_negative_values_sum = []\n",
        "for x in false_negative_sum:\n",
        "  false_negative_values_sum.append(x[1])\n",
        "\n",
        "axes[3, 1].hist(false_negative_values_sum, bins=[3.5,4.5,5.5], rwidth=0.5)\n",
        "axes[3,1].set_title(\"Summation False Negative\")\n",
        "\n",
        "\n",
        "for ax in fig.get_axes():\n",
        "    ax.label_outer()\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 8 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "wppt3g51HyHq",
        "outputId": "0e496451-f7e3-401d-b4b7-88e96adb78b8"
      },
      "source": [
        "accuracy_plot = {}\n",
        "\n",
        "for year in correct:\n",
        "  accuracy_plot[year] = correct[year] / total[year]\n",
        "\n",
        "plt.plot(list(accuracy_plot.keys()), list(accuracy_plot.values()), label='No Year')\n",
        "\n",
        "for year in accuracy_plot:\n",
        "  plt.text(year, accuracy_plot[year], str(np.around(accuracy_plot[year], decimals=3)))\n",
        "\n",
        "accuracy_plot_year = {}\n",
        "\n",
        "for year in correct_year:\n",
        "  accuracy_plot_year[year] = correct_year[year] / total[year]\n",
        "\n",
        "plt.plot(list(accuracy_plot_year.keys()), list(accuracy_plot_year.values()), label='W/ Year')\n",
        "for year in accuracy_plot_year:\n",
        "  plt.text(year, accuracy_plot_year[year], str(np.around(accuracy_plot_year[year], decimals=3)))\n",
        "\n",
        "accuracy_plot_year_concat = {}\n",
        "\n",
        "for year in correct_year_concat:\n",
        "  accuracy_plot_year_concat[year] = correct_year_concat[year] / total[year]\n",
        "\n",
        "plt.plot(list(accuracy_plot_year_concat.keys()), list(accuracy_plot_year_concat.values()), label='W/ Year Concat.')\n",
        "for year in accuracy_plot_year_concat:\n",
        "  plt.text(year, accuracy_plot_year_concat[year], str(np.around(accuracy_plot_year_concat[year], decimals=3)))\n",
        "\n",
        "accuracy_plot_year_sum = {}\n",
        "\n",
        "for year in correct_year_sum:\n",
        "  accuracy_plot_year_sum[year] = correct_year_sum[year] / total[year]\n",
        "\n",
        "plt.plot(list(accuracy_plot_year_sum.keys()), list(accuracy_plot_year_sum.values()), label='W/ Year Sum')\n",
        "for year in accuracy_plot_year_concat:\n",
        "  plt.text(year, accuracy_plot_year_sum[year], str(np.around(accuracy_plot_year_sum[year], decimals=3)))\n",
        "\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlFb30-U8dKU",
        "outputId": "b45d61e4-f9a3-447f-cd75-c7ea54038cdc"
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as gensim_api\n",
        "\n",
        "w2v = gensim_api.load(\"glove-wiki-gigaword-100\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3mQfcehR_RF"
      },
      "source": [
        "all_train_text_w2v = train_data['reviewText']\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "clean_text_w2v = []\n",
        "for x in all_train_text_w2v:\n",
        "  #unigrams\n",
        "  x = x.lower()\n",
        "  tokens = x.split()\n",
        "\n",
        "  #remove punctuation\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "\n",
        "  #remove stopwords\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  clean_text_w2v.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "filtered_text_w2v = []\n",
        "for text in clean_text_w2v:\n",
        "  text = [w for w in text if w in w2v.vocab]\n",
        "  filtered_text_w2v.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLMqOthfaIUy"
      },
      "source": [
        "all_test_text_w2v = test_data['reviewText']\n",
        "\n",
        "clean_text_w2v = []\n",
        "for x in all_test_text_w2v:\n",
        "  x = x.lower()\n",
        "  tokens = x.split()\n",
        "\n",
        "  #remove punctuation\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "\n",
        "  #remove stopwords\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  clean_text_w2v.append(tokens)\n",
        "\n",
        "filtered_text_test_w2v = []\n",
        "for text in clean_text_w2v:\n",
        "  text = [w for w in text if w in w2v.vocab]\n",
        "  filtered_text_test_w2v.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaqnqUCU_0QH",
        "outputId": "41c092c6-2f56-421e-bc10-cdf171bb4cae"
      },
      "source": [
        "# embeddings = np.empty([len(train_data), 250, 100])\n",
        "\n",
        "# row_counter = 0\n",
        "# for text in filtered_text_w2v:\n",
        "#   column_counter = 0\n",
        "#   for i in range(len(text)):\n",
        "#     embedding = w2v[str(text[i])]\n",
        "#     embeddings[row_counter][column_counter] = embedding\n",
        "#     column_counter += 1\n",
        "#     if column_counter == 250:\n",
        "#       break\n",
        "#   row_counter += 1\n",
        "# print(embeddings[:10])\n",
        "# print(embeddings.shape)\n",
        "\n",
        "\n",
        "train_indicies = np.empty([len(filtered_text_w2v), max_len])\n",
        "for i in range(len(filtered_text_w2v)):\n",
        "  text = filtered_text_w2v[i]\n",
        "  entry = np.empty(max_len)\n",
        "  for j in range(max_len):\n",
        "    if j >= len(text):\n",
        "      entry[j] = 0\n",
        "    else: \n",
        "      word = text[j]\n",
        "      entry[j] = w2v.wv.vocab[word].index\n",
        "  train_indicies[i] = entry\n",
        "\n",
        "print(type(train_indicies))\n",
        "print(train_indicies[:5])\n",
        "embedding_layer_w2v = w2v.get_keras_embedding()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[[5.47200e+04 9.40000e+01 4.23600e+03 2.09610e+04 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00]\n",
            " [5.77200e+03 8.35000e+02 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00]\n",
            " [1.17000e+02 1.09000e+02 9.65000e+02 8.62000e+02 3.68900e+03 2.53000e+02\n",
            "  2.19000e+02 6.23300e+03 2.93600e+03 8.22000e+02 9.02000e+02 6.90000e+01\n",
            "  1.60000e+01 2.37800e+03 2.40700e+03 8.18000e+03 5.85600e+03 1.69000e+02\n",
            "  1.74790e+04 5.10100e+03 1.97500e+03 1.77500e+03 5.40800e+03 8.15400e+03\n",
            "  9.33250e+04 2.82000e+02 6.05000e+02 1.12900e+03 4.85400e+03 2.63000e+02\n",
            "  2.21500e+03 2.04940e+05 8.52000e+03 8.73000e+02 1.99550e+04 2.39800e+03\n",
            "  1.49000e+02 2.67390e+04 1.46630e+04 2.67900e+03 1.37000e+02 4.67680e+04\n",
            "  3.63100e+03 3.08000e+02 8.73000e+02 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00]\n",
            " [1.32700e+03 1.12550e+04 2.95590e+04 2.17900e+03 4.13000e+03 1.36420e+04\n",
            "  4.90300e+03 2.03000e+02 1.17700e+03 3.02000e+02 4.90300e+03 2.95590e+04\n",
            "  4.45000e+02 1.14500e+03 2.46000e+02 7.63610e+04 1.64900e+03 1.08900e+03\n",
            "  4.38060e+04 1.12700e+03 1.16130e+04 6.52020e+04 7.32200e+03 1.46500e+03\n",
            "  1.20000e+03 1.28500e+03 5.02200e+03 6.30490e+04 5.90700e+03 2.53000e+02\n",
            "  9.40000e+01 2.75000e+02 3.20200e+03 2.70000e+02 4.96000e+02 7.87200e+03\n",
            "  4.25000e+02 3.14000e+02 2.49000e+02 6.88000e+02 5.00000e+01 2.95590e+04\n",
            "  7.90500e+03 6.61000e+02 1.08000e+02 1.12020e+04 4.18800e+03 2.70000e+02\n",
            "  1.57300e+03 4.38220e+04 6.98700e+03 2.54000e+02 1.69000e+02 8.82900e+03\n",
            "  2.56700e+03 3.22200e+03 4.73000e+02 3.24000e+03 2.01000e+03 1.95770e+04\n",
            "  5.11000e+02 3.86280e+04 5.22000e+02 1.09000e+02 3.12400e+03 8.42700e+03\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00]\n",
            " [9.33250e+04 1.61000e+02 1.73832e+05 8.04000e+02 1.80000e+02 1.29800e+03\n",
            "  2.48000e+02 3.14220e+04 4.13500e+03 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
            "  0.00000e+00 0.00000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnvytMi6aeum",
        "outputId": "80268d49-41c4-4b4d-bb62-9d90c43a0588"
      },
      "source": [
        "# embeddings_test = np.empty([len(test_data), 250, 100])\n",
        "\n",
        "# row_counter = 0\n",
        "# for text in filtered_text_test_w2v:\n",
        "#   column_counter = 0\n",
        "#   for i in range(len(text)):\n",
        "#     embedding = w2v[str(text[i])]\n",
        "#     embeddings_test[row_counter][column_counter] = embedding\n",
        "#     column_counter += 1\n",
        "#     if column_counter == 250:\n",
        "#       break\n",
        "#   row_counter += 1\n",
        "\n",
        "\n",
        "test_indicies = np.empty([len(filtered_text_test_w2v), max_len])\n",
        "for i in range(len(filtered_text_test_w2v)):\n",
        "  text = filtered_text_test_w2v[i]\n",
        "  entry = np.empty(max_len)\n",
        "  for j in range(max_len):\n",
        "    if j >= len(text):\n",
        "      entry[j] = 0\n",
        "    else: \n",
        "      word = text[j]\n",
        "      entry[j] = w2v.wv.vocab[word].index\n",
        "  test_indicies[i] = entry"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt3R8j5JAR97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8bb1a6-7cb0-4f62-8545-07543606cdd1"
      },
      "source": [
        "# input_text = tf.keras.layers.Input(shape=(max_len,))\n",
        "# input_year = tf.keras.layers.Input(shape=(1,))\n",
        "# embedding_text = embedding_layer_w2v(input_text)\n",
        "# embedding_year = Embedding(7, 96, name='embedding_layer_year')(input_year)\n",
        "# conv_size_8 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_8', padding='same')(embedding_text)\n",
        "# conv_size_4 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_4', padding='same')(embedding_text)\n",
        "# conv_size_2 = Conv1D(filters=32, kernel_size=8, activation='relu', name='convolutional_layer_2', padding='same')(embedding_text)\n",
        "# pooling_8 = MaxPooling1D(pool_size=max_len, name='pooling_8')(conv_size_8)\n",
        "# pooling_4 = MaxPooling1D(pool_size=max_len, name='pooling_4')(conv_size_4)\n",
        "# pooling_2 = MaxPooling1D(pool_size=max_len, name='pooling_2')(conv_size_2)\n",
        "# concat = tf.keras.layers.concatenate([pooling_8, pooling_4, pooling_2], axis=2, name='concatenation')\n",
        "# flatten_text = Flatten(name='flattening_text')(concat)\n",
        "# flatten_year = Flatten(name='flattening_year')(embedding_year)\n",
        "# concat_year = tf.keras.layers.concatenate([flatten_text, flatten_year], axis=1, name='join_year')\n",
        "# dense = Dense(10, activation='relu', name='hidden_layer')(concat_year)\n",
        "# output = Dense(1, activation='sigmoid', name='output_layer')(dense)\n",
        "\n",
        "# model_year_w2v = tf.keras.Model(inputs=[input_text, input_year], outputs=output)\n",
        "# print(model_year_w2v.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 80)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 80, 100)      40000000    ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " convolutional_layer_8 (Conv1D)  (None, 80, 32)      25632       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " convolutional_layer_4 (Conv1D)  (None, 80, 32)      25632       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " convolutional_layer_2 (Conv1D)  (None, 80, 32)      25632       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " pooling_8 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_8[0][0]']  \n",
            "                                                                                                  \n",
            " pooling_4 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_4[0][0]']  \n",
            "                                                                                                  \n",
            " pooling_2 (MaxPooling1D)       (None, 1, 32)        0           ['convolutional_layer_2[0][0]']  \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenation (Concatenate)    (None, 1, 96)        0           ['pooling_8[0][0]',              \n",
            "                                                                  'pooling_4[0][0]',              \n",
            "                                                                  'pooling_2[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_layer_year (Embeddin  (None, 1, 96)       672         ['input_8[0][0]']                \n",
            " g)                                                                                               \n",
            "                                                                                                  \n",
            " flattening_text (Flatten)      (None, 96)           0           ['concatenation[0][0]']          \n",
            "                                                                                                  \n",
            " flattening_year (Flatten)      (None, 96)           0           ['embedding_layer_year[0][0]']   \n",
            "                                                                                                  \n",
            " join_year (Concatenate)        (None, 192)          0           ['flattening_text[0][0]',        \n",
            "                                                                  'flattening_year[0][0]']        \n",
            "                                                                                                  \n",
            " hidden_layer (Dense)           (None, 10)           1930        ['join_year[0][0]']              \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 1)            11          ['hidden_layer[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 40,079,509\n",
            "Trainable params: 79,509\n",
            "Non-trainable params: 40,000,000\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-17wMr4fHPOF"
      },
      "source": [
        "logdir=\"/content/gdrive/My Drive/large_CNN_w2v/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHW52LLfdolF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e93492-b6c2-4c83-ff5c-da172e99a70e"
      },
      "source": [
        "# model_year_w2v.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model_year_w2v.fit([train_indicies, year_tokens], train_sentiment, epochs=10, callbacks=[tensorboard_callback])\n",
        "# model_year_w2v.save(\"/content/gdrive/My Drive/large_CNN_w2v/model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 71s 47ms/step - loss: 0.4414 - accuracy: 0.7923\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 70s 47ms/step - loss: 0.3427 - accuracy: 0.8508\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 69s 46ms/step - loss: 0.2727 - accuracy: 0.8869\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 70s 47ms/step - loss: 0.2106 - accuracy: 0.9164\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 70s 47ms/step - loss: 0.1611 - accuracy: 0.9377\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 70s 47ms/step - loss: 0.1279 - accuracy: 0.9510\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 70s 47ms/step - loss: 0.1097 - accuracy: 0.9584\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 70s 47ms/step - loss: 0.0971 - accuracy: 0.9621\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 70s 47ms/step - loss: 0.0915 - accuracy: 0.9641\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 70s 47ms/step - loss: 0.0866 - accuracy: 0.9663\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/large_CNN_w2v/model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N51Vo10ZHjZT"
      },
      "source": [
        "# %tensorboard --logdir \"/content/gdrive/My Drive/large_CNN_w2v/logs/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL0dVW6yDTri",
        "outputId": "1ab471cd-85dc-40dd-ef8a-7a45ad4047d3"
      },
      "source": [
        "model_loaded_year_w2v = tf.keras.models.load_model('/content/gdrive/My Drive/large_CNN_w2v/model')\n",
        "loss, accuracy = model_loaded_year_w2v.evaluate([test_indicies, test_year_tokens], test_sentiment)\n",
        "print(accuracy * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375/375 [==============================] - 8s 20ms/step - loss: 0.8173 - accuracy: 0.8202\n",
            "82.02499747276306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LTbd5VkyVWV",
        "outputId": "15cf247b-c996-47e5-9e68-1e95ed1a4564"
      },
      "source": [
        "print(vocab_size)\n",
        "w2v_vocab = set(w2v.vocab.keys())\n",
        "print(len(w2v_vocab.intersection(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21413\n",
            "18748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfZiEOx5NgbN"
      },
      "source": [
        "TODO\n",
        "\n",
        "* Look into layers and see why the accuracy drops\n",
        "* hold out one year of data and see what happens there\n",
        "* Try with word2vec to extract better cosine similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKGBYW_TW9zV"
      },
      "source": [
        "* only one signal provided in one filter\n",
        "* No concept of context here\n",
        "* add year as its own embedding to the flattened vector\n",
        "* send into BERT year and the text separately. "
      ]
    }
  ]
}